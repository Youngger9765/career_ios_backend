# RAG Multi-Agent Architecture

## ç›®éŒ„
- [æ¦‚è¿°](#æ¦‚è¿°)
- [æ¶æ§‹è¨­è¨ˆ](#æ¶æ§‹è¨­è¨ˆ)
- [è³‡æ–™åº«è¨­è¨ˆ](#è³‡æ–™åº«è¨­è¨ˆ)
- [ç¨‹å¼ç¢¼æ¶æ§‹](#ç¨‹å¼ç¢¼æ¶æ§‹)
- [é·ç§»è·¯å¾‘](#é·ç§»è·¯å¾‘)
- [æ•ˆèƒ½ç›£æ§](#æ•ˆèƒ½ç›£æ§)
- [FAQ](#faq)

---

## æ¦‚è¿°

æœ¬ç³»çµ±æ”¯æ´å¤šå€‹ AI Agentï¼ˆè·æ¶¯è«®è©¢ã€å¿ƒç†è«®è©¢ã€è²¡å‹™è¦åŠƒç­‰ï¼‰ï¼Œæ¯å€‹ Agent æ“æœ‰ç¨ç«‹çš„çŸ¥è­˜åº«ã€‚

### è¨­è¨ˆåŸå‰‡

1. **ç¾éšæ®µ**ï¼šå–®ä¸€è³‡æ–™åº« + `agent_id` éæ¿¾ï¼ˆç°¡å–®ã€ç¶­è­·æˆæœ¬ä½ï¼‰
2. **æœªä¾†æ“´å±•**ï¼šå¯ç„¡ç—›é·ç§»åˆ°å¤š Schema éš”é›¢ï¼ˆæ•ˆèƒ½å„ªåŒ–ï¼‰
3. **æŠ½è±¡åŒ–**ï¼šä½¿ç”¨ Repository Patternï¼Œæ¥­å‹™é‚è¼¯ä¸ç›´æ¥æ“ä½œè³‡æ–™åº«

### æ–¹æ¡ˆå°æ¯”

| é …ç›® | å–® Bucket + agent_id | å¤š Bucket (Schemas) |
|------|---------------------|---------------------|
| **æ•ˆèƒ½** | âš¡ï¸âš¡ï¸ éœ€ WHERE éæ¿¾ | âš¡ï¸âš¡ï¸âš¡ï¸ æœ€å¿« |
| **ç¶­è­·** | âœ… ç°¡å–® | âŒ è¤‡é›œ |
| **éš”é›¢æ€§** | âš ï¸ é‚è¼¯éš”é›¢ | âœ… ç‰©ç†éš”é›¢ |
| **å½ˆæ€§** | âœ… é«˜ï¼ˆè·¨ agent æŸ¥è©¢ï¼‰ | âŒ ä½ |
| **é©ç”¨å ´æ™¯** | ä¸­å°è¦æ¨¡ï¼ˆ<10è¬ chunks/agentï¼‰ | å¤§è¦æ¨¡ï¼ˆ>10è¬ chunks/agentï¼‰ |

---

## æ¶æ§‹è¨­è¨ˆ

### æ•´é«”æ¶æ§‹åœ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  API Layer                       â”‚
â”‚  /api/rag/query?agent=career                    â”‚
â”‚  /api/rag/ingest?agent=psychology               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Repository Factory                     â”‚
â”‚  get_rag_repository() â†’ æ ¹æ“šè¨­å®šé¸æ“‡å¯¦ä½œ         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                         â”‚
        â†“                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Single Bucket     â”‚   â”‚ Multi Bucket          â”‚
â”‚ Repository        â”‚   â”‚ Repository            â”‚
â”‚ (Phase 1)         â”‚   â”‚ (Phase 2)             â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                       â”‚
      â†“                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PostgreSQL Database                 â”‚
â”‚  Phase 1: public schema (agent_id filter)       â”‚
â”‚  Phase 2: career/psychology/finance schemas     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## è³‡æ–™åº«è¨­è¨ˆ

### Phase 1: å–® Bucket + agent_id

#### æ–°å¢ agents è¡¨

```sql
CREATE TABLE agents (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) NOT NULL,           -- "è·æ¶¯è«®è©¢"
    slug VARCHAR(50) UNIQUE NOT NULL,     -- "career"
    description TEXT,                      -- æè¿°
    icon VARCHAR(50),                      -- "ğŸ’¼"
    color VARCHAR(20),                     -- "blue"
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- åˆå§‹è³‡æ–™
INSERT INTO agents (name, slug, description, icon, color) VALUES
('è·æ¶¯è«®è©¢', 'career', 'è·æ¶¯è¦åŠƒã€æ±‚è·ç­–ç•¥ã€å±¥æ­·é¢è©¦', 'ğŸ’¼', 'blue'),
('å¿ƒç†è«®è©¢', 'psychology', 'å¿ƒç†å¥åº·ã€æƒ…ç·’ç®¡ç†ã€å£“åŠ›èª¿é©', 'ğŸ§ ', 'purple'),
('è²¡å‹™è¦åŠƒ', 'finance', 'ç†è²¡æŠ•è³‡ã€é€€ä¼‘è¦åŠƒã€ä¿éšªé…ç½®', 'ğŸ’°', 'green');
```

#### ä¿®æ”¹ datasources è¡¨

```sql
-- æ–°å¢ agent_id æ¬„ä½
ALTER TABLE datasources
ADD COLUMN agent_id INTEGER NOT NULL REFERENCES agents(id) ON DELETE CASCADE;

-- å»ºç«‹ç´¢å¼•
CREATE INDEX idx_datasources_agent ON datasources(agent_id);
CREATE INDEX idx_documents_datasource ON documents(datasource_id);
```

#### ER Diagram

```
agents (1) â”€â”€â”€â”€â”€< (N) datasources
                       â”‚
                       â†“ (1)
                  documents (N)
                       â”‚
                       â†“ (1)
                   chunks (N)
                       â”‚
                       â†“ (1)
                  embeddings (1)
```

### Phase 2: å¤š Bucket (æœªä¾†é·ç§»)

```sql
-- å»ºç«‹ç¨ç«‹ schemas
CREATE SCHEMA career;
CREATE SCHEMA psychology;
CREATE SCHEMA finance;

-- åœ¨æ¯å€‹ schema å»ºç«‹ç›¸åŒçµæ§‹
CREATE TABLE career.datasources (...);
CREATE TABLE career.documents (...);
CREATE TABLE career.chunks (...);
CREATE TABLE career.embeddings (...);

-- è¤‡è£½è³‡æ–™ï¼ˆé·ç§»æ™‚åŸ·è¡Œï¼‰
INSERT INTO career.datasources
SELECT id, type, source_uri, created_at
FROM public.datasources
WHERE agent_id = 1;
```

---

## ç¨‹å¼ç¢¼æ¶æ§‹

### ç›®éŒ„çµæ§‹

```
app/
â”œâ”€â”€ repositories/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                    # RAGRepository æŠ½è±¡ä»‹é¢
â”‚   â”œâ”€â”€ single_bucket_repo.py     # å–® Bucket å¯¦ä½œ
â”‚   â”œâ”€â”€ multi_bucket_repo.py      # å¤š Bucket å¯¦ä½œï¼ˆæœªä¾†ï¼‰
â”‚   â””â”€â”€ factory.py                 # å·¥å» æ¨¡å¼ï¼šå‹•æ…‹é¸æ“‡å¯¦ä½œ
â”œâ”€â”€ models/
â”‚   â””â”€â”€ agent.py                   # Agent model
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ rag_query.py               # æŸ¥è©¢ API
â”‚   â””â”€â”€ rag_ingest.py              # ä¸Šå‚³ API
â””â”€â”€ core/
    â””â”€â”€ config.py                  # RAG_MODE è¨­å®š
```

### 1. æŠ½è±¡ä»‹é¢

**`app/repositories/base.py`**

```python
from abc import ABC, abstractmethod
from typing import List, Optional
from app.models.document import Datasource, Document

class RAGRepository(ABC):
    """RAG è³‡æ–™å­˜å–æŠ½è±¡ä»‹é¢"""

    @abstractmethod
    async def get_datasources(self, agent_id: int) -> List[Datasource]:
        """å–å¾—ç‰¹å®š agent çš„æ‰€æœ‰ datasources"""
        pass

    @abstractmethod
    async def create_document(
        self,
        agent_id: int,
        title: str,
        file_content: bytes,
        **kwargs
    ) -> Document:
        """å»ºç«‹æ–°æ–‡ä»¶"""
        pass

    @abstractmethod
    async def search_similar_chunks(
        self,
        agent_id: int,
        query_vector: List[float],
        limit: int = 5
    ) -> List[dict]:
        """å‘é‡ç›¸ä¼¼åº¦æœå°‹"""
        pass

    @abstractmethod
    async def reprocess_document(
        self,
        agent_id: int,
        doc_id: int,
        chunk_size: int,
        overlap: int
    ) -> dict:
        """é‡æ–°è™•ç†æ–‡ä»¶"""
        pass

    @abstractmethod
    async def delete_document(self, agent_id: int, doc_id: int):
        """åˆªé™¤æ–‡ä»¶ï¼ˆå«å®‰å…¨æª¢æŸ¥ï¼‰"""
        pass

    @abstractmethod
    async def get_stats(self, agent_id: Optional[int] = None) -> dict:
        """å–å¾—çµ±è¨ˆè³‡æ–™"""
        pass
```

### 2. Single Bucket å¯¦ä½œ

**`app/repositories/single_bucket_repo.py`**

```python
from sqlalchemy.orm import Session
from sqlalchemy import text, delete
from app.repositories.base import RAGRepository
from app.models.document import Datasource, Document, Chunk, Embedding

class SingleBucketRepository(RAGRepository):
    """å–®ä¸€è³‡æ–™åº« + agent_id éæ¿¾å¯¦ä½œ"""

    def __init__(self, db: Session):
        self.db = db

    async def get_datasources(self, agent_id: int) -> List[Datasource]:
        return self.db.query(Datasource)\
            .filter(Datasource.agent_id == agent_id)\
            .all()

    async def create_document(
        self,
        agent_id: int,
        title: str,
        file_content: bytes,
        **kwargs
    ) -> Document:
        from app.services.storage import StorageService
        from app.services.pdf_service import PDFService
        from app.services.chunking import ChunkingService
        from app.services.openai_service import OpenAIService

        # 1. ä¸Šå‚³åˆ° Storage
        storage = StorageService()
        storage_url = await storage.upload_file(
            file_content,
            f"documents/{agent_id}/{title}",
            content_type="application/pdf"
        )

        # 2. å»ºç«‹ datasourceï¼ˆå« agent_idï¼‰
        datasource = Datasource(
            type="pdf",
            agent_id=agent_id,  # â† é—œéµï¼šè¨˜éŒ„ agent
            source_uri=storage_url
        )
        self.db.add(datasource)
        self.db.flush()

        # 3. æå–æ–‡å­—ä¸¦å»ºç«‹ document
        pdf_service = PDFService()
        text = pdf_service.extract_text(file_content)
        metadata = pdf_service.extract_metadata(file_content)

        document = Document(
            datasource_id=datasource.id,
            title=title,
            bytes=len(file_content),
            pages=metadata.get("pages", 0),
            text_length=len(text),
            meta_json=metadata
        )
        self.db.add(document)
        self.db.flush()

        # 4. Chunking
        chunking = ChunkingService(
            chunk_size=kwargs.get("chunk_size", 400),
            overlap=kwargs.get("overlap", 80)
        )
        chunks = chunking.split_text(text, split_by_sentence=True)

        # 5. ç”Ÿæˆ embeddings
        openai = OpenAIService()
        for idx, chunk_text in enumerate(chunks):
            chunk = Chunk(
                doc_id=document.id,
                ordinal=idx,
                text=chunk_text,
                meta_json={}
            )
            self.db.add(chunk)
            self.db.flush()

            embedding_vector = await openai.create_embedding(chunk_text)
            embedding = Embedding(
                chunk_id=chunk.id,
                embedding=embedding_vector
            )
            self.db.add(embedding)

        self.db.commit()
        return document

    async def search_similar_chunks(
        self,
        agent_id: int,
        query_vector: List[float],
        limit: int = 5
    ) -> List[dict]:
        """å‘é‡æœå°‹ï¼ˆå« agent_id éæ¿¾ï¼‰"""
        result = self.db.execute(text("""
            SELECT
                c.text,
                d.title,
                d.id as doc_id,
                e.embedding <=> :query_vector::vector AS distance
            FROM embeddings e
            JOIN chunks c ON e.chunk_id = c.id
            JOIN documents d ON c.doc_id = d.id
            JOIN datasources ds ON d.datasource_id = ds.id
            WHERE ds.agent_id = :agent_id
            ORDER BY distance
            LIMIT :limit
        """), {
            "agent_id": agent_id,
            "query_vector": query_vector,
            "limit": limit
        })

        return [
            {
                "text": row.text,
                "title": row.title,
                "doc_id": row.doc_id,
                "distance": float(row.distance)
            }
            for row in result
        ]

    async def delete_document(self, agent_id: int, doc_id: int):
        """åˆªé™¤æ–‡ä»¶ï¼ˆå«å®‰å…¨æª¢æŸ¥ï¼‰"""
        # é©—è­‰ document æ˜¯å¦å±¬æ–¼è©² agent
        doc = self.db.query(Document)\
            .join(Datasource)\
            .filter(
                Document.id == doc_id,
                Datasource.agent_id == agent_id  # â† é˜²æ­¢è·¨ agent åˆªé™¤
            ).first()

        if not doc:
            raise ValueError(f"Document {doc_id} not found or access denied")

        # Cascade deleteï¼ˆchunks, embeddingsï¼‰
        self.db.delete(doc)
        self.db.commit()

    async def get_stats(self, agent_id: Optional[int] = None) -> dict:
        """å–å¾—çµ±è¨ˆè³‡æ–™ï¼ˆå¯é¸ agent éæ¿¾ï¼‰"""
        query = text("""
            SELECT
                d.id,
                d.title,
                d.pages,
                d.bytes,
                d.text_length,
                d.created_at,
                COUNT(c.id) as chunks_count,
                COALESCE(SUM(LENGTH(c.text)), 0) as total_text_chars
            FROM documents d
            LEFT JOIN chunks c ON d.id = c.doc_id
            JOIN datasources ds ON d.datasource_id = ds.id
            WHERE (:agent_id IS NULL OR ds.agent_id = :agent_id)
            GROUP BY d.id, d.title, d.pages, d.bytes, d.text_length, d.created_at
            ORDER BY d.created_at DESC
        """)

        result = self.db.execute(query, {"agent_id": agent_id})
        return result.fetchall()
```

### 3. Multi Bucket å¯¦ä½œï¼ˆæœªä¾†ç”¨ï¼‰

**`app/repositories/multi_bucket_repo.py`**

```python
from sqlalchemy import create_engine
from sqlalchemy.orm import Session
from app.repositories.base import RAGRepository
from app.core.config import settings

class MultiBucketRepository(RAGRepository):
    """å¤š Schema éš”é›¢å¯¦ä½œ"""

    def __init__(self):
        # æ¯å€‹ agent å°æ‡‰ä¸€å€‹ schema
        self.schema_map = {
            1: 'career',
            2: 'psychology',
            3: 'finance'
        }

        # å»ºç«‹å„ schema çš„ engine
        self.engines = {}
        for agent_id, schema in self.schema_map.items():
            self.engines[agent_id] = create_engine(
                settings.DATABASE_URL,
                connect_args={"options": f"-csearch_path={schema}"}
            )

    def _get_session(self, agent_id: int) -> Session:
        """æ ¹æ“š agent_id å–å¾—å°æ‡‰çš„ DB session"""
        if agent_id not in self.engines:
            raise ValueError(f"Unknown agent_id: {agent_id}")

        from sqlalchemy.orm import sessionmaker
        SessionLocal = sessionmaker(bind=self.engines[agent_id])
        return SessionLocal()

    async def search_similar_chunks(
        self,
        agent_id: int,
        query_vector: List[float],
        limit: int = 5
    ) -> List[dict]:
        """å‘é‡æœå°‹ï¼ˆä¸éœ€è¦ WHERE agent_idï¼Œå› ç‚ºå·²åœ¨ä¸åŒ schemaï¼‰"""
        db = self._get_session(agent_id)

        result = db.execute(text("""
            SELECT
                c.text,
                d.title,
                d.id as doc_id,
                e.embedding <=> :query_vector::vector AS distance
            FROM embeddings e
            JOIN chunks c ON e.chunk_id = c.id
            JOIN documents d ON c.doc_id = d.id
            ORDER BY distance
            LIMIT :limit
        """), {"query_vector": query_vector, "limit": limit})

        return [
            {
                "text": row.text,
                "title": row.title,
                "doc_id": row.doc_id,
                "distance": float(row.distance)
            }
            for row in result
        ]

    # ... å…¶ä»–æ–¹æ³•é¡ä¼¼å¯¦ä½œ
```

### 4. å·¥å» æ¨¡å¼

**`app/repositories/factory.py`**

```python
from sqlalchemy.orm import Session
from app.repositories.base import RAGRepository
from app.repositories.single_bucket_repo import SingleBucketRepository
from app.repositories.multi_bucket_repo import MultiBucketRepository
from app.core.config import settings

def get_rag_repository(db: Session = None) -> RAGRepository:
    """
    æ ¹æ“šè¨­å®šå‹•æ…‹é¸æ“‡ Repository å¯¦ä½œ

    ç’°å¢ƒè®Šæ•¸ RAG_MODE:
    - "single_bucket": ä½¿ç”¨å–®ä¸€è³‡æ–™åº« + agent_id éæ¿¾
    - "multi_bucket": ä½¿ç”¨å¤š Schema éš”é›¢
    """
    mode = settings.RAG_MODE

    if mode == "single_bucket":
        if not db:
            raise ValueError("SingleBucketRepository requires db session")
        return SingleBucketRepository(db)

    elif mode == "multi_bucket":
        return MultiBucketRepository()

    else:
        raise ValueError(f"Unknown RAG_MODE: {mode}. Use 'single_bucket' or 'multi_bucket'")
```

### 5. è¨­å®šæª”

**`app/core/config.py`**

```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # ... å…¶ä»–è¨­å®š

    # RAG æ¨¡å¼ï¼šsingle_bucket æˆ– multi_bucket
    RAG_MODE: str = "single_bucket"

    class Config:
        env_file = ".env"

settings = Settings()
```

**`.env`**

```bash
# RAG æ¶æ§‹æ¨¡å¼
RAG_MODE=single_bucket  # ç¾éšæ®µç”¨é€™å€‹

# æœªä¾†é·ç§»æ™‚æ”¹æˆ
# RAG_MODE=multi_bucket
```

### 6. API å±¤ä½¿ç”¨

**`app/api/rag_query.py`**

```python
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.models.agent import Agent
from app.repositories.factory import get_rag_repository
from app.services.openai_service import OpenAIService

router = APIRouter(prefix="/api/rag", tags=["rag"])

@router.post("/query")
async def query_rag(
    query: str,
    agent_slug: str = "career",
    limit: int = 5,
    db: Session = Depends(get_db)
):
    """
    RAG æŸ¥è©¢ï¼ˆè‡ªå‹•æ ¹æ“š agent éæ¿¾çŸ¥è­˜åº«ï¼‰

    Args:
        query: ä½¿ç”¨è€…å•é¡Œ
        agent_slug: agent è­˜åˆ¥ç¢¼ï¼ˆcareer/psychology/financeï¼‰
        limit: å›å‚³çµæœæ•¸é‡
    """
    # 1. å–å¾— agent
    agent = db.query(Agent).filter(Agent.slug == agent_slug).first()
    if not agent:
        raise HTTPException(status_code=404, detail=f"Agent '{agent_slug}' not found")

    # 2. å–å¾— Repositoryï¼ˆè‡ªå‹•é¸æ“‡å¯¦ä½œï¼‰
    repo = get_rag_repository(db)

    # 3. ç”Ÿæˆ query embedding
    openai_service = OpenAIService()
    query_vector = await openai_service.create_embedding(query)

    # 4. æœå°‹ç›¸ä¼¼ chunksï¼ˆRepository æœƒè™•ç† agent éš”é›¢ï¼‰
    results = await repo.search_similar_chunks(
        agent_id=agent.id,
        query_vector=query_vector,
        limit=limit
    )

    return {
        "agent": agent_slug,
        "query": query,
        "results": results
    }
```

**`app/api/rag_ingest.py`**

```python
@router.post("/ingest")
async def ingest_file(
    file: UploadFile = File(...),
    agent_slug: str = "career",  # â† æ–°å¢ agent åƒæ•¸
    chunk_size: int = 400,
    overlap: int = 80,
    db: Session = Depends(get_db)
):
    """ä¸Šå‚³æ–‡ä»¶åˆ°æŒ‡å®š agent çš„çŸ¥è­˜åº«"""

    # 1. é©—è­‰ agent
    agent = db.query(Agent).filter(Agent.slug == agent_slug).first()
    if not agent:
        raise HTTPException(status_code=404, detail=f"Agent '{agent_slug}' not found")

    # 2. è®€å–æª”æ¡ˆ
    file_content = await file.read()

    # 3. ä½¿ç”¨ Repository å»ºç«‹æ–‡ä»¶
    repo = get_rag_repository(db)
    document = await repo.create_document(
        agent_id=agent.id,  # â† é—œéµï¼šæŒ‡å®š agent
        title=file.filename,
        file_content=file_content,
        chunk_size=chunk_size,
        overlap=overlap
    )

    return {
        "message": f"Successfully uploaded to {agent.name}",
        "document_id": document.id,
        "agent": agent_slug
    }
```

---

## é·ç§»è·¯å¾‘

### Phase 1 â†’ Phase 2 é·ç§»æ­¥é©Ÿ

#### 1. è³‡æ–™é·ç§»è…³æœ¬

**`scripts/migrate_to_multi_bucket.py`**

```python
"""
å°‡ Single Bucket é·ç§»åˆ° Multi Bucket
åŸ·è¡Œæ–¹å¼ï¼špoetry run python scripts/migrate_to_multi_bucket.py
"""

import asyncio
from sqlalchemy import create_engine, text
from app.core.config import settings
from app.core.database import SessionLocal
from app.models.agent import Agent

async def migrate():
    db = SessionLocal()

    print("ğŸš€ é–‹å§‹é·ç§»...")

    # 1. å–å¾—æ‰€æœ‰ agents
    agents = db.query(Agent).all()
    print(f"ğŸ“‹ æ‰¾åˆ° {len(agents)} å€‹ agents")

    # 2. ç‚ºæ¯å€‹ agent å»ºç«‹ schema
    for agent in agents:
        schema = agent.slug
        print(f"\nğŸ“¦ è™•ç† agent: {agent.name} (schema: {schema})")

        # å»ºç«‹ schema
        db.execute(text(f"CREATE SCHEMA IF NOT EXISTS {schema}"))
        print(f"  âœ“ Schema '{schema}' å·²å»ºç«‹")

        # åœ¨æ–° schema å»ºç«‹ tables
        db.execute(text(f"""
            CREATE TABLE {schema}.datasources (
                id SERIAL PRIMARY KEY,
                type VARCHAR(50) NOT NULL,
                source_uri TEXT,
                created_at TIMESTAMP DEFAULT NOW()
            )
        """))

        db.execute(text(f"""
            CREATE TABLE {schema}.documents (
                id SERIAL PRIMARY KEY,
                datasource_id INTEGER REFERENCES {schema}.datasources(id) ON DELETE CASCADE,
                title VARCHAR(500) NOT NULL,
                bytes INTEGER,
                pages INTEGER,
                text_length INTEGER,
                meta_json JSONB DEFAULT '{{}}',
                created_at TIMESTAMP DEFAULT NOW(),
                updated_at TIMESTAMP DEFAULT NOW()
            )
        """))

        db.execute(text(f"""
            CREATE TABLE {schema}.chunks (
                id SERIAL PRIMARY KEY,
                doc_id INTEGER REFERENCES {schema}.documents(id) ON DELETE CASCADE,
                ordinal INTEGER NOT NULL,
                text TEXT NOT NULL,
                meta_json JSONB DEFAULT '{{}}',
                created_at TIMESTAMP DEFAULT NOW()
            )
        """))

        db.execute(text(f"""
            CREATE TABLE {schema}.embeddings (
                id SERIAL PRIMARY KEY,
                chunk_id INTEGER REFERENCES {schema}.chunks(id) ON DELETE CASCADE UNIQUE,
                embedding vector(1536),
                created_at TIMESTAMP DEFAULT NOW()
            )
        """))

        print(f"  âœ“ Tables å·²å»ºç«‹")

        # 3. è¤‡è£½è³‡æ–™
        # Datasources
        db.execute(text(f"""
            INSERT INTO {schema}.datasources (id, type, source_uri, created_at)
            SELECT id, type, source_uri, created_at
            FROM public.datasources
            WHERE agent_id = :agent_id
        """), {"agent_id": agent.id})

        # Documents
        db.execute(text(f"""
            INSERT INTO {schema}.documents (id, datasource_id, title, bytes, pages, text_length, meta_json, created_at, updated_at)
            SELECT d.id, d.datasource_id, d.title, d.bytes, d.pages, d.text_length, d.meta_json, d.created_at, d.updated_at
            FROM public.documents d
            JOIN public.datasources ds ON d.datasource_id = ds.id
            WHERE ds.agent_id = :agent_id
        """), {"agent_id": agent.id})

        # Chunks
        db.execute(text(f"""
            INSERT INTO {schema}.chunks (id, doc_id, ordinal, text, meta_json, created_at)
            SELECT c.id, c.doc_id, c.ordinal, c.text, c.meta_json, c.created_at
            FROM public.chunks c
            JOIN public.documents d ON c.doc_id = d.id
            JOIN public.datasources ds ON d.datasource_id = ds.id
            WHERE ds.agent_id = :agent_id
        """), {"agent_id": agent.id})

        # Embeddings
        db.execute(text(f"""
            INSERT INTO {schema}.embeddings (id, chunk_id, embedding, created_at)
            SELECT e.id, e.chunk_id, e.embedding, e.created_at
            FROM public.embeddings e
            JOIN public.chunks c ON e.chunk_id = c.id
            JOIN public.documents d ON c.doc_id = d.id
            JOIN public.datasources ds ON d.datasource_id = ds.id
            WHERE ds.agent_id = :agent_id
        """), {"agent_id": agent.id})

        # å»ºç«‹ç´¢å¼•
        db.execute(text(f"""
            CREATE INDEX idx_{schema}_embeddings_vector
            ON {schema}.embeddings USING ivfflat (embedding vector_cosine_ops)
            WITH (lists = 100)
        """))

        print(f"  âœ“ è³‡æ–™å·²è¤‡è£½ä¸¦å»ºç«‹ç´¢å¼•")

    db.commit()
    db.close()

    print("\nâœ… é·ç§»å®Œæˆï¼")
    print("\nä¸‹ä¸€æ­¥ï¼š")
    print("1. ä¿®æ”¹ .env: RAG_MODE=multi_bucket")
    print("2. é‡å•Ÿæœå‹™: poetry run uvicorn app.main:app --reload")
    print("3. æ¸¬è©¦æŸ¥è©¢æ˜¯å¦æ­£å¸¸")
    print("4. ç¢ºèªç„¡èª¤å¾Œï¼Œå¯åˆªé™¤ public schema çš„èˆŠè³‡æ–™")

if __name__ == "__main__":
    asyncio.run(migrate())
```

#### 2. åŸ·è¡Œé·ç§»

```bash
# 1. å‚™ä»½è³‡æ–™åº«
pg_dump $DATABASE_URL > backup_before_migration.sql

# 2. åŸ·è¡Œé·ç§»è…³æœ¬
poetry run python scripts/migrate_to_multi_bucket.py

# 3. ä¿®æ”¹è¨­å®š
echo "RAG_MODE=multi_bucket" >> .env

# 4. é‡å•Ÿæœå‹™
poetry run uvicorn app.main:app --reload

# 5. æ¸¬è©¦
curl -X POST "http://localhost:8000/api/rag/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "å¦‚ä½•å¯«å±¥æ­·ï¼Ÿ", "agent_slug": "career"}'
```

#### 3. å›æ»¾æ–¹æ¡ˆ

```bash
# å¦‚æœé·ç§»å¾Œæœ‰å•é¡Œï¼Œå¯ç«‹å³å›æ»¾
echo "RAG_MODE=single_bucket" >> .env
poetry run uvicorn app.main:app --reload

# èˆŠè³‡æ–™ä»åœ¨ public schemaï¼Œå¯ç¹¼çºŒä½¿ç”¨
```

---

## æ•ˆèƒ½ç›£æ§

### 1. ç›£æ§æŒ‡æ¨™

**`app/repositories/metrics.py`**

```python
import time
import logging
from functools import wraps
from typing import Callable

logger = logging.getLogger(__name__)

def track_query_performance(func: Callable):
    """è¿½è¹¤æŸ¥è©¢æ•ˆèƒ½"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start = time.time()
        result = await func(*args, **kwargs)
        duration = time.time() - start

        # è¨˜éŒ„æ…¢æŸ¥è©¢ï¼ˆ>2ç§’ï¼‰
        if duration > 2.0:
            logger.warning(
                f"âš ï¸ Slow query detected: {func.__name__} "
                f"took {duration:.3f}s with args={args[1:3]}"
            )
        else:
            logger.info(f"âœ“ {func.__name__} completed in {duration:.3f}s")

        return result
    return wrapper

# åœ¨ Repository ä¸­ä½¿ç”¨
class SingleBucketRepository(RAGRepository):
    @track_query_performance
    async def search_similar_chunks(self, agent_id: int, ...):
        # ... å¯¦ä½œ
```

### 2. æ•ˆèƒ½åŸºæº–

| æŒ‡æ¨™ | Single Bucket ç›®æ¨™ | Multi Bucket ç›®æ¨™ |
|------|-------------------|------------------|
| æŸ¥è©¢å»¶é² | <2ç§’ | <1ç§’ |
| ä¸Šå‚³è™•ç† | <30ç§’/æ–‡ä»¶ | <30ç§’/æ–‡ä»¶ |
| Chunks æ•¸é‡ | <100,000/agent | >100,000/agent |
| ä¸¦ç™¼æŸ¥è©¢ | 10 QPS | 50 QPS |

### 3. é·ç§»æ±ºç­–æ¨¹

```
é–‹å§‹ç›£æ§
    â†“
æŸ¥è©¢å»¶é² >2ç§’ ï¼Ÿ
    â†“ Yes
Chunks æ•¸é‡ >100,000 ï¼Ÿ
    â†“ Yes
åŸ·è¡Œé·ç§»åˆ° Multi Bucket
    â†“
ç›£æ§æ–°ç’°å¢ƒæ•ˆèƒ½
    â†“
æ•ˆèƒ½æ”¹å–„ >50% ï¼Ÿ
    â†“ Yes
ä¿ç•™ Multi Bucket
    â†“
åˆªé™¤èˆŠè³‡æ–™
```

---

## FAQ

### Q1: ç‚ºä»€éº¼ä¸ä¸€é–‹å§‹å°±ç”¨ Multi Bucketï¼Ÿ

**A:**
- ç¾éšæ®µè³‡æ–™é‡å°ï¼ˆ<10è¬ chunksï¼‰ï¼ŒSingle Bucket æ•ˆèƒ½è¶³å¤ 
- ç¶­è­·æˆæœ¬ä½ï¼Œé©åˆå°åœ˜éšŠ
- ä¿ç•™å½ˆæ€§ï¼Œæœªä¾†éœ€è¦è·¨ agent æŸ¥è©¢æ™‚ä¸ç”¨é‡æ§‹

### Q2: ä»€éº¼æ™‚å€™è©²é·ç§»åˆ° Multi Bucketï¼Ÿ

**A:** ç•¶ä½ é‡åˆ°ä»¥ä¸‹æƒ…æ³ä¹‹ä¸€ï¼š
- æŸ¥è©¢å»¶é² >2ç§’
- æ¯å€‹ agent æœ‰ >10è¬ chunks
- éœ€è¦æ›´åš´æ ¼çš„è³‡æ–™éš”é›¢
- æœ‰å°ˆé–€çš„ DBA åœ˜éšŠç¶­è­·

### Q3: é·ç§»æœƒå½±éŸ¿æœå‹™å—ï¼Ÿ

**A:**
- é·ç§»éç¨‹ï¼šå¯ä»¥åœ¨ä½å³°æ™‚æ®µåŸ·è¡Œï¼Œç´„ 10-30 åˆ†é˜
- æœå‹™åˆ‡æ›ï¼šä¿®æ”¹ `.env` ä¸¦é‡å•Ÿï¼Œç´„ 1 åˆ†é˜
- å›æ»¾ï¼šæ”¹å› `RAG_MODE=single_bucket` å³å¯ï¼Œç„¡è³‡æ–™æå¤±

### Q4: å¯ä»¥æ··åˆä½¿ç”¨å—ï¼Ÿ

**A:** å¯ä»¥ï¼ä¾‹å¦‚ï¼š
- å¤§é‡è³‡æ–™çš„ agentï¼ˆcareerï¼‰ç”¨ç¨ç«‹ schema
- å°é‡è³‡æ–™çš„ agentï¼ˆfinanceï¼‰ç•™åœ¨ public schema
- åªéœ€åœ¨ `MultiBucketRepository` ä¸­åŠ å…¥ fallback é‚è¼¯

### Q5: è·¨ agent æŸ¥è©¢æ€éº¼è¾¦ï¼Ÿ

**A:**
- **Single Bucket**: ç°¡å–®ï¼Œç›´æ¥ä¸å¸¶ `agent_id` éæ¿¾
- **Multi Bucket**: è¤‡é›œï¼Œéœ€è¦ UNION ALL å¤šå€‹ schema
  ```sql
  SELECT * FROM career.embeddings WHERE ...
  UNION ALL
  SELECT * FROM psychology.embeddings WHERE ...
  ```

### Q6: å¦‚ä½•æ¸¬è©¦å…©ç¨®æ¨¡å¼çš„ä¸€è‡´æ€§ï¼Ÿ

**A:** ä½¿ç”¨ pytest çš„ parametrizeï¼š
```python
@pytest.mark.parametrize("mode", ["single_bucket", "multi_bucket"])
async def test_repository_consistency(mode):
    # åˆ‡æ›æ¨¡å¼ä¸¦æ¸¬è©¦ç›¸åŒæ“ä½œ
    # é©—è­‰çµæœä¸€è‡´
```

---

## é™„éŒ„

### ç›¸é—œæ–‡ä»¶
- [RAG Chunking æ¶æ§‹](./RAG_CHUNKING.md)
- [Vector Search å„ªåŒ–](./VECTOR_SEARCH_OPTIMIZATION.md)
- [API æ–‡ä»¶](./API.md)

### æŠ€è¡“æ£§
- PostgreSQL 15+
- pgvector extension
- SQLAlchemy 2.0
- FastAPI
- Supabase (PaaS)

### ç¶­è­·è€…
- æ¶æ§‹è¨­è¨ˆï¼šClaude Code
- æœ€å¾Œæ›´æ–°ï¼š2025-10-03
