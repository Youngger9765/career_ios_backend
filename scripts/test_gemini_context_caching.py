#!/usr/bin/env python3
"""
æ¸¬è©¦ Gemini Context Caching æ˜¯å¦çœŸçš„èƒ½çœ 50% æ™‚é–“
"""

import asyncio
import os
import sys
import time
from datetime import timedelta

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    import google.generativeai as genai
    from google.generativeai import caching

    from app.core.config import settings

    genai.configure(api_key=settings.GOOGLE_API_KEY)
    GEMINI_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ ç„¡æ³•å°å…¥ Gemini: {e}")
    GEMINI_AVAILABLE = False


async def test_without_caching():
    """æ¸¬è©¦ï¼šæ²’æœ‰ä½¿ç”¨ Cachingï¼ˆåŸºæº–ç·šï¼‰"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æ¸¬è©¦ 1: æ²’æœ‰ä½¿ç”¨ Context Cachingï¼ˆåŸºæº–ç·šï¼‰")
    print("=" * 60)

    # æ¨¡æ“¬å®Œæ•´çš„ contextï¼ˆ8 å¤§æµæ´¾ç†è«– + å®Œæ•´å°è©±ï¼‰
    full_context = """ä½ æ˜¯å°ˆæ¥­è¦ªå­æ•™é¤Šé¡§å•ï¼Œç²¾é€š 8 å¤§æµæ´¾ï¼š
1. é˜¿å¾·å‹’æ­£å‘æ•™é¤Š
2. è–©æçˆ¾æ¨¡å¼ï¼ˆå†°å±±ç†è«–ï¼‰
3. è¡Œç‚ºåˆ†æå­¸æ´¾ (ABA)
4. äººéš›ç¥ç¶“ç”Ÿç‰©å­¸ (Dan Siegel)
5. æƒ…ç·’è¼”å° (John Gottman)
6. å”ä½œè§£æ±ºå•é¡Œ (Ross Greene)
7. ç¾ä»£ä¾é™„èˆ‡å…§åœ¨è§€é» (Dr. Becky Kennedy)
8. ç¤¾æœƒæ„è­˜èˆ‡åƒ¹å€¼è§€æ•™é¤Š

å®Œæ•´å°è©±é€å­—ç¨¿ï¼š
å®¶é•·ï¼šä½ ä»Šå¤©åœ¨å­¸æ ¡éå¾—æ€éº¼æ¨£ï¼Ÿ
å­©å­ï¼šé‚„å¥½å•Š
å®¶é•·ï¼šæœ‰ä»€éº¼é–‹å¿ƒçš„äº‹å—ï¼Ÿ
å­©å­ï¼šæ²’æœ‰
å®¶é•·ï¼šé‚£æœ‰ä»€éº¼ä¸é–‹å¿ƒçš„äº‹å—ï¼Ÿ
å­©å­ï¼šä¹Ÿæ²’æœ‰
å®¶é•·ï¼šä½ ç¢ºå®šå—ï¼Ÿæ„Ÿè¦ºä½ å¥½åƒæœ‰é»ä¸é–‹å¿ƒï¼Ÿ
å­©å­ï¼šå°±é‚„å¥½å•Šï¼Œæ²’ä»€éº¼ç‰¹åˆ¥çš„
ï¼ˆç´„ 500 å­—çš„å®Œæ•´å°è©±...çœç•¥ï¼‰
"""

    times = []

    for i in range(3):
        print(f"\n   ğŸ”„ ç¬¬ {i+1} æ¬¡è«‹æ±‚:")

        # å®Œæ•´ promptï¼ˆæ¯æ¬¡éƒ½åŒ…å«å®Œæ•´ contextï¼‰
        prompt = f"""{full_context}

æœ€è¿‘å°è©±ï¼š
å®¶é•·ï¼šä½ ç¢ºå®šå—ï¼Ÿæ„Ÿè¦ºä½ å¥½åƒæœ‰é»ä¸é–‹å¿ƒï¼Ÿ
å­©å­ï¼šå°±é‚„å¥½å•Šï¼Œæ²’ä»€éº¼ç‰¹åˆ¥çš„

è«‹åˆ†æä¸¦è¿”å› JSONï¼š
{{"safety_level": "green|yellow|red", "severity": 1-3, "display_text": "æç¤º"}}
"""

        start = time.time()

        model = genai.GenerativeModel("gemini-3-flash-preview")
        await model.generate_content_async(
            prompt,
            generation_config={
                "temperature": 0.3,
                "max_output_tokens": 200,
                "response_mime_type": "application/json",
            },
        )

        elapsed = time.time() - start
        times.append(elapsed)

        print(f"      è€—æ™‚: {elapsed*1000:.0f} ms ({elapsed:.2f}s)")

        # ç­‰å¾…é¿å… rate limiting
        if i < 2:
            await asyncio.sleep(2)

    avg = sum(times) / len(times)
    print(f"\n   ğŸ“Š å¹³å‡è€—æ™‚: {avg:.2f}s ({avg*1000:.0f} ms)")
    return avg


async def test_with_caching():
    """æ¸¬è©¦ï¼šä½¿ç”¨ Context Caching"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æ¸¬è©¦ 2: ä½¿ç”¨ Context Caching")
    print("=" * 60)

    # ä¸è®Šçš„ contextï¼ˆå¯ä»¥ç·©å­˜ï¼‰
    system_instruction = """ä½ æ˜¯å°ˆæ¥­è¦ªå­æ•™é¤Šé¡§å•ï¼Œç²¾é€š 8 å¤§æµæ´¾ï¼š
1. é˜¿å¾·å‹’æ­£å‘æ•™é¤Š
2. è–©æçˆ¾æ¨¡å¼ï¼ˆå†°å±±ç†è«–ï¼‰
3. è¡Œç‚ºåˆ†æå­¸æ´¾ (ABA)
4. äººéš›ç¥ç¶“ç”Ÿç‰©å­¸ (Dan Siegel)
5. æƒ…ç·’è¼”å° (John Gottman)
6. å”ä½œè§£æ±ºå•é¡Œ (Ross Greene)
7. ç¾ä»£ä¾é™„èˆ‡å…§åœ¨è§€é» (Dr. Becky Kennedy)
8. ç¤¾æœƒæ„è­˜èˆ‡åƒ¹å€¼è§€æ•™é¤Š
"""

    cached_content = """å®Œæ•´å°è©±é€å­—ç¨¿ï¼š
å®¶é•·ï¼šä½ ä»Šå¤©åœ¨å­¸æ ¡éå¾—æ€éº¼æ¨£ï¼Ÿ
å­©å­ï¼šé‚„å¥½å•Š
å®¶é•·ï¼šæœ‰ä»€éº¼é–‹å¿ƒçš„äº‹å—ï¼Ÿ
å­©å­ï¼šæ²’æœ‰
å®¶é•·ï¼šé‚£æœ‰ä»€éº¼ä¸é–‹å¿ƒçš„äº‹å—ï¼Ÿ
å­©å­ï¼šä¹Ÿæ²’æœ‰
å®¶é•·ï¼šä½ ç¢ºå®šå—ï¼Ÿæ„Ÿè¦ºä½ å¥½åƒæœ‰é»ä¸é–‹å¿ƒï¼Ÿ
å­©å­ï¼šå°±é‚„å¥½å•Šï¼Œæ²’ä»€éº¼ç‰¹åˆ¥çš„
ï¼ˆç´„ 500 å­—çš„å®Œæ•´å°è©±...çœç•¥ï¼‰
"""

    times = []
    cache_obj = None

    for i in range(3):
        print(f"\n   ğŸ”„ ç¬¬ {i+1} æ¬¡è«‹æ±‚:")

        start = time.time()

        if i == 0:
            # ç¬¬ä¸€æ¬¡ï¼šå»ºç«‹ç·©å­˜
            print("      å»ºç«‹ç·©å­˜...")
            try:
                cache_obj = caching.CachedContent.create(
                    model="gemini-3-flash-preview",
                    system_instruction=system_instruction,
                    contents=[cached_content],
                    ttl=timedelta(minutes=60),
                )
                print(f"      âœ… ç·©å­˜å»ºç«‹æˆåŠŸï¼ŒID: {cache_obj.name}")
            except Exception as e:
                print(f"      âŒ ç·©å­˜å»ºç«‹å¤±æ•—: {e}")
                print(f"      éŒ¯èª¤è©³æƒ…: {type(e).__name__}")
                import traceback

                traceback.print_exc()
                return None
        else:
            # ç¬¬ 2-3 æ¬¡ï¼šä½¿ç”¨ç·©å­˜
            print(f"      ä½¿ç”¨ç·©å­˜: {cache_obj.name}")

        # ä½¿ç”¨ç·©å­˜çš„æ¨¡å‹
        try:
            model = genai.GenerativeModel.from_cached_content(cache_obj)
        except Exception as e:
            print(f"      âŒ ç„¡æ³•ä½¿ç”¨ç·©å­˜: {e}")
            return None

        # åªéœ€è¦å‚³æ–°çš„å°è©±ç‰‡æ®µ
        prompt = """æœ€è¿‘å°è©±ï¼š
å®¶é•·ï¼šä½ ç¢ºå®šå—ï¼Ÿæ„Ÿè¦ºä½ å¥½åƒæœ‰é»ä¸é–‹å¿ƒï¼Ÿ
å­©å­ï¼šå°±é‚„å¥½å•Šï¼Œæ²’ä»€éº¼ç‰¹åˆ¥çš„

è«‹åˆ†æä¸¦è¿”å› JSONï¼š
{"safety_level": "green|yellow|red", "severity": 1-3, "display_text": "æç¤º"}
"""

        try:
            response = await model.generate_content_async(
                prompt,
                generation_config={
                    "temperature": 0.3,
                    "max_output_tokens": 200,
                    "response_mime_type": "application/json",
                },
            )

            elapsed = time.time() - start
            times.append(elapsed)

            # é¡¯ç¤º token ä½¿ç”¨é‡
            if hasattr(response, "usage_metadata"):
                usage = response.usage_metadata
                cached_tokens = getattr(usage, "cached_content_token_count", 0)
                prompt_tokens = getattr(usage, "prompt_token_count", 0)
                print(f"      è€—æ™‚: {elapsed*1000:.0f} ms ({elapsed:.2f}s)")
                print(f"      Token: prompt={prompt_tokens}, cached={cached_tokens}")
            else:
                print(f"      è€—æ™‚: {elapsed*1000:.0f} ms ({elapsed:.2f}s)")

        except Exception as e:
            print(f"      âŒ è«‹æ±‚å¤±æ•—: {e}")
            return None

        # ç­‰å¾…é¿å… rate limiting
        if i < 2:
            await asyncio.sleep(2)

    # æ¸…ç†ç·©å­˜
    if cache_obj:
        try:
            cache_obj.delete()
            print("\n   ğŸ—‘ï¸  ç·©å­˜å·²åˆªé™¤")
        except Exception:
            pass

    avg = sum(times) / len(times)
    first_call = times[0]
    subsequent_avg = sum(times[1:]) / len(times[1:]) if len(times) > 1 else times[0]

    print(f"\n   ğŸ“Š ç¬¬ä¸€æ¬¡ï¼ˆå»ºç«‹ç·©å­˜ï¼‰: {first_call:.2f}s ({first_call*1000:.0f} ms)")
    print(
        f"   ğŸ“Š å¾ŒçºŒå¹³å‡ï¼ˆä½¿ç”¨ç·©å­˜ï¼‰: {subsequent_avg:.2f}s ({subsequent_avg*1000:.0f} ms)"
    )
    print(f"   ğŸ“Š ç¸½å¹³å‡: {avg:.2f}s ({avg*1000:.0f} ms)")

    return {"first": first_call, "subsequent": subsequent_avg, "average": avg}


async def main():
    """ä¸»æ¸¬è©¦æµç¨‹"""

    if not GEMINI_AVAILABLE:
        print("âŒ Gemini SDK ä¸å¯ç”¨ï¼Œç„¡æ³•åŸ·è¡Œæ¸¬è©¦")
        return

    print("=" * 60)
    print("ğŸš€ Gemini Context Caching çœŸå¯¦æ¸¬è©¦")
    print("=" * 60)
    print()
    print("ç›®çš„ï¼šé©—è­‰ Context Caching æ˜¯å¦çœŸçš„èƒ½çœ 50% æ™‚é–“")
    print()

    # æ¸¬è©¦ 1: æ²’æœ‰ cachingï¼ˆåŸºæº–ç·šï¼‰
    baseline = await test_without_caching()

    # æ¸¬è©¦ 2: ä½¿ç”¨ caching
    caching_result = await test_with_caching()

    # æ¯”è¼ƒçµæœ
    if baseline and caching_result:
        print("\n" + "=" * 60)
        print("ğŸ“Š æ¸¬è©¦çµæœæ¯”è¼ƒ")
        print("=" * 60)
        print()

        print("æ²’æœ‰ Cachingï¼ˆåŸºæº–ç·šï¼‰:")
        print(f"   å¹³å‡è€—æ™‚: {baseline:.2f}s ({baseline*1000:.0f} ms)")
        print()

        print("ä½¿ç”¨ Caching:")
        print(
            f"   ç¬¬ä¸€æ¬¡ï¼ˆå»ºç«‹ç·©å­˜ï¼‰: {caching_result['first']:.2f}s ({caching_result['first']*1000:.0f} ms)"
        )
        print(
            f"   å¾ŒçºŒè«‹æ±‚ï¼ˆä½¿ç”¨ç·©å­˜ï¼‰: {caching_result['subsequent']:.2f}s ({caching_result['subsequent']*1000:.0f} ms)"
        )
        print()

        # è¨ˆç®—ç¯€çœç™¾åˆ†æ¯”
        savings = (baseline - caching_result["subsequent"]) / baseline * 100
        speedup = baseline / caching_result["subsequent"]

        print("ğŸ¯ é—œéµçµè«–:")
        print(f"   åŸºæº–ç·š: {baseline:.2f}s")
        print(f"   ä½¿ç”¨ç·©å­˜: {caching_result['subsequent']:.2f}s")
        print(f"   ç¯€çœæ™‚é–“: {baseline - caching_result['subsequent']:.2f}s")
        print(f"   ç¯€çœæ¯”ä¾‹: {savings:.1f}%")
        print(f"   åŠ é€Ÿå€æ•¸: {speedup:.2f}x")
        print()

        # åˆ¤æ–·æ˜¯å¦é”åˆ°é æœŸ
        if savings >= 40:
            print("âœ… é©—è­‰æˆåŠŸï¼Context Caching ç¢ºå¯¦èƒ½çœ 40% ä»¥ä¸Šæ™‚é–“ï¼")
        elif savings >= 20:
            print("ğŸŸ¡ éƒ¨åˆ†æœ‰æ•ˆï¼šçœäº† {:.1f}%ï¼Œä½†æ²’é”åˆ° 50% ç›®æ¨™".format(savings))
        else:
            print(f"âŒ æ•ˆæœä¸ä½³ï¼šåªçœäº† {savings:.1f}%ï¼Œå¯èƒ½ä¸å€¼å¾—å¯¦ä½œ")

        # å¯¦éš›æ‡‰ç”¨å ´æ™¯ä¼°ç®—
        print()
        print("ğŸ’¡ å¯¦éš›æ‡‰ç”¨å ´æ™¯ä¼°ç®—ï¼ˆ1 å°æ™‚æœƒè«‡ï¼‰:")
        print(
            f"   ç•¶å‰ï¼ˆç„¡ç·©å­˜ï¼‰: {baseline:.2f}s Ã— 60 æ¬¡ = {baseline*60:.0f}s ({baseline*60/60:.1f} åˆ†é˜)"
        )
        print("   å„ªåŒ–å¾Œï¼ˆæœ‰ç·©å­˜ï¼‰:")
        print(f"      ç¬¬ 1 æ¬¡: {caching_result['first']:.2f}s")
        print(
            f"      ç¬¬ 2-60 æ¬¡: {caching_result['subsequent']:.2f}s Ã— 59 = {caching_result['subsequent']*59:.0f}s"
        )
        print(
            f"      ç¸½è¨ˆ: {caching_result['first'] + caching_result['subsequent']*59:.0f}s ({(caching_result['first'] + caching_result['subsequent']*59)/60:.1f} åˆ†é˜)"
        )
        print(
            f"   ç¯€çœ: {baseline*60 - (caching_result['first'] + caching_result['subsequent']*59):.0f}s ({(baseline*60 - (caching_result['first'] + caching_result['subsequent']*59))/60:.1f} åˆ†é˜)"
        )


if __name__ == "__main__":
    asyncio.run(main())
