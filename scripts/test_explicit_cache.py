#!/usr/bin/env python3
"""
Gemini Explicit Context Caching å¯¦é©—
æ¸¬è©¦ç´¯ç© transcript å ´æ™¯çš„ cache æ•ˆèƒ½ (1-60 åˆ†é˜)
"""

import asyncio
import json
import time
from datetime import datetime

import vertexai
from vertexai.preview import caching
from vertexai.preview.generative_models import GenerativeModel

# Project configuration
PROJECT_ID = "groovy-iris-473015-h3"
LOCATION = "global"
MODEL_NAME = "gemini-3-flash-preview"

# ç³»çµ± Prompt (996 tokens, å›ºå®šä¸è®Šçš„éƒ¨åˆ†)
SYSTEM_INSTRUCTION = """ä½ æ˜¯å°ˆæ¥­è«®è©¢ç£å°ï¼Œåˆ†æå³æ™‚è«®è©¢å°è©±ã€‚ä½ çš„è§’è‰²æ˜¯ç«™åœ¨æ¡ˆä¸»èˆ‡è«®è©¢å¸«ä¹‹é–“ï¼Œæä¾›æº«æš–ã€åŒç†ä¸”å…·é«”å¯è¡Œçš„å°ˆæ¥­å»ºè­°ã€‚

ã€è§’è‰²å®šç¾©ã€‘CRITICAL - å¿…é ˆåš´æ ¼éµå®ˆï¼š
- "counselor" = è«®è©¢å¸«/è¼”å°å¸«ï¼ˆå°ˆæ¥­åŠ©äººè€…ï¼Œæä¾›å”åŠ©çš„ä¸€æ–¹ï¼‰
- "client" = æ¡ˆä¸»/å€‹æ¡ˆ/å®¶é•·ï¼ˆæ±‚åŠ©è€…ï¼Œæœ‰å›°æ“¾éœ€è¦å”åŠ©çš„ä¸€æ–¹ï¼‰
- æ‰€æœ‰å•é¡Œã€å›°æ“¾ã€ç—‡ç‹€éƒ½æ˜¯ã€Œæ¡ˆä¸»/å€‹æ¡ˆã€é¢è‡¨çš„ï¼Œä¸æ˜¯è«®è©¢å¸«çš„å•é¡Œ
- åˆ†æç„¦é»ï¼šæ¡ˆä¸»çš„ç‹€æ³ã€éœ€æ±‚ã€é¢¨éšª
- å»ºè­°å°è±¡ï¼šçµ¦è«®è©¢å¸«çš„å°ˆæ¥­å»ºè­°ï¼ˆå¦‚ä½•å”åŠ©æ¡ˆä¸»ï¼‰

ã€åˆ†æç¯„åœã€‘CRITICAL - å¿…é ˆåš´æ ¼éµå®ˆï¼š
ğŸ¯ **ä¸»è¦åˆ†æç„¦é»**ï¼šæœ€æ–°ä¸€åˆ†é˜å…§çš„å°è©±å…§å®¹
   - ä½ æœƒæ”¶åˆ°å®Œæ•´çš„å°è©±è¨˜éŒ„ï¼ˆå¯èƒ½é•·é”æ•¸ååˆ†é˜ï¼‰
   - ä½†ä½ çš„åˆ†æå¿…é ˆèšç„¦åœ¨ã€Œæœ€å¾Œå‡ºç¾çš„å°è©±ã€ï¼ˆæœ€æ–°ä¸€åˆ†é˜ï¼‰
   - å‰é¢çš„å°è©±åƒ…ä½œç‚ºèƒŒæ™¯è„ˆçµ¡åƒè€ƒï¼Œå¹«åŠ©ä½ ç†è§£å‰å› å¾Œæœ

ã€æ ¸å¿ƒåŸå‰‡ã€‘åŒç†å„ªå…ˆã€æº«å’Œå¼•å°ã€å…·é«”è¡Œå‹•ï¼š

1. **åŒç†èˆ‡ç†è§£ç‚ºå…ˆ**
   - æ°¸é å…ˆç†è§£èˆ‡åŒç†æ¡ˆä¸»ï¼ˆå®¶é•·ï¼‰çš„æ„Ÿå—å’Œè™•å¢ƒ
   - èªå¯æ•™é¤Šå£“åŠ›ã€æƒ…ç·’å¤±æ§æ˜¯æ­£å¸¸çš„äººæ€§åæ‡‰
   - é¿å…æ‰¹åˆ¤ã€æŒ‡è²¬æˆ–è®“æ¡ˆä¸»æ„Ÿåˆ°è¢«å¦å®š

2. **æº«å’Œã€éæ‰¹åˆ¤çš„èªæ°£**
   - âŒ ç¦æ­¢ç”¨èªï¼šã€Œè¡¨é”å‡ºå°å­©å­ä½¿ç”¨èº«é«”æš´åŠ›çš„è¡å‹•ã€ã€Œå¯èƒ½é€ æˆå‚·å®³ã€ã€Œä¸ç•¶ç®¡æ•™ã€
   - âœ… å»ºè­°ç”¨èªï¼šã€Œç†è§£åˆ°åœ¨æ•™é¤Šå£“åŠ›ä¸‹ï¼Œçˆ¶æ¯æœ‰æ™‚æœƒæ„Ÿåˆ°æƒ…ç·’å¤±æ§æ˜¯å¾ˆæ­£å¸¸çš„ã€
   - âœ… ä½¿ç”¨ï¼šã€Œå¯ä»¥è€ƒæ…®ã€ã€Œæˆ–è¨±ã€ã€Œè©¦è©¦çœ‹ã€ç­‰æŸ”å’Œå¼•å°è©
   - âœ… ç„¦é»æ”¾åœ¨ã€Œå¦‚ä½•èª¿æ•´ã€è€Œéã€Œå“ªè£¡åšéŒ¯ã€

3. **å…·é«”ã€ç°¡æ½”çš„å»ºè­°**
   - å»ºè­°è¦å…·é«”å¯è¡Œï¼Œä½†ä¿æŒç°¡çŸ­ï¼ˆä¸è¶…é 50 å­—ï¼‰
   - é¿å…æŠ½è±¡æ¦‚å¿µï¼Œç”¨å…·é«”åšæ³•
   - ä¸è¦å†—é•·çš„æ­¥é©Ÿèªªæ˜æˆ–å°è©±ç¯„ä¾‹

ã€è¼¸å‡ºæ ¼å¼ã€‘è«‹æä¾›ä»¥ä¸‹ JSON æ ¼å¼å›æ‡‰ï¼š

{
  "summary": "æ¡ˆä¸»è™•å¢ƒç°¡è¿°ï¼ˆ1-2 å¥ï¼‰",
  "alerts": [
    "ğŸ’¡ åŒç†æ¡ˆä¸»æ„Ÿå—ï¼ˆ1 å¥ï¼‰",
    "âš ï¸ éœ€é—œæ³¨çš„éƒ¨åˆ†ï¼ˆ1 å¥ï¼‰"
  ],
  "suggestions": [
    "ğŸ’¡ æ ¸å¿ƒå»ºè­°ï¼ˆç°¡çŸ­ï¼Œ< 50 å­—ï¼‰",
    "ğŸ’¡ å…·é«”åšæ³•ï¼ˆç°¡çŸ­ï¼Œ< 50 å­—ï¼‰"
  ]
}

ã€èªæ°£è¦æ±‚ã€‘æº«å’Œã€åŒç†ã€ç°¡æ½”ï¼Œé¿å…æ‰¹åˆ¤æˆ–éåº¦èªªæ•™ã€‚
"""

# æ¸¬è©¦å°è©± - æ“´å±•åˆ° 10 åˆ†é˜ï¼Œç„¶å¾Œå¾ªç’°ä½¿ç”¨ä¾†å‰µé€  60 åˆ†é˜
CONVERSATION_BASE = {
    1: """è«®è©¢å¸«ï¼šä»Šå¤©æƒ³èŠäº›ä»€éº¼å‘¢ï¼Ÿ
å®¶é•·ï¼šæœ€è¿‘è·Ÿå­©å­çš„é—œä¿‚è®Šå¾—å¾ˆç·Šå¼µã€‚ä»–åœ¨å­¸æ ¡ç¸½æ˜¯è·ŸåŒå­¸èµ·è¡çªï¼Œè€å¸«ä¹Ÿåæ˜ ä»–ä¸Šèª²ä¸å°ˆå¿ƒã€‚å›åˆ°å®¶æˆ‘æƒ³è·Ÿä»–èŠèŠï¼Œä»–å°±å¾ˆä¸è€ç…©ã€‚""",
    2: """è«®è©¢å¸«ï¼šä½ å‰›æåˆ°ä»–æœƒä¸è€ç…©ï¼Œé‚£æ™‚å€™ä½ çš„æ„Ÿå—æ˜¯ä»€éº¼ï¼Ÿ
å®¶é•·ï¼šæˆ‘è¦ºå¾—å¾ˆå—å‚·ï¼Œä¹Ÿæœ‰é»ç”Ÿæ°£ã€‚æˆ‘æ˜æ˜æ˜¯é—œå¿ƒä»–ï¼Œä»–ç‚ºä»€éº¼è¦é€™æ¨£å°æˆ‘ï¼Ÿæˆ‘èŠ±é€™éº¼å¤šæ™‚é–“ã€ç²¾åŠ›åœ¨ä»–èº«ä¸Šï¼Œä»–å»å®Œå…¨ä¸é ˜æƒ…ã€‚""",
    3: """è«®è©¢å¸«ï¼šé‚£ä½ é€šå¸¸æœƒæ€éº¼å›æ‡‰ä»–çš„ä¸è€ç…©å‘¢ï¼Ÿ
å®¶é•·ï¼šæˆ‘æœƒå¿ä¸ä½èªªã€Œä½ é€™æ˜¯ä»€éº¼æ…‹åº¦ï¼Ÿæˆ‘æ˜¯ä½ åª½åª½è€¶ï¼ã€ç„¶å¾Œæˆ‘å€‘å°±æœƒåµèµ·ä¾†ã€‚åµå®Œä¹‹å¾Œæˆ‘åˆå¾ˆå¾Œæ‚”ï¼Œè¦ºå¾—è‡ªå·±å¤ªè¡å‹•äº†ã€‚""",
    4: """è«®è©¢å¸«ï¼šè½èµ·ä¾†ä½ å€‘çš„äº’å‹•é™·å…¥äº†ä¸€å€‹å¾ªç’°ã€‚ä½ èªªåµå®Œæœƒå¾Œæ‚”ï¼Œé‚£æ™‚å€™ä½ åœ¨æƒ³ä»€éº¼ï¼Ÿ
å®¶é•·ï¼šæˆ‘æœƒè¦ºå¾—è‡ªå·±å¾ˆå¤±æ•—ï¼Œç‚ºä»€éº¼æˆ‘é€£è‡ªå·±çš„å­©å­éƒ½ç®¡ä¸å¥½ï¼Ÿæœ‰æ™‚å€™æˆ‘çœŸçš„å¿«ç˜‹äº†ï¼Œå¾ˆæƒ³å°ä»–å¤§å¼ï¼Œç”šè‡³æƒ³æ‰“ä»–ä¸€é “ã€‚ä½†æˆ‘çŸ¥é“é€™æ¨£ä¸å°ã€‚""",
    5: """è«®è©¢å¸«ï¼šä½ èƒ½å¦æ‰¿é€™äº›æ„Ÿå—å¾ˆä¸å®¹æ˜“ã€‚é€™äº›å¿µé ­å‡ºç¾çš„æ™‚å€™ï¼Œä½ æœƒæ€éº¼åšï¼Ÿ
å®¶é•·ï¼šæˆ‘æœƒç›¡é‡å¿ä½ï¼Œä½†æœ‰æ™‚å€™çœŸçš„å¿ä¸äº†ã€‚æˆ‘çŸ¥é“é€™æ¨£ä¸å°ï¼Œä½†æˆ‘çœŸçš„ä¸çŸ¥é“è©²æ€éº¼è¾¦äº†ã€‚æˆ‘è¦ºå¾—è‡ªå·±æ˜¯å€‹å¾ˆç³Ÿç³•çš„çˆ¶æ¯ã€‚""",
    6: """è«®è©¢å¸«ï¼šå¾ä½ çš„æè¿°è½èµ·ä¾†ï¼Œä½ æ‰¿å—äº†å¾ˆå¤§çš„å£“åŠ›ã€‚ä½ æœ‰å˜—è©¦éå…¶ä»–æ–¹å¼å—ï¼Ÿ
å®¶é•·ï¼šæˆ‘æœ‰è©¦éè·Ÿä»–å¥½å¥½è¬›ï¼Œä½†ä»–æ ¹æœ¬ä¸è½ã€‚æˆ‘ä¹Ÿè©¦éçµ¦ä»–ä¸€äº›ç©ºé–“ï¼Œä½†ä»–å°±é—œåœ¨æˆ¿é–“è£¡æ‰“é›»å‹•ã€‚æˆ‘çœŸçš„ä¸çŸ¥é“è¦æ€éº¼è·Ÿä»–æºé€šã€‚""",
    7: """è«®è©¢å¸«ï¼šä½ å‰›æåˆ°ä»–æœƒé—œåœ¨æˆ¿é–“æ‰“é›»å‹•ï¼Œé‚£æ™‚å€™ä½ æœƒæ“”å¿ƒä»€éº¼ï¼Ÿ
å®¶é•·ï¼šæˆ‘æ“”å¿ƒä»–æœƒæ²‰è¿·éŠæˆ²ï¼Œè’å»¢å­¸æ¥­ã€‚è€Œä¸”ä»–éƒ½ä¸è·Ÿæˆ‘å€‘äº’å‹•ï¼Œæˆ‘ä¸çŸ¥é“ä»–åœ¨æƒ³ä»€éº¼ã€‚æˆ‘æ€•ä»–æ˜¯ä¸æ˜¯å¿ƒè£¡æœ‰ä»€éº¼å•é¡Œï¼Œä½†ä»–åˆä¸é¡˜æ„è·Ÿæˆ‘èªªã€‚""",
    8: """è«®è©¢å¸«ï¼šä½ æœ‰æ²’æœ‰æƒ³éï¼Œä»–å¯èƒ½ä¹Ÿæœ‰ä»–è‡ªå·±çš„å£“åŠ›å’Œå›°æ“¾ï¼Ÿ
å®¶é•·ï¼šæˆ‘ç•¶ç„¶çŸ¥é“ä»–ä¹Ÿæœ‰å£“åŠ›ï¼Œä½†ä»–éƒ½ä¸è·Ÿæˆ‘èªªå•Šï¼æˆ‘å•ä»–ç™¼ç”Ÿä»€éº¼äº‹ï¼Œä»–å°±èªªæ²’äº‹ã€‚æˆ‘çœŸçš„å¾ˆæƒ³å¹«ä»–ï¼Œä½†ä»–æŠŠæˆ‘æ¨å¾—é é çš„ã€‚""",
    9: """è«®è©¢å¸«ï¼šè½èµ·ä¾†ä½ å€‘ä¹‹é–“å¥½åƒæœ‰ä¸€é“ç‰†ã€‚ä½ è¦ºå¾—é€™é“ç‰†æ˜¯ä»€éº¼ï¼Ÿ
å®¶é•·ï¼šæˆ‘ä¹Ÿä¸çŸ¥é“ã€‚å¯èƒ½æ˜¯æˆ‘å¤ªåš´æ ¼äº†ï¼Ÿé‚„æ˜¯æˆ‘å¤ªå›‰å—¦äº†ï¼Ÿæˆ‘çœŸçš„ä¸çŸ¥é“æ€éº¼åšæ‰å°ã€‚æˆ‘åªæ˜¯æƒ³ç•¶ä¸€å€‹å¥½åª½åª½ï¼Œä½†æˆ‘å¥½åƒè¶ŠåŠªåŠ›è¶Šç³Ÿç³•ã€‚""",
    10: """è«®è©¢å¸«ï¼šä½ å‰›æ‰èªªã€Œè¶ŠåŠªåŠ›è¶Šç³Ÿç³•ã€ï¼Œé€™å¥è©±èƒŒå¾Œæœ‰ä»€éº¼æ„Ÿå—ï¼Ÿ
å®¶é•·ï¼šæˆ‘è¦ºå¾—å¾ˆç„¡åŠ›ã€å¾ˆæŒ«æŠ˜ã€‚æˆ‘èŠ±äº†é€™éº¼å¤šæ™‚é–“æƒ³è¦æ”¹å–„é—œä¿‚ï¼Œä½†å¥½åƒæ²’æœ‰ä»»ä½•é€²å±•ã€‚æœ‰æ™‚å€™æˆ‘ç”šè‡³åœ¨æƒ³ï¼Œæ˜¯ä¸æ˜¯æˆ‘å°±æ˜¯å€‹å¤±æ•—çš„çˆ¶æ¯ï¼Ÿ""",
}


def generate_60_minutes_conversation():
    """ç”Ÿæˆ 60 åˆ†é˜çš„å°è©±å…§å®¹ï¼ˆå¾ªç’°ä½¿ç”¨ 1-10 åˆ†é˜çš„å°è©±ï¼‰"""
    conversation = {}
    for minute in range(1, 61):
        base_minute = ((minute - 1) % 10) + 1
        conversation[minute] = CONVERSATION_BASE[base_minute]
    return conversation


CONVERSATION_MINUTES = generate_60_minutes_conversation()


async def create_cache(minute: int):
    """å‰µå»ºåŒ…å«ç´¯ç© transcript çš„ cache"""
    # çµ„åˆç´¯ç©å°è©±
    accumulated_transcript = "\n\n".join(
        [CONVERSATION_MINUTES[m] for m in range(1, minute + 1)]
    )

    print(f"\nğŸ”¨ å‰µå»º Cache (ç´¯ç©ç¬¬ 1-{minute} åˆ†é˜å°è©±)")
    print(f"ğŸ“ Transcript é•·åº¦: {len(accumulated_transcript)} å­—ç¬¦")

    # å‰µå»º cache
    cached_content = caching.CachedContent.create(
        model_name=MODEL_NAME,
        system_instruction=SYSTEM_INSTRUCTION,
        contents=[accumulated_transcript],
        ttl="3600s",  # 1 hour for longer tests
        display_name=f"counseling-transcript-min{minute}",
    )

    print(f"âœ… Cache å·²å‰µå»º: {cached_content.name}")
    # Note: usage_metadata is not available on CachedContent object
    # Token count will be shown during cache hit analysis
    print(f"â° Cache éæœŸæ™‚é–“: {cached_content.expire_time}")

    return cached_content


async def analyze_with_cache(cache, new_transcript: str, minute: int):
    """ä½¿ç”¨ cache åˆ†ææ–°çš„å°è©±"""
    print(f"\nğŸ” ä½¿ç”¨ Cache åˆ†æç¬¬ {minute} åˆ†é˜...")

    start_time = time.time()

    # ä½¿ç”¨ cached content å‰µå»º model
    model = GenerativeModel.from_cached_content(cached_content=cache)

    # Generate response
    response = model.generate_content(
        f"å°è©±å…§å®¹ï¼š\n{new_transcript}",
        generation_config={
            "temperature": 0.7,
            "max_output_tokens": 4000,
            "response_mime_type": "application/json",
        },
    )

    elapsed_time = time.time() - start_time

    # Extract usage metadata
    usage = response.usage_metadata
    cached_tokens = getattr(usage, "cached_content_token_count", 0)
    prompt_tokens = getattr(usage, "prompt_token_count", 0)
    output_tokens = getattr(usage, "candidates_token_count", 0)

    print(f"âœ… åˆ†æå®Œæˆ ({elapsed_time:.2f}s)")
    print(f"ğŸ¯ Cached tokens: {cached_tokens}")
    print(f"ğŸ“ Prompt tokens: {prompt_tokens}")
    print(f"ğŸ’¬ Output tokens: {output_tokens}")

    # Parse JSON
    try:
        result = json.loads(response.text)
        print(f"ğŸ“Š Summary: {result.get('summary', '')[:50]}...")
    except json.JSONDecodeError:
        result = {"summary": "è§£æå¤±æ•—", "alerts": [], "suggestions": []}

    return {
        "minute": minute,
        "cached_tokens": cached_tokens,
        "prompt_tokens": prompt_tokens,
        "output_tokens": output_tokens,
        "response_time": round(elapsed_time, 2),
        "summary": result.get("summary", ""),
    }


async def main():
    """ä¸»æ¸¬è©¦æµç¨‹ - æ¸¬è©¦ 60 åˆ†é˜å ´æ™¯"""
    print("=" * 80)
    print("ğŸ§ª Gemini Explicit Context Caching å¯¦é©— (60 åˆ†é˜)")
    print("=" * 80)
    print(f"æ¸¬è©¦æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Model: {MODEL_NAME}")
    print(f"Project: {PROJECT_ID}")
    print("=" * 80)

    # Initialize Vertex AI
    vertexai.init(project=PROJECT_ID, location=LOCATION)

    results = []

    # æ¸¬è©¦å ´æ™¯ï¼šå‰µå»ºåŒ…å«å‰ 10 åˆ†é˜çš„ cacheï¼Œç„¶å¾Œæ¸¬è©¦ç¬¬ 11-60 åˆ†é˜
    try:
        # Step 1: å‰µå»ºåŒ…å«ç¬¬ 1-10 åˆ†é˜çš„ cache
        cache = await create_cache(minute=10)

        # Step 2: æ¸¬è©¦ç¬¬ 11-60 åˆ†é˜ (cache hits!)
        # æ¯ 5 åˆ†é˜æ¸¬è©¦ä¸€æ¬¡ï¼Œå…±æ¸¬è©¦ 10 å€‹é»
        test_minutes = [11, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]

        for minute in test_minutes:
            result = await analyze_with_cache(
                cache, CONVERSATION_MINUTES[minute], minute=minute
            )
            results.append(result)
            await asyncio.sleep(0.5)  # Brief pause between calls

        # Cleanup
        print(f"\nğŸ—‘ï¸  åˆªé™¤ Cache: {cache.name}")
        cache.delete()

    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback

        traceback.print_exc()
        return

    # è¼¸å‡ºçµæœ
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¸¬è©¦çµæœæ‘˜è¦ (60 åˆ†é˜å ´æ™¯)")
    print("=" * 80)

    total_cached = 0
    total_prompt = 0
    total_output = 0
    total_time = 0

    for r in results:
        cache_ratio = (
            r["cached_tokens"] / (r["cached_tokens"] + r["prompt_tokens"]) * 100
            if (r["cached_tokens"] + r["prompt_tokens"]) > 0
            else 0
        )
        print(f"\nç¬¬ {r['minute']} åˆ†é˜:")
        print(f"  ğŸ¯ Cached: {r['cached_tokens']} tokens")
        print(f"  ğŸ“ Prompt: {r['prompt_tokens']} tokens")
        print(f"  ğŸ’¬ Output: {r['output_tokens']} tokens")
        print(f"  ğŸ“Š Cache æ¯”ä¾‹: {cache_ratio:.1f}%")
        print(f"  â±ï¸  éŸ¿æ‡‰æ™‚é–“: {r['response_time']}s")

        total_cached += r["cached_tokens"]
        total_prompt += r["prompt_tokens"]
        total_output += r["output_tokens"]
        total_time += r["response_time"]

    # è¨ˆç®—å¹³å‡å€¼
    avg_cache_ratio = (
        total_cached / (total_cached + total_prompt) * 100
        if (total_cached + total_prompt) > 0
        else 0
    )
    avg_response_time = total_time / len(results) if results else 0

    print("\n" + "=" * 80)
    print("ğŸ“ˆ ç¸½é«”çµ±è¨ˆ")
    print("=" * 80)
    print(f"æ¸¬è©¦æ¬¡æ•¸: {len(results)}")
    print(f"ç¸½ Cached tokens: {total_cached}")
    print(f"ç¸½ Prompt tokens: {total_prompt}")
    print(f"ç¸½ Output tokens: {total_output}")
    print(f"å¹³å‡ Cache å‘½ä¸­ç‡: {avg_cache_ratio:.1f}%")
    print(f"å¹³å‡éŸ¿æ‡‰æ™‚é–“: {avg_response_time:.2f}s")
    print(f"Token ç¯€çœ: {total_cached} tokens (åŸæœ¬éœ€è¦ {total_cached + total_prompt})")

    # Save results
    output_file = "explicit_cache_results.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(
            {
                "timestamp": datetime.now().isoformat(),
                "model": MODEL_NAME,
                "test_scenario": "60-minute cumulative transcript",
                "cache_creation": "First 10 minutes",
                "cache_hit_tests": "Minutes 11-60 (sampled)",
                "results": results,
                "summary": {
                    "test_count": len(results),
                    "total_cached_tokens": total_cached,
                    "total_prompt_tokens": total_prompt,
                    "total_output_tokens": total_output,
                    "avg_cache_hit_ratio": round(avg_cache_ratio, 2),
                    "avg_response_time": round(avg_response_time, 2),
                    "token_savings": total_cached,
                },
            },
            f,
            ensure_ascii=False,
            indent=2,
        )

    print(f"\nâœ… çµæœå·²ä¿å­˜: {output_file}")
    print("=" * 80)


if __name__ == "__main__":
    asyncio.run(main())
