#!/usr/bin/env python3
"""
Ê∏¨Ë©¶ 8 Â§ßÊµÅÊ¥æ Practice Mode Prompt

Ê∏¨Ë©¶ 5 ÂÄãÁúüÂØ¶Ë¶™Â≠ê‰∫íÂãïÂ†¥ÊôØÔºö
1. Â≠©Â≠êÊãíÁµïÂØ´‰ΩúÊ•≠
2. ÊâãË∂≥Ë°ùÁ™Å
3. ÊÉÖÁ∑íÂ¥©ÊΩ∞ÔºàÂì≠È¨ßÔºâ
4. ÊåëÊà∞Ê¨äÂ®ÅÔºàÈ†ÇÂò¥Ôºâ
5. ÂàÜÈõ¢ÁÑ¶ÊÖÆ

Ëº∏Âá∫Ôºö
- Token Áî®Èáè
- Response ÊôÇÈñì
- detailed_scripts ÁØÑ‰æã
- ÁêÜË´ñ‰æÜÊ∫êÊ®ôË®ª
- ËàáËàä prompt ÁöÑÊØîËºÉ

‰ΩøÁî®ÊñπÂºèÔºö
    cd /Users/young/project/career_ios_backend
    poetry run python scripts/test_8_schools_prompt.py
"""

import asyncio
import json
import sys
import time
from pathlib import Path

from app.prompts.island_parents_8_schools_emergency_v1 import (
    ISLAND_PARENTS_8_SCHOOLS_EMERGENCY_PROMPT,
)
from app.prompts.island_parents_8_schools_emergency_v1 import (
    PROMPT_METADATA as EMERGENCY_METADATA,
)
from app.prompts.island_parents_8_schools_practice_v1 import (
    ISLAND_PARENTS_8_SCHOOLS_PRACTICE_PROMPT,
)
from app.prompts.island_parents_8_schools_practice_v1 import (
    PROMPT_METADATA as PRACTICE_METADATA,
)
from app.services.gemini_service import GeminiService

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ==============================================================================
# Ê∏¨Ë©¶Â†¥ÊôØÂÆöÁæ©
# ==============================================================================

TEST_SCENARIOS = [
    {
        "id": 1,
        "name": "Â≠©Â≠êÊãíÁµïÂØ´‰ΩúÊ•≠",
        "context": """Ê°à‰∏ªË≥áË®ä: ÁéãÂ™ΩÂ™Ω, Áï∂ÂâçÁãÄÊ≥Å: 7Ê≠≤ÂÖíÂ≠ê‰∏çÈ°òÊÑèÂØ´‰ΩúÊ•≠
Ê°à‰æãÁõÆÊ®ô: ÊîπÂñÑË¶™Â≠êÊ∫ùÈÄöÔºåÊ∏õÂ∞ë‰ΩúÊ•≠Ë°ùÁ™Å
ÊúÉË´áÊ¨°Êï∏: Á¨¨ 2 Ê¨°, ÊúÉË´áÂÇôË®ª: ‰∏äÊ¨°Ë®éË´ñÈÅéÊ≠£ÂêëÊïôÈ§äÊäÄÂ∑ß""",
        "full_transcript": """Ôºà‰πãÂâçÂ∞çË©±Ôºâ
Â™ΩÂ™ΩÔºö‰ªäÂ§©‰∏ãÂçàÂ≠©Â≠êÊîæÂ≠∏Âõû‰æÜÔºåÊàëÂè´‰ªñÂØ´‰ΩúÊ•≠Ôºå‰ªñË™™‰∏çË¶Å„ÄÇ
Ë´ÆË©¢Â∏´ÔºöÈÇ£ÊôÇÂÄô‰Ω†ÊÄéÈ∫ºÂõûÊáâÁöÑÂë¢Ôºü
Â™ΩÂ™ΩÔºöÊàëÂ∞±Ë™™„Äå‰∏çÂØ´‰∏çË°åÂïäÔºåËÄÅÂ∏´ÊúÉÁΩµ„ÄçÔºå‰ªñÂ∞±ÈñãÂßãÂì≠„ÄÇ""",
        "transcript_segment": """Â™ΩÂ™ΩÔºöÊàë‰ªäÂ§©ÂèàË©¶ËëóÂè´‰ªñÂØ´‰ΩúÊ•≠Ôºå‰ªñÈÇÑÊòØË™™‰∏çË¶Å„ÄÇ
Â≠©Â≠êÔºöÔºàË∫∫Âú®Ê≤ôÁôº‰∏äÔºâÊàë‰∏çÊÉ≥ÂØ´ÔºÅÊàëÂ•ΩÁ¥ØÔºÅ
Â™ΩÂ™ΩÔºö‰Ω†ÊØèÂ§©ÈÉΩË™™Á¥ØÔºåÂäüË™≤ÈÇÑÊòØË¶ÅÂØ´ÂïäÔºÅ
Â≠©Â≠êÔºöÔºàÈñãÂßãÁÖ©Ë∫ÅÔºâÊàëÂ∞±ÊòØ‰∏çÊÉ≥ÂØ´Âï¶ÔºÅ‰Ω†ÊØèÂ§©ÈÉΩÈÄºÊàëÔºÅ
Â™ΩÂ™ΩÔºöÔºàË™ûÊ∞£ÈñãÂßãÊÄ•Ë∫ÅÔºâÊàë‰∏çÊòØÈÄº‰Ω†ÔºåÊòØ‰Ω†Ëá™Â∑±ÁöÑÂäüË™≤ÂïäÔºÅ""",
        "expected_severity": 2,  # YELLOW
        "expected_theories": ["Ëñ©ÊèêÁàæÊ®°Âºè", "Dr. Becky Kennedy", "ÈòøÂæ∑ÂãíÊ≠£ÂêëÊïôÈ§ä"],
    },
    {
        "id": 2,
        "name": "ÊâãË∂≥Ë°ùÁ™Å",
        "context": """Ê°à‰∏ªË≥áË®ä: ÊûóÂ™ΩÂ™Ω, Áï∂ÂâçÁãÄÊ≥Å: 5Ê≠≤ÂßêÂßêÂíå3Ê≠≤ÂºüÂºüÁ∂ìÂ∏∏Êê∂Áé©ÂÖ∑
Ê°à‰æãÁõÆÊ®ô: Âª∫Á´ãÊâãË∂≥Áõ∏ËôïË¶èÂâáÔºåÂüπÈ§äÂêåÁêÜÂøÉ
ÊúÉË´áÊ¨°Êï∏: Á¨¨ 1 Ê¨°""",
        "full_transcript": "ÔºàÈ¶ñÊ¨°Ë´ÆË©¢ÔºåÂ∞öÁÑ°ÂÖàÂâçÂ∞çË©±Ôºâ",
        "transcript_segment": """Â™ΩÂ™ΩÔºöÂßêÂßêÂíåÂºüÂºüÂèàÂú®Êê∂Áé©ÂÖ∑‰∫Ü„ÄÇ
ÂßêÂßêÔºöÔºàÂ§ßÂè´ÔºâÈÄôÊòØÊàëÁöÑÔºÅ‰Ω†‰∏çÂèØ‰ª•ÊãøÔºÅ
ÂºüÂºüÔºöÔºàÂì≠ÔºâÊàë‰πüË¶ÅÁé©ÔºÅ
ÂßêÂßêÔºöÔºàÊé®ÈñãÂºüÂºüÔºâ‰Ω†Ëµ∞ÈñãÂï¶ÔºÅ
Â™ΩÂ™ΩÔºöÔºàÂ§ßËÅ≤Ôºâ‰∏çË¶ÅÊé®ÂºüÂºüÔºÅ‰Ω†ÊòØÂßêÂßêË¶ÅËÆìÂºüÂºüÔºÅ
ÂßêÂßêÔºöÔºàÂßîÂ±àÔºâÁÇ∫‰ªÄÈ∫ºÊØèÊ¨°ÈÉΩË¶ÅÊàëËÆìÔºÅ""",
        "expected_severity": 2,  # YELLOW
        "expected_theories": ["ÊÉÖÁ∑íËºîÂ∞é", "Âçî‰ΩúËß£Ê±∫ÂïèÈ°å", "Á§æÊúÉÊÑèË≠òËàáÂÉπÂÄºËßÄÊïôÈ§ä"],
    },
    {
        "id": 3,
        "name": "ÊÉÖÁ∑íÂ¥©ÊΩ∞ÔºàÂì≠È¨ßÔºâ",
        "context": """Ê°à‰∏ªË≥áË®ä: Èô≥Áà∏Áà∏, Áï∂ÂâçÁãÄÊ≥Å: 4Ê≠≤Â•≥ÂÖíÂÆπÊòìÊÉÖÁ∑íÂ§±Êéß
Ê°à‰æãÁõÆÊ®ô: Â≠∏ÁøíÊÉÖÁ∑íË™øÁØÄÊäÄÂ∑ß
ÊúÉË´áÊ¨°Êï∏: Á¨¨ 3 Ê¨°, ÊúÉË´áÂÇôË®ª: Â∑≤Ë®éË´ñÂÖ®ËÖ¶ÊïôÈ§äÊ¶ÇÂøµ""",
        "full_transcript": """Ôºà‰πãÂâçÂ∞çË©±Ôºâ
Áà∏Áà∏Ôºö‰∏äÊ¨°‰Ω†Ë™™ÁöÑ„Äå‰∏ãÂ±§ËÖ¶„ÄçÊàëÊúâÊ≥®ÊÑèÂà∞ÔºåÂ•≥ÂÖíÁîüÊ∞£ÊôÇÁúüÁöÑÂæàÈõ£Ë¨õÈÅìÁêÜ„ÄÇ
Ë´ÆË©¢Â∏´ÔºöÂ∞çÔºåÈÄôÊôÇÂÄôË¶ÅÂÖàÂÆâÊí´ÊÉÖÁ∑íÔºåÁ≠âÂ•πÂÜ∑Èùú‰∫ÜÂÜçÊ∫ùÈÄö„ÄÇ""",
        "transcript_segment": """Áà∏Áà∏Ôºö‰ªäÂ§©Â∏∂Â•πÂéªË≥£Â†¥ÔºåÂ•πË¶ÅË≤∑Áé©ÂÖ∑ÔºåÊàëË™™‰∏çË°å„ÄÇ
Â•≥ÂÖíÔºöÔºàÈñãÂßãÂì≠È¨ßÔºâÊàëË¶ÅË≤∑ÔºÅÊàëË¶ÅË≤∑ÔºÅ
Áà∏Áà∏ÔºöÔºàËπ≤‰∏ãÔºâÊàëÁü•ÈÅì‰Ω†ÂæàÊÉ≥Ë¶ÅÔºå‰ΩÜÊòØ...
Â•≥ÂÖíÔºöÔºàÂ§ßÂì≠Ôºâ‰Ω†ÊØèÊ¨°ÈÉΩË™™‰∏çË°åÔºÅÊàëË®éÂé≠‰Ω†ÔºÅÔºàË∫∫Âú®Âú∞‰∏äÊâìÊªæÔºâ
Áà∏Áà∏ÔºöÔºàÊúâÈªûÊÖåÔºâÂ•Ω‰∫ÜÂ•Ω‰∫ÜÔºå‰∏çË¶ÅÂì≠‰∫ÜÔºåÂ§ßÂÆ∂ÈÉΩÂú®Áúã...
Â•≥ÂÖíÔºöÔºàÂì≠ÂæóÊõ¥Â§ßËÅ≤ÔºâÊàëÂ∞±ÊòØË¶ÅË≤∑ÔºÅ""",
        "expected_severity": 3,  # RED
        "expected_theories": ["‰∫∫ÈöõÁ•ûÁ∂ìÁîüÁâ©Â≠∏", "ÊÉÖÁ∑íËºîÂ∞é", "Dr. Becky Kennedy"],
    },
    {
        "id": 4,
        "name": "ÊåëÊà∞Ê¨äÂ®ÅÔºàÈ†ÇÂò¥Ôºâ",
        "context": """Ê°à‰∏ªË≥áË®ä: ÂºµÂ™ΩÂ™Ω, Áï∂ÂâçÁãÄÊ≥Å: 9Ê≠≤ÂÖíÂ≠êÈñãÂßãÈ†ÇÂò¥
Ê°à‰æãÁõÆÊ®ô: Âª∫Á´ãÊ∫´ÂíåËÄåÂ†ÖÂÆöÁöÑÁïåÁ∑ö
ÊúÉË´áÊ¨°Êï∏: Á¨¨ 1 Ê¨°""",
        "full_transcript": "ÔºàÈ¶ñÊ¨°Ë´ÆË©¢ÔºåÂ∞öÁÑ°ÂÖàÂâçÂ∞çË©±Ôºâ",
        "transcript_segment": """Â™ΩÂ™ΩÔºöÊàëÂè´‰ªñÂéªÊ¥óÊæ°Ôºå‰ªñË™™„ÄåÁ≠â‰∏Ä‰∏ã„Äç„ÄÇ
Â≠©Â≠êÔºöÊàëÂÜçÁé©5ÂàÜÈêòÂ∞±ÂéªÔºÅ
Â™ΩÂ™ΩÔºöÁèæÂú®Â∞±ÂéªÔºå‰∏çÁÑ∂Ê∞¥ÊúÉÂÜ∑Êéâ„ÄÇ
Â≠©Â≠êÔºöÔºà‰∏çËÄêÁÖ©Ôºâ‰Ω†ÂæàÁÖ©ËÄ∂ÔºÅÊàëË™™Á≠â‰∏Ä‰∏ãÂ∞±Á≠â‰∏Ä‰∏ãÔºÅ
Â™ΩÂ™ΩÔºöÔºàÁîüÊ∞£Ôºâ‰Ω†Ë∑üË™∞Ë™™Ë©±Âë¢ÔºÅ‰Ω†ÊÄéÈ∫ºÂèØ‰ª•ÈÄôÊ®£Ë∑üÂ™ΩÂ™ΩË¨õË©±ÔºÅ
Â≠©Â≠êÔºö‰Ω†‰πüÂæàÁÖ©ÂïäÔºÅÊØèÊ¨°ÈÉΩÈÄôÊ®£ÔºÅ""",
        "expected_severity": 2,  # YELLOW
        "expected_theories": ["ÈòøÂæ∑ÂãíÊ≠£ÂêëÊïôÈ§ä", "Âçî‰ΩúËß£Ê±∫ÂïèÈ°å", "Ëñ©ÊèêÁàæÊ®°Âºè"],
    },
    {
        "id": 5,
        "name": "ÂàÜÈõ¢ÁÑ¶ÊÖÆ",
        "context": """Ê°à‰∏ªË≥áË®ä: ÂäâÂ™ΩÂ™Ω, Áï∂ÂâçÁãÄÊ≥Å: 3Ê≠≤ÂÖíÂ≠êÈÄÅÊâòÂ¨∞ÊôÇÂì≠È¨ß
Ê°à‰æãÁõÆÊ®ô: Á∑©Ëß£ÂàÜÈõ¢ÁÑ¶ÊÖÆÔºåÂª∫Á´ãÂÆâÂÖ®ÊÑü
ÊúÉË´áÊ¨°Êï∏: Á¨¨ 2 Ê¨°, ÊúÉË´áÂÇôË®ª: ‰∏äÊ¨°Ë®éË´ñÈÅé‰æùÈôÑÁêÜË´ñ""",
        "full_transcript": """Ôºà‰πãÂâçÂ∞çË©±Ôºâ
Â™ΩÂ™ΩÔºö‰ªñÊØèÊ¨°ÈÄÅÂéªÊâòÂ¨∞ÈÉΩÂì≠ÂæóÂæàÊÖòÔºåÊàë‰πüÂæàÊç®‰∏çÂæó„ÄÇ
Ë´ÆË©¢Â∏´ÔºöÂàÜÈõ¢ÁÑ¶ÊÖÆÊòØÊ≠£Â∏∏ÁöÑÔºåÊàëÂÄëÂèØ‰ª•Âª∫Á´ã‰∏Ä‰∫õÂÑÄÂºè‰æÜÂπ´Âä©‰ªñ„ÄÇ""",
        "transcript_segment": """Â™ΩÂ™ΩÔºö‰ªäÂ§©Êó©‰∏äÈÄÅ‰ªñÂéªÊâòÂ¨∞Ôºå‰ªñÂèàÈñãÂßãÂì≠‰∫Ü„ÄÇ
Â≠©Â≠êÔºöÔºàÁ∑äÊä±Â™ΩÂ™ΩÔºâÊàë‰∏çË¶ÅÂéªÔºÅÊàëË¶ÅË∑üÂ™ΩÂ™ΩÔºÅ
Â™ΩÂ™ΩÔºöÔºàÊ∫´Êüî‰ΩÜÁÑ¶ÊÄ•ÔºâÂØ∂Ë≤ù‰πñÔºåÂ™ΩÂ™Ω‰∏ãÂçàÂ∞±‰æÜÊé•‰Ω†...
Â≠©Â≠êÔºöÔºàÂì≠Ôºâ‰∏çË¶ÅÔºÅÊàë‰∏çË¶Å‰Ω†Ëµ∞ÔºÅ
ËÄÅÂ∏´ÔºöÔºàÈÅé‰æÜÊé•Ôºâ‰æÜÔºåÊàëÂÄëÂéªÁé©Áé©ÂÖ∑...
Â≠©Â≠êÔºöÔºàÂ§ßÂì≠ÔºåÊéôÊâéÔºâÂ™ΩÂ™Ω‰∏çË¶ÅËµ∞ÔºÅÂ™ΩÂ™ΩÔºÅ
Â™ΩÂ™ΩÔºöÔºàÁúºÁú∂Ê≥õÁ¥ÖÔºå‰∏çÁü•ÊâÄÊé™ÔºâÊÄéÈ∫ºËæ¶...Ë¶Å‰∏çË¶Å‰ªäÂ§©‰∏çÂéª‰∫ÜÔºü""",
        "expected_severity": 2,  # YELLOW (ÈõñÁÑ∂Âì≠È¨ßÔºå‰ΩÜÂ±¨ÊñºÁôºÂ±ïÊ≠£Â∏∏ÁØÑÂúç)
        "expected_theories": ["Áèæ‰ª£‰æùÈôÑËàáÂÖßÂú®ËßÄÈªû", "ÊÉÖÁ∑íËºîÂ∞é", "‰∫∫ÈöõÁ•ûÁ∂ìÁîüÁâ©Â≠∏"],
    },
]


# ==============================================================================
# Ê∏¨Ë©¶ÂáΩÊï∏
# ==============================================================================


async def test_practice_prompt(scenario: dict) -> dict:
    """‰ΩøÁî® Practice Mode prompt Ê∏¨Ë©¶ÂñÆ‰∏ÄÂ†¥ÊôØ"""
    print(f"\n{'='*80}")
    print(f"Â†¥ÊôØ {scenario['id']}: {scenario['name']} (Practice Mode)")
    print(f"{'='*80}")

    # ÁµÑË£ù prompt
    prompt = ISLAND_PARENTS_8_SCHOOLS_PRACTICE_PROMPT.format(
        context=scenario["context"],
        full_transcript=scenario["full_transcript"],
        transcript_segment=scenario["transcript_segment"],
    )

    # Ë®òÈåÑÈñãÂßãÊôÇÈñì
    start_time = time.time()

    # Ë™øÁî® Gemini
    gemini_service = GeminiService()
    try:
        ai_response = await gemini_service.generate_text(
            prompt, temperature=0.3, response_format={"type": "json_object"}
        )

        # Ë®òÈåÑÁµêÊùüÊôÇÈñì
        end_time = time.time()
        duration_ms = int((end_time - start_time) * 1000)

        # Ëß£Êûê response
        response_text = (
            ai_response.text if hasattr(ai_response, "text") else str(ai_response)
        )

        # ÊèêÂèñ JSON
        try:
            json_start = response_text.find("{")
            json_end = response_text.rfind("}") + 1
            json_str = response_text[json_start:json_end]
            result_data = json.loads(json_str)
        except json.JSONDecodeError as e:
            print(f"‚ùå JSON Ëß£ÊûêÂ§±Êïó: {e}")
            result_data = {
                "error": "JSON parse failed",
                "raw_response": response_text[:500],
            }

        # ÊèêÂèñ token ‰ΩøÁî®Èáè
        token_usage = getattr(ai_response, "usage_metadata", None)
        if token_usage:
            prompt_tokens = getattr(token_usage, "prompt_token_count", 0)
            completion_tokens = getattr(token_usage, "candidates_token_count", 0)
            total_tokens = getattr(token_usage, "total_token_count", 0)
        else:
            prompt_tokens = 0
            completion_tokens = 0
            total_tokens = 0

        # Ë®àÁÆóÊàêÊú¨ (Gemini 3 Flash: $0.50/1M input, $3/1M output)
        input_cost = prompt_tokens * 0.00000050
        output_cost = completion_tokens * 0.000003
        total_cost = input_cost + output_cost

        # Êï¥ÁêÜÁµêÊûú
        test_result = {
            "scenario_id": scenario["id"],
            "scenario_name": scenario["name"],
            "duration_ms": duration_ms,
            "token_usage": {
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": total_tokens,
            },
            "cost_usd": {
                "input": round(input_cost, 6),
                "output": round(output_cost, 6),
                "total": round(total_cost, 6),
            },
            "response": result_data,
        }

        # È°ØÁ§∫ÁµêÊûú
        print(f"\n‚è±Ô∏è  Response Time: {duration_ms} ms")
        print(
            f"üìä Token Usage: {total_tokens} tokens (prompt: {prompt_tokens}, completion: {completion_tokens})"
        )
        print(f"üí∞ Cost: ${total_cost:.6f} USD")
        print(f"\nüéØ Safety Level: {result_data.get('safety_level', 'N/A')}")
        print(f"üìà Severity: {result_data.get('severity', 'N/A')}")
        print(f"üí¨ Display Text: {result_data.get('display_text', 'N/A')}")
        print("\nüìù Action Suggestion:")
        print(f"   {result_data.get('action_suggestion', 'N/A')}")

        # È°ØÁ§∫ detailed_scripts
        if "detailed_scripts" in result_data and result_data["detailed_scripts"]:
            print(f"\nüìñ Detailed Scripts ({len(result_data['detailed_scripts'])} ÂÄã):")
            for i, script in enumerate(result_data["detailed_scripts"], 1):
                print(f"\n   --- Script {i} ---")
                print(f"   Situation: {script.get('situation', 'N/A')}")
                print(f"   Theory Basis: {script.get('theory_basis', 'N/A')}")
                print(f"   Step: {script.get('step', 'N/A')}")
                print("\n   Parent Script:")
                parent_script = script.get("parent_script", "N/A")
                # È°ØÁ§∫Ââç 300 Â≠ó
                if len(parent_script) > 300:
                    print(f"   {parent_script[:300]}...")
                    print(f"   (total {len(parent_script)} chars)")
                else:
                    print(f"   {parent_script}")
                    print(f"   ({len(parent_script)} chars)")

                print("\n   Child Likely Response:")
                print(f"   {script.get('child_likely_response', 'N/A')}")
        else:
            print("\n‚ö†Ô∏è  No detailed_scripts in response")

        # È°ØÁ§∫ÁêÜË´ñÊ°ÜÊû∂
        if (
            "theoretical_frameworks" in result_data
            and result_data["theoretical_frameworks"]
        ):
            print("\nüß† Theoretical Frameworks:")
            for theory in result_data["theoretical_frameworks"]:
                print(f"   - {theory}")
        else:
            print("\n‚ö†Ô∏è  No theoretical_frameworks in response")

        return test_result

    except Exception as e:
        print(f"‚ùå Error: {e}")
        return {
            "scenario_id": scenario["id"],
            "scenario_name": scenario["name"],
            "error": str(e),
        }


async def test_emergency_prompt(scenario: dict) -> dict:
    """‰ΩøÁî® Emergency Mode prompt Ê∏¨Ë©¶ÂñÆ‰∏ÄÂ†¥ÊôØ"""
    print(f"\n{'='*80}")
    print(f"Â†¥ÊôØ {scenario['id']}: {scenario['name']} (Emergency Mode)")
    print(f"{'='*80}")

    # ÁµÑË£ù prompt
    prompt = ISLAND_PARENTS_8_SCHOOLS_EMERGENCY_PROMPT.format(
        context=scenario["context"],
        full_transcript=scenario["full_transcript"],
        transcript_segment=scenario["transcript_segment"],
    )

    # Ë®òÈåÑÈñãÂßãÊôÇÈñì
    start_time = time.time()

    # Ë™øÁî® Gemini
    gemini_service = GeminiService()
    try:
        ai_response = await gemini_service.generate_text(
            prompt, temperature=0.3, response_format={"type": "json_object"}
        )

        # Ë®òÈåÑÁµêÊùüÊôÇÈñì
        end_time = time.time()
        duration_ms = int((end_time - start_time) * 1000)

        # Ëß£Êûê response
        response_text = (
            ai_response.text if hasattr(ai_response, "text") else str(ai_response)
        )

        # ÊèêÂèñ JSON
        try:
            json_start = response_text.find("{")
            json_end = response_text.rfind("}") + 1
            json_str = response_text[json_start:json_end]
            result_data = json.loads(json_str)
        except json.JSONDecodeError as e:
            print(f"‚ùå JSON Ëß£ÊûêÂ§±Êïó: {e}")
            result_data = {
                "error": "JSON parse failed",
                "raw_response": response_text[:500],
            }

        # ÊèêÂèñ token ‰ΩøÁî®Èáè
        token_usage = getattr(ai_response, "usage_metadata", None)
        if token_usage:
            prompt_tokens = getattr(token_usage, "prompt_token_count", 0)
            completion_tokens = getattr(token_usage, "candidates_token_count", 0)
            total_tokens = getattr(token_usage, "total_token_count", 0)
        else:
            prompt_tokens = 0
            completion_tokens = 0
            total_tokens = 0

        # Ë®àÁÆóÊàêÊú¨ (Gemini 3 Flash: $0.50/1M input, $3/1M output)
        input_cost = prompt_tokens * 0.00000050
        output_cost = completion_tokens * 0.000003
        total_cost = input_cost + output_cost

        # Êï¥ÁêÜÁµêÊûú
        test_result = {
            "scenario_id": scenario["id"],
            "scenario_name": scenario["name"],
            "duration_ms": duration_ms,
            "token_usage": {
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": total_tokens,
            },
            "cost_usd": {
                "input": round(input_cost, 6),
                "output": round(output_cost, 6),
                "total": round(total_cost, 6),
            },
            "response": result_data,
        }

        # È°ØÁ§∫ÁµêÊûú
        print(f"\n‚è±Ô∏è  Response Time: {duration_ms} ms")
        print(
            f"üìä Token Usage: {total_tokens} tokens (prompt: {prompt_tokens}, completion: {completion_tokens})"
        )
        print(f"üí∞ Cost: ${total_cost:.6f} USD")
        print(f"\nüéØ Safety Level: {result_data.get('safety_level', 'N/A')}")
        print(f"üìà Severity: {result_data.get('severity', 'N/A')}")
        print(f"üí¨ Display Text: {result_data.get('display_text', 'N/A')}")
        print("\nüìù Action Suggestion:")
        print(f"   {result_data.get('action_suggestion', 'N/A')}")

        # È°ØÁ§∫ detailed_scripts
        if "detailed_scripts" in result_data and result_data["detailed_scripts"]:
            print(f"\nüìñ Detailed Scripts ({len(result_data['detailed_scripts'])} ÂÄã):")
            for i, script in enumerate(result_data["detailed_scripts"], 1):
                print(f"\n   --- Script {i} ---")
                print(f"   Situation: {script.get('situation', 'N/A')}")
                print(f"   Theory Basis: {script.get('theory_basis', 'N/A')}")
                print(f"   Step: {script.get('step', 'N/A')}")
                print("\n   Parent Script:")
                parent_script = script.get("parent_script", "N/A")
                # È°ØÁ§∫Ââç 200 Â≠ó (Emergency Mode ËºÉÁü≠)
                if len(parent_script) > 200:
                    print(f"   {parent_script[:200]}...")
                    print(f"   (total {len(parent_script)} chars)")
                else:
                    print(f"   {parent_script}")
                    print(f"   ({len(parent_script)} chars)")

                print("\n   Child Likely Response:")
                print(f"   {script.get('child_likely_response', 'N/A')}")
        else:
            print("\n‚ö†Ô∏è  No detailed_scripts in response")

        # È°ØÁ§∫ÁêÜË´ñÊ°ÜÊû∂
        if (
            "theoretical_frameworks" in result_data
            and result_data["theoretical_frameworks"]
        ):
            print("\nüß† Theoretical Frameworks:")
            for theory in result_data["theoretical_frameworks"]:
                print(f"   - {theory}")
        else:
            print("\n‚ö†Ô∏è  No theoretical_frameworks in response")

        return test_result

    except Exception as e:
        print(f"‚ùå Error: {e}")
        return {
            "scenario_id": scenario["id"],
            "scenario_name": scenario["name"],
            "error": str(e),
        }


async def test_old_prompt(scenario: dict) -> dict:
    """‰ΩøÁî®Ëàä prompt Ê∏¨Ë©¶ÂñÆ‰∏ÄÂ†¥ÊôØÔºà‰ΩúÁÇ∫Â∞çÁÖßÔºâ"""
    # Ëàä prompt (Âæû keyword_analysis_service.py Line 126-171)
    old_prompt_template = """‰Ω†ÊòØË¶™Â≠êÊïôÈ§äÂ∞àÂÆ∂ÔºåÊèê‰æõË©≥Á¥∞ÊïôÂ≠∏ÊåáÂ∞é„ÄÇÈÄôÊòØ‰∫ãÂâçÁ∑¥ÁøíÊ®°ÂºèÔºåÂèØ‰ª•Êèê‰æõÊõ¥ÂÆåÊï¥ÁöÑÂàÜÊûêÂíåÂª∫Ë≠∞„ÄÇ

ËÉåÊôØË≥áË®äÔºö
{context}

ÂÆåÊï¥Â∞çË©±ÈÄêÂ≠óÁ®øÔºà‰æõÂèÉËÄÉÔºåÁêÜËß£ËÉåÊôØËÑàÁµ°ÔºâÔºö
{full_transcript}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
„ÄêÊúÄËøëÂ∞çË©± - Áî®ÊñºÂÆâÂÖ®Ë©ï‰º∞„Äë
ÔºàË´ãÊ†πÊìöÊ≠§ÂçÄÂ°äÂà§Êñ∑Áï∂ÂâçÂÆâÂÖ®Á≠âÁ¥öÔºâ
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
{transcript_segment}
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Ë´ãÂàÜÊûê‰∏¶ËøîÂõû JSON Ê†ºÂºèÔºö
{{
    "safety_level": "green|yellow|red",
    "severity": 1-3,
    "display_text": "Áµ¶ÂÆ∂Èï∑ÁöÑÊèêÁ§∫ÊñáÂ≠ó",
    "action_suggestion": "Ë©≥Á¥∞Âª∫Ë≠∞Ôºà3-4Âè•ÔºâÔºåÂåÖÂê´ Bridge ÊäÄÂ∑ßË™™Êòé",
    "suggested_interval_seconds": 30,
    "keywords": ["ÈóúÈçµË©û1", "ÈóúÈçµË©û2", "ÈóúÈçµË©û3"],
    "categories": ["È°ûÂà•1", "È°ûÂà•2"]
}}

Á¥ÖÈªÉÁ∂†ÁáàÂà§Êñ∑Ê®ôÊ∫ñÔºö
- üî¥ RED (Âö¥Èáç): Â≠©Â≠êÊÉÖÁ∑íÂ¥©ÊΩ∞„ÄÅÂÆ∂Èï∑Â§±Êéß„ÄÅË°ùÁ™ÅÂçáÁ¥ö„ÄÅË™ûË®ÄÊö¥Âäõ
- üü° YELLOW (ÈúÄË™øÊï¥): Ê∫ùÈÄö‰∏çËâØ„ÄÅÊÉÖÁ∑íÁ∑äÂºµ„ÄÅÂñÆÂêëÊåáË≤¨„ÄÅÂøΩÁï•ÊÑüÂèó
- üü¢ GREEN (ËâØÂ•Ω): Ê∫ùÈÄöÈ†ÜÊö¢„ÄÅÊÉÖÁ∑íÁ©©ÂÆö„ÄÅ‰∫íÁõ∏Â∞äÈáç„ÄÅÊúâÊïàÂÇæËÅΩ

‚ö†Ô∏è PRACTICE MODE Ë¶ÅÊ±ÇÔºö
- Êèê‰æõ 3-4 Âè•Ë©≥Á¥∞Âª∫Ë≠∞
- Ë™™Êòé Bridge ÊäÄÂ∑ßÂíåÊ∫ùÈÄöÁ≠ñÁï•
- Âπ´Âä©ÂÆ∂Èï∑ÁêÜËß£Â≠©Â≠êË°åÁÇ∫ËÉåÂæåÁöÑÈúÄÊ±Ç
- Âª∫Ë≠∞ÂÖ∑È´îÂ∞çË©±ÊñπÂºèÂíåË™øÊï¥ÊñπÊ≥ï

‚ö†Ô∏è CRITICAL: ÂÆâÂÖ®Á≠âÁ¥öË©ï‰º∞Ë´ãÂè™Ê†πÊìö„Äå„ÄêÊúÄËøëÂ∞çË©± - Áî®ÊñºÂÆâÂÖ®Ë©ï‰º∞„Äë„ÄçÂçÄÂ°äÂà§Êñ∑Ôºå
ÂÆåÊï¥Â∞çË©±ÂÉÖ‰ΩúÁÇ∫ÁêÜËß£ËÑàÁµ°ÂèÉËÄÉ„ÄÇÂ¶ÇÊûúÊúÄËøëÂ∞çË©±Â∑≤Á∑©ÂíåÔºåÂç≥‰Ωø‰πãÂâçÊúâÂç±Èö™ÂÖßÂÆπÔºå
‰πüÊáâË©ï‰º∞ÁÇ∫ËºÉ‰ΩéÈ¢®Èö™„ÄÇ

Ê≥®ÊÑèÔºö
- display_text: ÊèèËø∞Áï∂ÂâçË¶™Â≠ê‰∫íÂãïÁãÄÊ≥ÅÔºåÁµ¶ÂÆ∂Èï∑ÂÖ∑È´îÁöÑËßÄÂØüÊèêÁ§∫
- action_suggestion: ÂÖ∑È´îÂèØË°åÁöÑÊ∫ùÈÄöË™øÊï¥Âª∫Ë≠∞ÔºåÂåÖÂê´ÊïôÂ≠∏ÊÄßÂÖßÂÆπ
- severity: 1=ËºïÂæÆ, 2=‰∏≠Á≠â, 3=Âö¥Èáç
"""

    prompt = old_prompt_template.format(
        context=scenario["context"],
        full_transcript=scenario["full_transcript"],
        transcript_segment=scenario["transcript_segment"],
    )

    start_time = time.time()
    gemini_service = GeminiService()

    try:
        ai_response = await gemini_service.generate_text(
            prompt, temperature=0.3, response_format={"type": "json_object"}
        )

        end_time = time.time()
        duration_ms = int((end_time - start_time) * 1000)

        response_text = (
            ai_response.text if hasattr(ai_response, "text") else str(ai_response)
        )

        try:
            json_start = response_text.find("{")
            json_end = response_text.rfind("}") + 1
            json_str = response_text[json_start:json_end]
            result_data = json.loads(json_str)
        except json.JSONDecodeError:
            result_data = {"error": "JSON parse failed"}

        token_usage = getattr(ai_response, "usage_metadata", None)
        if token_usage:
            prompt_tokens = getattr(token_usage, "prompt_token_count", 0)
            completion_tokens = getattr(token_usage, "candidates_token_count", 0)
            total_tokens = getattr(token_usage, "total_token_count", 0)
        else:
            prompt_tokens = 0
            completion_tokens = 0
            total_tokens = 0

        total_cost = prompt_tokens * 0.00000050 + completion_tokens * 0.000003

        return {
            "scenario_id": scenario["id"],
            "scenario_name": scenario["name"],
            "duration_ms": duration_ms,
            "token_usage": {
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": total_tokens,
            },
            "cost_usd": total_cost,
            "response": result_data,
        }

    except Exception as e:
        return {
            "scenario_id": scenario["id"],
            "scenario_name": scenario["name"],
            "error": str(e),
        }


async def run_all_tests():
    """Âü∑Ë°åÊâÄÊúâÊ∏¨Ë©¶Â†¥ÊôØ"""
    print("\n" + "=" * 80)
    print("üß™ 8 Â§ßÊµÅÊ¥æ Prompt Ê∏¨Ë©¶ (Practice Mode vs Emergency Mode)")
    print("=" * 80)
    print(f"\nüìã Ê∏¨Ë©¶Â†¥ÊôØ: {len(TEST_SCENARIOS)} ÂÄã")
    print(f"üìä Practice Mode Version: {PRACTICE_METADATA['version']}")
    print(f"üìä Emergency Mode Version: {EMERGENCY_METADATA['version']}")
    print(f"üìÖ Date: {PRACTICE_METADATA['date']}")

    practice_prompt_results = []
    emergency_prompt_results = []
    old_prompt_results = []

    # Ê∏¨Ë©¶ Practice Mode prompt
    print("\n" + "=" * 80)
    print("üÜï Ê∏¨Ë©¶ Practice Mode Prompt (8 Â§ßÊµÅÊ¥æÊï¥ÂêàÁâà)")
    print("=" * 80)

    for scenario in TEST_SCENARIOS:
        result = await test_practice_prompt(scenario)
        practice_prompt_results.append(result)
        await asyncio.sleep(1)  # ÈÅøÂÖç API rate limit

    # Ê∏¨Ë©¶ Emergency Mode prompt
    print("\n\n" + "=" * 80)
    print("üö® Ê∏¨Ë©¶ Emergency Mode Prompt (8 Â§ßÊµÅÊ¥æÁ∞°ÊΩîÁâà)")
    print("=" * 80)

    for scenario in TEST_SCENARIOS:
        result = await test_emergency_prompt(scenario)
        emergency_prompt_results.append(result)
        await asyncio.sleep(1)

    # Ê∏¨Ë©¶Ëàä prompt
    print("\n\n" + "=" * 80)
    print("üìú Ê∏¨Ë©¶Ëàä Prompt (ÂéüÁâà)")
    print("=" * 80)

    for scenario in TEST_SCENARIOS:
        print(f"\nÂ†¥ÊôØ {scenario['id']}: {scenario['name']} (ËàäÁâà)")
        result = await test_old_prompt(scenario)
        old_prompt_results.append(result)
        print(
            f"‚è±Ô∏è  {result['duration_ms']} ms | üìä {result['token_usage']['total_tokens']} tokens"
        )
        await asyncio.sleep(1)

    # ÁîüÊàêÊØîËºÉÂ†±Âëä
    print("\n\n" + "=" * 80)
    print("üìä ‰∏âÁâàÊú¨ÊØîËºÉÂ†±Âëä")
    print("=" * 80)

    # Ë®àÁÆóÂπ≥ÂùáÂÄº
    practice_avg_tokens = sum(
        r["token_usage"]["total_tokens"]
        for r in practice_prompt_results
        if "token_usage" in r
    ) / len(practice_prompt_results)
    emergency_avg_tokens = sum(
        r["token_usage"]["total_tokens"]
        for r in emergency_prompt_results
        if "token_usage" in r
    ) / len(emergency_prompt_results)
    old_avg_tokens = sum(
        r["token_usage"]["total_tokens"]
        for r in old_prompt_results
        if "token_usage" in r
    ) / len(old_prompt_results)

    practice_avg_time = sum(
        r["duration_ms"] for r in practice_prompt_results if "duration_ms" in r
    ) / len(practice_prompt_results)
    emergency_avg_time = sum(
        r["duration_ms"] for r in emergency_prompt_results if "duration_ms" in r
    ) / len(emergency_prompt_results)
    old_avg_time = sum(
        r["duration_ms"] for r in old_prompt_results if "duration_ms" in r
    ) / len(old_prompt_results)

    practice_avg_cost = sum(
        r["cost_usd"]["total"] for r in practice_prompt_results if "cost_usd" in r
    ) / len(practice_prompt_results)
    emergency_avg_cost = sum(
        r["cost_usd"]["total"] for r in emergency_prompt_results if "cost_usd" in r
    ) / len(emergency_prompt_results)
    old_avg_cost = sum(
        r["cost_usd"] for r in old_prompt_results if "cost_usd" in r
    ) / len(old_prompt_results)

    print("\nüìà Âπ≥Âùá Token ‰ΩøÁî®Èáè:")
    print(f"   Practice Mode:  {practice_avg_tokens:.0f} tokens")
    if practice_avg_tokens > 0:
        print(
            f"   Emergency Mode: {emergency_avg_tokens:.0f} tokens ({((emergency_avg_tokens / practice_avg_tokens - 1) * 100):+.1f}% vs Practice)"
        )
    else:
        print(
            f"   Emergency Mode: {emergency_avg_tokens:.0f} tokens (N/A - token data unavailable)"
        )
    print(f"   ÂéüÁâà:           {old_avg_tokens:.0f} tokens")

    print("\n‚è±Ô∏è  Âπ≥Âùá Response Time:")
    print(f"   Practice Mode:  {practice_avg_time:.0f} ms")
    if practice_avg_time > 0:
        print(
            f"   Emergency Mode: {emergency_avg_time:.0f} ms ({((emergency_avg_time / practice_avg_time - 1) * 100):+.1f}% vs Practice)"
        )
    else:
        print(f"   Emergency Mode: {emergency_avg_time:.0f} ms")
    print(f"   ÂéüÁâà:           {old_avg_time:.0f} ms")

    print("\nüí∞ Âπ≥ÂùáÊàêÊú¨:")
    print(f"   Practice Mode:  ${practice_avg_cost:.6f} USD")
    if practice_avg_cost > 0:
        print(
            f"   Emergency Mode: ${emergency_avg_cost:.6f} USD ({((emergency_avg_cost / practice_avg_cost - 1) * 100):+.1f}% vs Practice)"
        )
    else:
        print(f"   Emergency Mode: ${emergency_avg_cost:.6f} USD")
    print(f"   ÂéüÁâà:           ${old_avg_cost:.6f} USD")

    # Ê™¢Êü•Êñ∞ÂäüËÉΩ (Practice Mode)
    practice_has_scripts = sum(
        1
        for r in practice_prompt_results
        if "response" in r and "detailed_scripts" in r["response"]
    )
    practice_has_theories = sum(
        1
        for r in practice_prompt_results
        if "response" in r and "theoretical_frameworks" in r["response"]
    )

    # Ê™¢Êü•Êñ∞ÂäüËÉΩ (Emergency Mode)
    emergency_has_scripts = sum(
        1
        for r in emergency_prompt_results
        if "response" in r and "detailed_scripts" in r["response"]
    )
    emergency_has_theories = sum(
        1
        for r in emergency_prompt_results
        if "response" in r and "theoretical_frameworks" in r["response"]
    )

    print("\n‚ú® Êñ∞ÂäüËÉΩË¶ÜËìãÁéá:")
    print("   Practice Mode:")
    print(
        f"     - detailed_scripts: {practice_has_scripts}/{len(practice_prompt_results)} ({practice_has_scripts/len(practice_prompt_results)*100:.0f}%)"
    )
    print(
        f"     - theoretical_frameworks: {practice_has_theories}/{len(practice_prompt_results)} ({practice_has_theories/len(practice_prompt_results)*100:.0f}%)"
    )
    print("   Emergency Mode:")
    print(
        f"     - detailed_scripts: {emergency_has_scripts}/{len(emergency_prompt_results)} ({emergency_has_scripts/len(emergency_prompt_results)*100:.0f}%)"
    )
    print(
        f"     - theoretical_frameworks: {emergency_has_theories}/{len(emergency_prompt_results)} ({emergency_has_theories/len(emergency_prompt_results)*100:.0f}%)"
    )

    # Ë®àÁÆóË©±Ë°ìÈï∑Â∫¶ÔºàEmergency Mode ÊáâË©≤Êõ¥Áü≠Ôºâ
    practice_scripts_lengths = []
    emergency_scripts_lengths = []

    for r in practice_prompt_results:
        if "response" in r and "detailed_scripts" in r["response"]:
            for script in r["response"]["detailed_scripts"]:
                if "parent_script" in script:
                    practice_scripts_lengths.append(len(script["parent_script"]))

    for r in emergency_prompt_results:
        if "response" in r and "detailed_scripts" in r["response"]:
            for script in r["response"]["detailed_scripts"]:
                if "parent_script" in script:
                    emergency_scripts_lengths.append(len(script["parent_script"]))

    if practice_scripts_lengths and emergency_scripts_lengths:
        practice_avg_script_len = sum(practice_scripts_lengths) / len(
            practice_scripts_lengths
        )
        emergency_avg_script_len = sum(emergency_scripts_lengths) / len(
            emergency_scripts_lengths
        )

        print("\nüìè Ë©±Ë°ìÈï∑Â∫¶ÊØîËºÉ:")
        print(
            f"   Practice Mode:  Âπ≥Âùá {practice_avg_script_len:.0f} Â≠ó (ÁõÆÊ®ô: 150-300 Â≠ó)"
        )
        print(
            f"   Emergency Mode: Âπ≥Âùá {emergency_avg_script_len:.0f} Â≠ó (ÁõÆÊ®ô: 100-200 Â≠ó)"
        )
        if practice_avg_script_len > 0:
            print(
                f"   Â∑ÆÁï∞: {((emergency_avg_script_len / practice_avg_script_len - 1) * 100):+.1f}%"
            )
        print(
            f"\n‚úÖ Practice Mode: {len([x for x in practice_scripts_lengths if 150 <= x <= 350])}/{len(practice_scripts_lengths)} Á¨¶ÂêàÁõÆÊ®ôÈï∑Â∫¶"
        )
        print(
            f"‚úÖ Emergency Mode: {len([x for x in emergency_scripts_lengths if 100 <= x <= 200])}/{len(emergency_scripts_lengths)} Á¨¶ÂêàÁõÆÊ®ôÈï∑Â∫¶"
        )

    # ÂÑ≤Â≠òÂÆåÊï¥ÁµêÊûú
    output_file = project_root / "scripts" / "test_8_schools_prompt_results.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(
            {
                "metadata": {
                    "practice_mode": PRACTICE_METADATA,
                    "emergency_mode": EMERGENCY_METADATA,
                },
                "test_date": time.strftime("%Y-%m-%d %H:%M:%S"),
                "scenarios": TEST_SCENARIOS,
                "practice_prompt_results": practice_prompt_results,
                "emergency_prompt_results": emergency_prompt_results,
                "old_prompt_results": old_prompt_results,
                "summary": {
                    "practice_avg_tokens": practice_avg_tokens,
                    "emergency_avg_tokens": emergency_avg_tokens,
                    "old_avg_tokens": old_avg_tokens,
                    "practice_avg_time_ms": practice_avg_time,
                    "emergency_avg_time_ms": emergency_avg_time,
                    "old_avg_time_ms": old_avg_time,
                    "practice_avg_cost_usd": practice_avg_cost,
                    "emergency_avg_cost_usd": emergency_avg_cost,
                    "old_avg_cost_usd": old_avg_cost,
                    "emergency_vs_practice": {
                        "token_reduction_pct": (
                            ((emergency_avg_tokens / practice_avg_tokens - 1) * 100)
                            if practice_avg_tokens > 0
                            else 0
                        ),
                        "time_reduction_pct": (
                            ((emergency_avg_time / practice_avg_time - 1) * 100)
                            if practice_avg_time > 0
                            else 0
                        ),
                        "cost_reduction_pct": (
                            ((emergency_avg_cost / practice_avg_cost - 1) * 100)
                            if practice_avg_cost > 0
                            else 0
                        ),
                    },
                    "practice_mode_coverage": {
                        "detailed_scripts": practice_has_scripts
                        / len(practice_prompt_results),
                        "theories": practice_has_theories
                        / len(practice_prompt_results),
                    },
                    "emergency_mode_coverage": {
                        "detailed_scripts": emergency_has_scripts
                        / len(emergency_prompt_results),
                        "theories": emergency_has_theories
                        / len(emergency_prompt_results),
                    },
                    "script_lengths": {
                        "practice_avg": (
                            sum(practice_scripts_lengths)
                            / len(practice_scripts_lengths)
                            if practice_scripts_lengths
                            else 0
                        ),
                        "emergency_avg": (
                            sum(emergency_scripts_lengths)
                            / len(emergency_scripts_lengths)
                            if emergency_scripts_lengths
                            else 0
                        ),
                    },
                },
            },
            f,
            ensure_ascii=False,
            indent=2,
        )

    print(f"\nüíæ ÂÆåÊï¥ÁµêÊûúÂ∑≤ÂÑ≤Â≠òËá≥: {output_file}")

    print("\n" + "=" * 80)
    print("‚úÖ Ê∏¨Ë©¶ÂÆåÊàê")
    print("=" * 80)


# ==============================================================================
# Main Entry Point
# ==============================================================================

if __name__ == "__main__":
    asyncio.run(run_all_tests())
