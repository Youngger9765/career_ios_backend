#!/usr/bin/env python3
"""
æ¸¬è©¦ç­–ç•¥ Aï¼šæ¨¡æ“¬é‡å»º cacheï¼ˆæ¯æ¬¡ç™¼é€å®Œæ•´ç´¯ç©å°è©±ï¼‰

é€é Staging API æ¸¬è©¦ï¼Œè®“ Gemini çš„ implicit caching è™•ç†ã€‚
ç”±æ–¼æˆ‘å€‘ç„¡æ³•ç›´æ¥æ§åˆ¶ cache å‰µå»º/åˆªé™¤ï¼Œé€™å€‹æ¸¬è©¦ä¸»è¦è§€å¯Ÿï¼š
- æ¯æ¬¡éƒ½ç™¼é€å®Œæ•´ç´¯ç©å°è©±
- è§€å¯Ÿ Gemini å¦‚ä½•è‡ªå‹•è™•ç† caching

å¯¦éš›å ´æ™¯ï¼š
- ç¬¬ 1 åˆ†é˜ï¼šç™¼é€ "å°è©±1"
- ç¬¬ 2 åˆ†é˜ï¼šç™¼é€ "å°è©±1+2" (å®Œæ•´)
- ç¬¬ 3 åˆ†é˜ï¼šç™¼é€ "å°è©±1+2+3" (å®Œæ•´)
"""

import asyncio
import json
import time
from datetime import datetime

import httpx

# API Configuration
API_BASE_URL = "https://career-app-api-staging-kxaznpplqq-uc.a.run.app/api/v1"
REALTIME_ENDPOINT = f"{API_BASE_URL}/realtime/analyze"

# æ¸¬è©¦å°è©± - 10 åˆ†é˜
CONVERSATION = {
    1: """è«®è©¢å¸«ï¼šä»Šå¤©æƒ³èŠäº›ä»€éº¼å‘¢ï¼Ÿ
å®¶é•·ï¼šæœ€è¿‘è·Ÿå­©å­çš„é—œä¿‚è®Šå¾—å¾ˆç·Šå¼µã€‚ä»–åœ¨å­¸æ ¡ç¸½æ˜¯è·ŸåŒå­¸èµ·è¡çªï¼Œè€å¸«ä¹Ÿåæ˜ ä»–ä¸Šèª²ä¸å°ˆå¿ƒã€‚å›åˆ°å®¶æˆ‘æƒ³è·Ÿä»–èŠèŠï¼Œä»–å°±å¾ˆä¸è€ç…©ã€‚""",
    2: """è«®è©¢å¸«ï¼šä½ å‰›æåˆ°ä»–æœƒä¸è€ç…©ï¼Œé‚£æ™‚å€™ä½ çš„æ„Ÿå—æ˜¯ä»€éº¼ï¼Ÿ
å®¶é•·ï¼šæˆ‘è¦ºå¾—å¾ˆå—å‚·ï¼Œä¹Ÿæœ‰é»ç”Ÿæ°£ã€‚æˆ‘æ˜æ˜æ˜¯é—œå¿ƒä»–ï¼Œä»–ç‚ºä»€éº¼è¦é€™æ¨£å°æˆ‘ï¼Ÿæˆ‘èŠ±é€™éº¼å¤šæ™‚é–“ã€ç²¾åŠ›åœ¨ä»–èº«ä¸Šï¼Œä»–å»å®Œå…¨ä¸é ˜æƒ…ã€‚""",
    3: """è«®è©¢å¸«ï¼šé‚£ä½ é€šå¸¸æœƒæ€éº¼å›æ‡‰ä»–çš„ä¸è€ç…©å‘¢ï¼Ÿ
å®¶é•·ï¼šæˆ‘æœƒå¿ä¸ä½èªªã€Œä½ é€™æ˜¯ä»€éº¼æ…‹åº¦ï¼Ÿæˆ‘æ˜¯ä½ åª½åª½è€¶ï¼ã€ç„¶å¾Œæˆ‘å€‘å°±æœƒåµèµ·ä¾†ã€‚åµå®Œä¹‹å¾Œæˆ‘åˆå¾ˆå¾Œæ‚”ï¼Œè¦ºå¾—è‡ªå·±å¤ªè¡å‹•äº†ã€‚""",
    4: """è«®è©¢å¸«ï¼šè½èµ·ä¾†ä½ å€‘çš„äº’å‹•é™·å…¥äº†ä¸€å€‹å¾ªç’°ã€‚ä½ èªªåµå®Œæœƒå¾Œæ‚”ï¼Œé‚£æ™‚å€™ä½ åœ¨æƒ³ä»€éº¼ï¼Ÿ
å®¶é•·ï¼šæˆ‘æœƒè¦ºå¾—è‡ªå·±å¾ˆå¤±æ•—ï¼Œç‚ºä»€éº¼æˆ‘é€£è‡ªå·±çš„å­©å­éƒ½ç®¡ä¸å¥½ï¼Ÿæœ‰æ™‚å€™æˆ‘çœŸçš„å¿«ç˜‹äº†ï¼Œå¾ˆæƒ³å°ä»–å¤§å¼ï¼Œç”šè‡³æƒ³æ‰“ä»–ä¸€é “ã€‚ä½†æˆ‘çŸ¥é“é€™æ¨£ä¸å°ã€‚""",
    5: """è«®è©¢å¸«ï¼šä½ èƒ½å¦æ‰¿é€™äº›æ„Ÿå—å¾ˆä¸å®¹æ˜“ã€‚é€™äº›å¿µé ­å‡ºç¾çš„æ™‚å€™ï¼Œä½ æœƒæ€éº¼åšï¼Ÿ
å®¶é•·ï¼šæˆ‘æœƒç›¡é‡å¿ä½ï¼Œä½†æœ‰æ™‚å€™çœŸçš„å¿ä¸äº†ã€‚æˆ‘çŸ¥é“é€™æ¨£ä¸å°ï¼Œä½†æˆ‘çœŸçš„ä¸çŸ¥é“è©²æ€éº¼è¾¦äº†ã€‚æˆ‘è¦ºå¾—è‡ªå·±æ˜¯å€‹å¾ˆç³Ÿç³•çš„çˆ¶æ¯ã€‚""",
    6: """è«®è©¢å¸«ï¼šå¾ä½ çš„æè¿°è½èµ·ä¾†ï¼Œä½ æ‰¿å—äº†å¾ˆå¤§çš„å£“åŠ›ã€‚ä½ æœ‰å˜—è©¦éå…¶ä»–æ–¹å¼å—ï¼Ÿ
å®¶é•·ï¼šæˆ‘æœ‰è©¦éè·Ÿä»–å¥½å¥½è¬›ï¼Œä½†ä»–æ ¹æœ¬ä¸è½ã€‚æˆ‘ä¹Ÿè©¦éçµ¦ä»–ä¸€äº›ç©ºé–“ï¼Œä½†ä»–å°±é—œåœ¨æˆ¿é–“è£¡æ‰“é›»å‹•ã€‚æˆ‘çœŸçš„ä¸çŸ¥é“è¦æ€éº¼è·Ÿä»–æºé€šã€‚""",
    7: """è«®è©¢å¸«ï¼šä½ å‰›æåˆ°ä»–æœƒé—œåœ¨æˆ¿é–“æ‰“é›»å‹•ï¼Œé‚£æ™‚å€™ä½ æœƒæ“”å¿ƒä»€éº¼ï¼Ÿ
å®¶é•·ï¼šæˆ‘æ“”å¿ƒä»–æœƒæ²‰è¿·éŠæˆ²ï¼Œè’å»¢å­¸æ¥­ã€‚è€Œä¸”ä»–éƒ½ä¸è·Ÿæˆ‘å€‘äº’å‹•ï¼Œæˆ‘ä¸çŸ¥é“ä»–åœ¨æƒ³ä»€éº¼ã€‚æˆ‘æ€•ä»–æ˜¯ä¸æ˜¯å¿ƒè£¡æœ‰ä»€éº¼å•é¡Œï¼Œä½†ä»–åˆä¸é¡˜æ„è·Ÿæˆ‘èªªã€‚""",
    8: """è«®è©¢å¸«ï¼šä½ æœ‰æ²’æœ‰æƒ³éï¼Œä»–å¯èƒ½ä¹Ÿæœ‰ä»–è‡ªå·±çš„å£“åŠ›å’Œå›°æ“¾ï¼Ÿ
å®¶é•·ï¼šæˆ‘ç•¶ç„¶çŸ¥é“ä»–ä¹Ÿæœ‰å£“åŠ›ï¼Œä½†ä»–éƒ½ä¸è·Ÿæˆ‘èªªå•Šï¼æˆ‘å•ä»–ç™¼ç”Ÿä»€éº¼äº‹ï¼Œä»–å°±èªªæ²’äº‹ã€‚æˆ‘çœŸçš„å¾ˆæƒ³å¹«ä»–ï¼Œä½†ä»–æŠŠæˆ‘æ¨å¾—é é çš„ã€‚""",
    9: """è«®è©¢å¸«ï¼šè½èµ·ä¾†ä½ å€‘ä¹‹é–“å¥½åƒæœ‰ä¸€é“ç‰†ã€‚ä½ è¦ºå¾—é€™é“ç‰†æ˜¯ä»€éº¼ï¼Ÿ
å®¶é•·ï¼šæˆ‘ä¹Ÿä¸çŸ¥é“ã€‚å¯èƒ½æ˜¯æˆ‘å¤ªåš´æ ¼äº†ï¼Ÿé‚„æ˜¯æˆ‘å¤ªå›‰å—¦äº†ï¼Ÿæˆ‘çœŸçš„ä¸çŸ¥é“æ€éº¼åšæ‰å°ã€‚æˆ‘åªæ˜¯æƒ³ç•¶ä¸€å€‹å¥½åª½åª½ï¼Œä½†æˆ‘å¥½åƒè¶ŠåŠªåŠ›è¶Šç³Ÿç³•ã€‚""",
    10: """è«®è©¢å¸«ï¼šä½ å‰›æ‰èªªã€Œè¶ŠåŠªåŠ›è¶Šç³Ÿç³•ã€ï¼Œé€™å¥è©±èƒŒå¾Œæœ‰ä»€éº¼æ„Ÿå—ï¼Ÿ
å®¶é•·ï¼šæˆ‘è¦ºå¾—å¾ˆç„¡åŠ›ã€å¾ˆæŒ«æŠ˜ã€‚æˆ‘èŠ±äº†é€™éº¼å¤šæ™‚é–“æƒ³è¦æ”¹å–„é—œä¿‚ï¼Œä½†å¥½åƒæ²’æœ‰ä»»ä½•é€²å±•ã€‚æœ‰æ™‚å€™æˆ‘ç”šè‡³åœ¨æƒ³ï¼Œæ˜¯ä¸æ˜¯æˆ‘å°±æ˜¯å€‹å¤±æ•—çš„çˆ¶æ¯ï¼Ÿ""",
}


async def analyze_cumulative(minute: int):
    """
    ç™¼é€ç´¯ç©å°è©±ï¼ˆç¬¬ 1-minute åˆ†é˜ï¼‰
    æ¨¡æ“¬ã€Œé‡å»º cacheã€ï¼šæ¯æ¬¡éƒ½ç™¼é€å®Œæ•´çš„ç´¯ç©å…§å®¹
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“ ç¬¬ {minute} åˆ†é˜ - ç­–ç•¥ Aï¼ˆå®Œæ•´ç´¯ç©å°è©±ï¼‰")
    print(f"{'='*80}")

    # çµ„åˆç´¯ç©å°è©±
    accumulated = "\n\n".join([CONVERSATION[m] for m in range(1, minute + 1)])
    transcript_chars = len(accumulated)
    estimated_tokens = int(transcript_chars * 0.4)  # ç¹é«”ä¸­æ–‡ç´„ 0.4 tokens/char

    print(f"ç´¯ç©å°è©±: ç¬¬ 1-{minute} åˆ†é˜")
    print(f"å­—ç¬¦æ•¸: {transcript_chars}")
    print(f"ä¼°ç®— tokens: {estimated_tokens}")

    # ç™¼é€è«‹æ±‚
    request_data = {
        "transcript": accumulated,
        "speakers": [],
        "time_range": f"0:00-{minute}:00",
    }

    start_time = time.time()

    try:
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                REALTIME_ENDPOINT,
                json=request_data,
                headers={"Content-Type": "application/json"},
            )

            elapsed_time = time.time() - start_time

            if response.status_code == 200:
                result = response.json()
                usage = result.get("usage_metadata", {})

                cached_tokens = usage.get("cached_content_token_count", 0)
                prompt_tokens = usage.get("prompt_token_count", 0)
                output_tokens = usage.get("candidates_token_count", 0)

                cache_ratio = (
                    cached_tokens / (cached_tokens + prompt_tokens) * 100
                    if (cached_tokens + prompt_tokens) > 0
                    else 0
                )

                print(f"âœ… æˆåŠŸ ({elapsed_time:.2f}s)")
                print(f"ğŸ¯ Cached: {cached_tokens} tokens")
                print(f"ğŸ“ Prompt: {prompt_tokens} tokens")
                print(f"ğŸ’¬ Output: {output_tokens} tokens")
                print(f"ğŸ“Š Cache æ¯”ä¾‹: {cache_ratio:.1f}%")

                return {
                    "minute": minute,
                    "success": True,
                    "transcript_chars": transcript_chars,
                    "cached_tokens": cached_tokens,
                    "prompt_tokens": prompt_tokens,
                    "output_tokens": output_tokens,
                    "response_time": round(elapsed_time, 2),
                    "cache_ratio": round(cache_ratio, 1),
                    "strategy": "A - Full Cumulative",
                }
            else:
                print(f"âŒ å¤±æ•—: HTTP {response.status_code}")
                print(f"éŒ¯èª¤: {response.text}")
                return {
                    "minute": minute,
                    "success": False,
                    "error": f"HTTP {response.status_code}",
                }

    except Exception as e:
        elapsed_time = time.time() - start_time
        print(f"âŒ ç•°å¸¸: {str(e)}")
        return {
            "minute": minute,
            "success": False,
            "error": str(e),
            "response_time": round(elapsed_time, 2),
        }


async def main():
    print("=" * 80)
    print("ğŸ§ª æ¸¬è©¦ç­–ç•¥ Aï¼šå®Œæ•´ç´¯ç©å°è©±ï¼ˆæ¨¡æ“¬é‡å»º cacheï¼‰")
    print("=" * 80)
    print(f"æ¸¬è©¦æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"API: {REALTIME_ENDPOINT}")
    print("=" * 80)

    results = []

    # æ¸¬è©¦ 10 åˆ†é˜
    for minute in range(1, 11):
        result = await analyze_cumulative(minute)
        results.append(result)

        # çŸ­æš«å»¶é²ï¼Œè®“ implicit cache ç”Ÿæ•ˆ
        if minute < 10:
            await asyncio.sleep(0.1)

    # è¨ˆç®—çµ±è¨ˆ
    successful = [r for r in results if r.get("success")]

    if successful:
        total_cached = sum(r["cached_tokens"] for r in successful)
        total_prompt = sum(r["prompt_tokens"] for r in successful)
        total_output = sum(r["output_tokens"] for r in successful)
        total_time = sum(r["response_time"] for r in successful)

        avg_cache_ratio = (
            total_cached / (total_cached + total_prompt) * 100
            if (total_cached + total_prompt) > 0
            else 0
        )

        print("\n" + "=" * 80)
        print("ğŸ“Š ç­–ç•¥ A æ¸¬è©¦çµæœæ‘˜è¦")
        print("=" * 80)

        for r in successful:
            print(f"\nç¬¬ {r['minute']} åˆ†é˜:")
            print(f"  ğŸ¯ Cached: {r['cached_tokens']} tokens")
            print(f"  ğŸ“ Prompt: {r['prompt_tokens']} tokens")
            print(f"  ğŸ’¬ Output: {r['output_tokens']} tokens")
            print(f"  ğŸ“Š Cache æ¯”ä¾‹: {r['cache_ratio']}%")
            print(f"  â±ï¸  éŸ¿æ‡‰æ™‚é–“: {r['response_time']}s")

        print("\n" + "=" * 80)
        print("ğŸ“ˆ ç¸½é«”çµ±è¨ˆ")
        print("=" * 80)
        print(f"æ¸¬è©¦æ¬¡æ•¸: {len(successful)}")
        print(f"ç¸½ Cached tokens: {total_cached:,}")
        print(f"ç¸½ Prompt tokens: {total_prompt:,}")
        print(f"ç¸½ Output tokens: {total_output:,}")
        print(f"ç¸½è¼¸å…¥ tokens: {total_cached + total_prompt:,}")
        print(f"å¹³å‡ Cache å‘½ä¸­ç‡: {avg_cache_ratio:.1f}%")
        print(f"ç¸½éŸ¿æ‡‰æ™‚é–“: {total_time:.2f}s")

        # å„²å­˜çµæœ
        output_file = "strategy_a_results.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "timestamp": datetime.now().isoformat(),
                    "strategy": "A - Full Cumulative (Simulated Rebuild Cache)",
                    "description": "æ¯æ¬¡ç™¼é€å®Œæ•´ç´¯ç©å°è©±ï¼Œæ¨¡æ“¬é‡å»º cache",
                    "results": results,
                    "summary": {
                        "test_count": len(successful),
                        "total_cached_tokens": total_cached,
                        "total_prompt_tokens": total_prompt,
                        "total_output_tokens": total_output,
                        "total_input_tokens": total_cached + total_prompt,
                        "avg_cache_hit_ratio": round(avg_cache_ratio, 2),
                        "total_response_time": round(total_time, 2),
                        "total_time": round(total_time, 2),
                        "total_cache_creation_time": 0,  # N/A for implicit caching
                    },
                },
                f,
                ensure_ascii=False,
                indent=2,
            )

        print(f"\nâœ… çµæœå·²ä¿å­˜: {output_file}")
        print("=" * 80)
    else:
        print("\nâŒ æ¸¬è©¦å¤±æ•—ï¼Œç„¡æˆåŠŸçµæœ")


if __name__ == "__main__":
    asyncio.run(main())
