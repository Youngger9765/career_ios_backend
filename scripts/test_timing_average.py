#!/usr/bin/env python3
"""
å¤šæ¬¡æ¸¬è©¦å–å¹³å‡å€¼ - æ‰¾å‡ºçœŸå¯¦çš„æ™‚é–“åˆ†å¸ƒ
"""

import asyncio
import os
import sys
import time

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.core.database import SessionLocal  # noqa: E402
from app.models.case import Case  # noqa: E402
from app.models.client import Client  # noqa: E402
from app.models.session import Session as SessionModel  # noqa: E402
from app.services.gemini_service import GeminiService  # noqa: E402
from app.services.openai_service import OpenAIService  # noqa: E402
from app.services.rag_retriever import RAGRetriever  # noqa: E402


async def test_single_run(
    gemini, rag_retriever, db, session, client, case, test_transcript, run_num
):
    """å–®æ¬¡æ¸¬è©¦"""
    print(f"\n   ğŸ”„ ç¬¬ {run_num} æ¬¡æ¸¬è©¦:")

    total_start = time.time()
    timings = {}

    # Context + Prompt
    context_str = _build_context(session, client, case)
    full_transcript = session.transcript_text or "ï¼ˆå°šç„¡å®Œæ•´é€å­—ç¨¿ï¼‰"
    prompt_template = """ä½ æ˜¯è¦ªå­æ•™é¤Šå°ˆå®¶ã€‚

èƒŒæ™¯: {context}
å®Œæ•´å°è©±: {full_transcript}
æœ€è¿‘å°è©±: {transcript_segment}

JSON: {{"safety_level": "green|yellow|red", "severity": 1-3}}
"""
    prompt = prompt_template.format(
        context=context_str,
        full_transcript=full_transcript,
        transcript_segment=test_transcript[:500],
    )

    # Gemini API
    gemini_start = time.time()
    await gemini.generate_text(
        prompt, temperature=0.3, response_format={"type": "json_object"}
    )
    timings["gemini"] = (time.time() - gemini_start) * 1000

    # RAG æª¢ç´¢
    rag_start = time.time()
    try:
        await rag_retriever.search(
            query=test_transcript[:200],
            top_k=3,
            threshold=0.7,
            db=db,
            category="parenting",
        )
        timings["rag"] = (time.time() - rag_start) * 1000
    except Exception:
        timings["rag"] = (time.time() - rag_start) * 1000

    timings["total"] = (time.time() - total_start) * 1000

    print(f"      Gemini: {timings['gemini']:.0f} ms ({timings['gemini']/1000:.2f}s)")
    print(f"      RAG:    {timings['rag']:.0f} ms ({timings['rag']/1000:.2f}s)")
    print(f"      ç¸½è¨ˆ:   {timings['total']:.0f} ms ({timings['total']/1000:.2f}s)")

    return timings


async def test_timing_average():
    """æ¸¬è©¦å¤šæ¬¡å–å¹³å‡"""
    db = SessionLocal()

    try:
        print("=" * 60)
        print("ğŸ“Š å¤šæ¬¡æ¸¬è©¦å–å¹³å‡å€¼")
        print("=" * 60)

        # æº–å‚™æ•¸æ“š
        session = (
            db.query(SessionModel).filter(SessionModel.tenant_id == "island").first()
        )
        case = db.query(Case).filter(Case.id == session.case_id).first()
        client = db.query(Client).filter(Client.id == case.client_id).first()

        test_transcript = """
        å®¶é•·ï¼šä½ ä»Šå¤©åœ¨å­¸æ ¡éå¾—æ€éº¼æ¨£ï¼Ÿ
        å­©å­ï¼šé‚„å¥½å•Š
        å®¶é•·ï¼šæœ‰ä»€éº¼é–‹å¿ƒçš„äº‹å—ï¼Ÿ
        å­©å­ï¼šæ²’æœ‰
        """

        # åˆå§‹åŒ–æœå‹™
        gemini = GeminiService()
        openai_service = OpenAIService()
        rag_retriever = RAGRetriever(openai_service)

        # æ¸¬è©¦ 5 æ¬¡
        print("\nğŸš€ é–‹å§‹æ¸¬è©¦ï¼ˆ5 æ¬¡ï¼‰")
        print("-" * 60)

        all_results = []
        for i in range(5):
            result = await test_single_run(
                gemini, rag_retriever, db, session, client, case, test_transcript, i + 1
            )
            all_results.append(result)

            # ç­‰å¾… 2 ç§’é¿å… rate limiting
            if i < 4:
                await asyncio.sleep(2)

        # çµ±è¨ˆ
        print()
        print("=" * 60)
        print("ğŸ“Š çµ±è¨ˆçµæœ")
        print("=" * 60)
        print()

        gemini_times = [r["gemini"] for r in all_results]
        rag_times = [r["rag"] for r in all_results]
        total_times = [r["total"] for r in all_results]

        print("Gemini API æ™‚é–“:")
        print(f"   æœ€å¿«: {min(gemini_times)/1000:.2f}s ({min(gemini_times):.0f} ms)")
        print(f"   æœ€æ…¢: {max(gemini_times)/1000:.2f}s ({max(gemini_times):.0f} ms)")
        print(
            f"   å¹³å‡: {sum(gemini_times)/len(gemini_times)/1000:.2f}s ({sum(gemini_times)/len(gemini_times):.0f} ms)"
        )
        print()

        print("RAG æª¢ç´¢æ™‚é–“:")
        print(f"   æœ€å¿«: {min(rag_times)/1000:.2f}s ({min(rag_times):.0f} ms)")
        print(f"   æœ€æ…¢: {max(rag_times)/1000:.2f}s ({max(rag_times):.0f} ms)")
        print(
            f"   å¹³å‡: {sum(rag_times)/len(rag_times)/1000:.2f}s ({sum(rag_times)/len(rag_times):.0f} ms)"
        )
        print()

        print("ç¸½æ™‚é–“:")
        print(f"   æœ€å¿«: {min(total_times)/1000:.2f}s ({min(total_times):.0f} ms)")
        print(f"   æœ€æ…¢: {max(total_times)/1000:.2f}s ({max(total_times):.0f} ms)")
        print(
            f"   å¹³å‡: {sum(total_times)/len(total_times)/1000:.2f}s ({sum(total_times)/len(total_times):.0f} ms)"
        )
        print()

        # é—œéµçµè«–
        avg_total = sum(total_times) / len(total_times) / 1000
        avg_gemini = sum(gemini_times) / len(gemini_times) / 1000
        avg_rag = sum(rag_times) / len(rag_times) / 1000

        print("ğŸ¯ é—œéµçµè«–:")
        print(f"   âœ… å¹³å‡ç¸½æ™‚é–“: {avg_total:.2f}s")
        print(f"   â”œâ”€ Gemini API: {avg_gemini:.2f}s ({avg_gemini/avg_total*100:.1f}%)")
        print(f"   â””â”€ RAG æª¢ç´¢:   {avg_rag:.2f}s ({avg_rag/avg_total*100:.1f}%)")
        print()
        print(
            f"   âš ï¸  Gemini è®Šç•°ç¯„åœ: {min(gemini_times)/1000:.2f}s - {max(gemini_times)/1000:.2f}s"
        )
        print(
            f"        è®Šç•°å¹…åº¦: Â±{(max(gemini_times) - min(gemini_times))/2/1000:.2f}s"
        )

    finally:
        db.close()


def _build_context(session, client, case) -> str:
    context_parts = []
    context_parts.append(f"æ¡ˆä¸»: {client.name}")
    context_parts.append(f"ç›®æ¨™: {case.goals or 'æœªè¨­å®š'}")
    context_parts.append(f"æœƒè«‡: ç¬¬ {session.session_number} æ¬¡")
    return "\n".join(context_parts)


if __name__ == "__main__":
    asyncio.run(test_timing_average())
