"""API endpoints for RAG-powered chat"""

from typing import List, Optional

from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel
from sqlalchemy import Float, Integer, String, bindparam, text
from sqlalchemy.ext.asyncio import AsyncSession

from app.database import get_db
from app.services.openai_service import OpenAIService

router = APIRouter(prefix="/api/rag/chat", tags=["rag-chat"])


class ChatRequest(BaseModel):
    question: str
    top_k: int = 7
    similarity_threshold: float = 0.45
    system_prompt: Optional[str] = None
    temperature: float = 0.6
    chunk_strategy: Optional[str] = None  # NEW: filter by chunk strategy


class Citation(BaseModel):
    chunk_id: int
    doc_id: int
    document_title: str
    text: str
    similarity_score: float


class ChatResponse(BaseModel):
    question: str
    answer: str
    citations: List[Citation]
    total_citations: int


@router.post("/", response_model=ChatResponse)
async def chat_with_rag(request: ChatRequest, db: AsyncSession = Depends(get_db)):
    """
    Intelligent chat with optional RAG (Retrieval-Augmented Generation)

    Process:
    1. Determine if question needs document search
    2. If yes: Search and answer with context
    3. If no: Direct answer or guide user about available content

    Args:
        request: Chat question and parameters
        db: Database session

    Returns:
        ChatResponse with answer and citations
    """

    try:
        # Initialize OpenAI service
        openai_service = OpenAIService()

        # Step 0.5: Get available documents from database
        from sqlalchemy import select
        from app.models.document import Document

        result = db.execute(select(Document.title).distinct())
        doc_titles = [row[0] for row in result.fetchall()]

        # Format document list for prompt
        doc_list = "\n".join([f"- {title}" for title in sorted(set(doc_titles))])

        # Step 1: Determine if question needs RAG search with improved prompt
        intent_system_prompt = f"""You are a classifier for a career counseling AI assistant.

Your job: Determine if the question needs to search our professional documents.

ğŸ“š **Available documents in our database:**
{doc_list}

**Document coverage includes:**
- è·æ¶¯è«®è©¢æ¦‚è«–èˆ‡èˆˆè¶£ç†±æƒ… (Career counseling fundamentals & passion exploration)
- å„ªå‹¢è·èƒ½åˆ†æ (Strengths & competency analysis)
- ç”Ÿæ¶¯æˆç†Ÿèˆ‡åƒ¹å€¼è§€ (Career maturity & values)
- æ±‚è·ç­–ç•¥ã€å±¥æ­·èˆ‡é¢è©¦æŠ€å·§ (Job search strategies, resume & interview skills)
- å¿ƒç†è«®è©¢æŠ€å·§ (Psychological counseling techniques)
- ç¶œåˆè·æ¶¯å¯¦æˆ°éŒ¦å›Š (Comprehensive career practice toolkit)
- ä¸»äººæ€ç¶­ (Owner mindset and proactive thinking)
- è·éŠç²¾é¸æ–‡ç«  (Curated career development articles)

Reply ONLY with JSON:
{{"needs_search": true/false, "reason": "brief explanation"}}

âœ… needs_search = TRUE for:
- Career-related questions (è·æ¶¯ã€å·¥ä½œã€æ±‚è·ã€é¢è©¦ã€å±¥æ­·)
- Personal development questions (æˆé•·ã€ç™¼å±•ã€ç›®æ¨™ã€è¦åŠƒã€èˆˆè¶£ã€ç†±æƒ…)
- Life purpose questions (äººç”Ÿæ„ç¾©ã€åƒ¹å€¼è§€ã€æƒ³è¦çš„ç”Ÿæ´»)
- Career confusion or exploration (è¿·èŒ«ã€å›°æƒ‘ã€é¸æ“‡ã€æ¢ç´¢)
- Skill or competency questions (èƒ½åŠ›ã€å„ªå‹¢ã€å°ˆé•·ã€æŠ€èƒ½)
- Work-life balance (å·¥ä½œç”Ÿæ´»å¹³è¡¡ã€å£“åŠ›)
- Career transitions (è½‰è·ã€æ›å·¥ä½œã€è·æ¶¯è½‰æ›)
- Mindset questions (æ€ç¶­ã€å¿ƒæ…‹ã€ä¸»äººæ€ç¶­)
- Any question that MIGHT relate to career counseling or personal development
- **Any question that mentions topics in our document titles**

âŒ needs_search = FALSE ONLY for:
- Pure greetings with no question (åªæ˜¯ã€Œä½ å¥½ã€ã€Œhiã€ã€Œhelloã€)
- System commands (é‡ç½®ã€æ¸…é™¤ã€è¨­å®š)
- Completely unrelated topics (å¤©æ°£ã€æ•¸å­¸è¨ˆç®—ã€å¨›æ¨‚å…«å¦ã€ä»Šå¤©åƒä»€éº¼)

ğŸ”‘ Key principle: When in doubt, choose TRUE.
Career counseling is broad - almost any life question can relate to career.

Examples:
- "æˆ‘æƒ³è¦æ´»å¾—æ›´å¥½" â†’ TRUE (relates to life purpose & career direction)
- "å› ç‚ºæƒ³è¦æ´»å¾—æ›´å¥½" â†’ TRUE (implies career motivation)
- "å¦‚ä½•æ‰¾åˆ°ç†±æƒ…" â†’ TRUE (passion exploration)
- "æˆ‘å¾ˆè¿·èŒ«" â†’ TRUE (career confusion)
- "ä¸»äººæ€ç¶­æ˜¯ä»€éº¼" â†’ TRUE (we have ä¸»äººæ€ç¶­å…¨.pdf)
- "ä½ å¥½" â†’ FALSE (just greeting)
- "ä»Šå¤©å¤©æ°£å¦‚ä½•" â†’ FALSE (weather)
- "1+1ç­‰æ–¼å¤šå°‘" â†’ FALSE (math calculation)"""

        intent_check = await openai_service.chat_completion(
            messages=[
                {"role": "system", "content": intent_system_prompt},
                {"role": "user", "content": request.question},
            ],
            temperature=0.2,  # Lower temperature for more consistent classification
        )

        # Parse intent
        import json
        import re

        try:
            intent_data = json.loads(intent_check)
        except json.JSONDecodeError:
            json_match = re.search(r"\{.*\}", intent_check, re.DOTALL)
            if json_match:
                intent_data = json.loads(json_match.group(0))
            else:
                # Default to search if parsing fails
                intent_data = {"needs_search": True, "reason": "default to search on parse error"}

        needs_search = intent_data.get("needs_search", True)  # Default TRUE

        # If doesn't need search, respond directly
        if not needs_search:
            system_prompt = request.system_prompt or (
                "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è·æ¶¯è«®è©¢åŠ©ç†ã€‚æ ¹æ“šå•é¡Œé¡å‹å›ç­”ï¼š\n\n"
                "å¦‚æœæ˜¯æ‰“æ‹›å‘¼ï¼šå‹å–„å›æ‡‰ï¼Œä¸¦ä»‹ç´¹ä½ å¯ä»¥å¹«åŠ©çš„å…§å®¹\n"
                "å¦‚æœæ˜¯é–’èŠï¼šç°¡çŸ­å›æ‡‰ï¼Œå¼•å°å›åˆ°è·æ¶¯ç›¸é—œè©±é¡Œ\n"
                "å¦‚æœè¶…å‡ºç¯„åœï¼šèªªæ˜ä½ çš„å°ˆæ¥­ç¯„åœåœ¨è·æ¶¯è«®è©¢ï¼Œè³‡æ–™åº«åŒ…å«ä»¥ä¸‹ä¸»é¡Œï¼š\n"
                "- è·æ¶¯è«®è©¢æ¦‚è«–èˆ‡èˆˆè¶£ç†±æƒ…\n"
                "- å„ªå‹¢è·èƒ½åˆ†æ\n"
                "- ç”Ÿæ¶¯æˆç†Ÿèˆ‡åƒ¹å€¼è§€\n"
                "- æ±‚è·ç­–ç•¥ã€å±¥æ­·èˆ‡é¢è©¦æŠ€å·§\n"
                "- å¿ƒç†è«®è©¢æŠ€å·§\n"
                "- ç¶œåˆè·æ¶¯å¯¦æˆ°éŒ¦å›Š\n\n"
                "ä½¿ç”¨èˆ‡å•é¡Œç›¸åŒçš„èªè¨€å›ç­”ï¼ˆç¹é«”ä¸­æ–‡æˆ–è‹±æ–‡ï¼‰"
            )

            answer = await openai_service.chat_completion(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": request.question},
                ],
                temperature=request.temperature,
            )

            return ChatResponse(
                question=request.question, answer=answer, citations=[], total_citations=0
            )

        # Step 2: Perform RAG search
        query_embedding = await openai_service.create_embedding(request.question)
        embedding_str = "[" + ",".join(map(str, query_embedding)) + "]"

        # Build SQL query based on chunk_strategy filter
        params = {
            "query_embedding": embedding_str,
            "threshold": request.similarity_threshold,
            "top_k": request.top_k,
        }

        if request.chunk_strategy:
            # Query with strategy filter
            query_sql = text(
                """
                SELECT
                    c.id as chunk_id,
                    c.doc_id,
                    c.text,
                    d.title as document_title,
                    1 - (e.embedding <=> CAST(:query_embedding AS vector)) as similarity_score
                FROM chunks c
                JOIN embeddings e ON c.id = e.chunk_id
                JOIN documents d ON c.doc_id = d.id
                WHERE 1 - (e.embedding <=> CAST(:query_embedding AS vector)) >= :threshold
                    AND c.chunk_strategy = :chunk_strategy
                ORDER BY e.embedding <=> CAST(:query_embedding AS vector)
                LIMIT :top_k
            """
            ).bindparams(
                bindparam("query_embedding", type_=String),
                bindparam("threshold", type_=Float),
                bindparam("top_k", type_=Integer),
                bindparam("chunk_strategy", type_=String),
            )
            params["chunk_strategy"] = request.chunk_strategy
        else:
            # Query without strategy filter
            query_sql = text(
                """
                SELECT
                    c.id as chunk_id,
                    c.doc_id,
                    c.text,
                    d.title as document_title,
                    1 - (e.embedding <=> CAST(:query_embedding AS vector)) as similarity_score
                FROM chunks c
                JOIN embeddings e ON c.id = e.chunk_id
                JOIN documents d ON c.doc_id = d.id
                WHERE 1 - (e.embedding <=> CAST(:query_embedding AS vector)) >= :threshold
                ORDER BY e.embedding <=> CAST(:query_embedding AS vector)
                LIMIT :top_k
            """
            ).bindparams(
                bindparam("query_embedding", type_=String),
                bindparam("threshold", type_=Float),
                bindparam("top_k", type_=Integer),
            )

        result = db.execute(query_sql, params)

        rows = result.fetchall()

        # If no results, guide user
        if not rows:
            guide_answer = (
                "æŠ±æ­‰ï¼Œæˆ‘åœ¨è³‡æ–™åº«ä¸­æ‰¾ä¸åˆ°èˆ‡ä½ å•é¡Œç›´æ¥ç›¸é—œçš„å…§å®¹ã€‚\n\n"
                "ğŸ“š æˆ‘çš„çŸ¥è­˜åº«ç›®å‰åŒ…å«ä»¥ä¸‹ä¸»é¡Œï¼š\n"
                "1. è·æ¶¯è«®è©¢æ¦‚è«–èˆ‡èˆˆè¶£ç†±æƒ…æ¢ç´¢\n"
                "2. å„ªå‹¢è·èƒ½åˆ†æèˆ‡ç™¼å±•\n"
                "3. ç”Ÿæ¶¯æˆç†Ÿåº¦èˆ‡åƒ¹å€¼è§€\n"
                "4. æ±‚è·ç­–ç•¥ã€å±¥æ­·æ’°å¯«èˆ‡é¢è©¦æŠ€å·§\n"
                "5. å¿ƒç†è«®è©¢æŠ€å·§èˆ‡å¯¦å‹™\n"
                "6. ç¶œåˆè·æ¶¯å¯¦æˆ°æ¡ˆä¾‹\n\n"
                "ğŸ’¡ å»ºè­°ï¼š\n"
                "- è©¦è‘—ç”¨æ›´å…·é«”çš„è·æ¶¯ç›¸é—œé—œéµå­—é‡æ–°æå•\n"
                "- ä¾‹å¦‚ï¼šã€Œå¦‚ä½•æ¢ç´¢è·æ¶¯èˆˆè¶£ï¼Ÿã€ã€ã€Œå±¥æ­·æ’°å¯«æŠ€å·§ã€ã€ã€Œé¢è©¦æº–å‚™è¦é»ã€\n"
                "- æˆ–ç›´æ¥å•æˆ‘ä¸Šè¿°ä»»ä¸€ä¸»é¡Œçš„å•é¡Œ"
            )

            if request.question.strip().lower() in [
                "hello",
                "hi",
                "ä½ å¥½",
                "å—¨",
                "å“ˆå›‰",
            ]:
                guide_answer = (
                    "ä½ å¥½ï¼æˆ‘æ˜¯è·æ¶¯è«®è©¢ AI åŠ©ç† ğŸ‘‹\n\n"
                    "æˆ‘å¯ä»¥å”åŠ©ä½ ï¼š\n"
                    "âœ¨ æ¢ç´¢è·æ¶¯èˆˆè¶£èˆ‡ç†±æƒ…\n"
                    "âœ¨ åˆ†æå„ªå‹¢è·èƒ½\n"
                    "âœ¨ æ±‚è·ç­–ç•¥èˆ‡å±¥æ­·é¢è©¦æŒ‡å°\n"
                    "âœ¨ è·æ¶¯ç™¼å±•è¦åŠƒå»ºè­°\n\n"
                    "æœ‰ä»€éº¼è·æ¶¯å•é¡Œæƒ³å•æˆ‘å—ï¼Ÿ"
                )

            return ChatResponse(
                question=request.question, answer=guide_answer, citations=[], total_citations=0
            )

        # Step 3: Construct context and generate answer
        context_parts = []
        citations = []

        for idx, row in enumerate(rows):
            context_parts.append(f"[{idx + 1}] {row.text}")

            citations.append(
                Citation(
                    chunk_id=row.chunk_id,
                    doc_id=row.doc_id,
                    document_title=row.document_title,
                    text=row.text,
                    similarity_score=float(row.similarity_score),
                )
            )

        context = "\n\n".join(context_parts)

        system_prompt = request.system_prompt or (
            "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è·æ¶¯è«®è©¢åŠ©ç†ã€‚æ ¹æ“šæä¾›çš„æ–‡æœ¬ä¾†å›ç­”å•é¡Œã€‚\n\n"
            "å›ç­”æ™‚è«‹éµå¾ªï¼š\n"
            "1. ä¸»è¦æ ¹æ“šæ–‡æœ¬å…§å®¹ä¸¦ç”¨ [1]ã€[2] æ¨™è¨»ä¾†æº\n"
            "2. ä¿æŒå°ˆæ¥­ã€å®¢è§€ä¸”å…·åŒç†å¿ƒçš„èªæ°£\n"
            "3. å¦‚æœæ–‡æœ¬ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹æ˜ç¢ºèªªæ˜\n"
            "4. é©ç•¶å¼•ç”¨é—œéµæ¦‚å¿µã€ç†è«–å’Œæœ€ä½³å¯¦è¸\n"
            "5. è€ƒæ…®å€‹åˆ¥å·®ç•°å’Œå¤šå…ƒè§€é»\n"
            "6. ä½¿ç”¨èˆ‡å•é¡Œç›¸åŒçš„èªè¨€å›ç­”ï¼ˆç¹é«”ä¸­æ–‡æˆ–è‹±æ–‡ï¼‰"
        )

        answer = await openai_service.chat_completion_with_context(
            question=request.question,
            context=context,
            system_prompt=system_prompt,
            temperature=request.temperature,
        )

        return ChatResponse(
            question=request.question,
            answer=answer,
            citations=citations,
            total_citations=len(citations),
        )

    except Exception as e:
        import traceback

        error_detail = f"Chat failed: {str(e)}\n{traceback.format_exc()}"
        raise HTTPException(status_code=500, detail=error_detail) from e
