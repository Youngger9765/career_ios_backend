"""
Session Analysis API - Deep analysis, quick feedback, and report generation
"""

import json
import logging
import re
import time
from datetime import datetime, timedelta, timezone
from typing import List, Tuple
from uuid import UUID

from fastapi import APIRouter, BackgroundTasks, Depends, Request
from sqlalchemy import select
from sqlalchemy.orm import Session as DBSession

from app.core.database import get_db
from app.core.deps import get_current_user, get_tenant_id
from app.core.exceptions import BadRequestError, InternalServerError, NotFoundError
from app.models.counselor import Counselor
from app.models.report import Report, ReportStatus
from app.schemas.session import (
    ParentsReportReference,
    ParentsReportResponse,
    ProviderMetadata,
    QuickFeedbackResponse,
    RealtimeAnalyzeResponse,
)
from app.services.analysis.keyword_analysis_service import KeywordAnalysisService
from app.services.core.session_service import SessionService

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/api/v1/sessions", tags=["Sessions - Analysis"])


def _log_analysis_background(
    session_id: UUID,
    counselor_id: UUID,
    tenant_id: str,
    transcript_segment: str,
    result_data: dict,
    token_usage_data: dict,
    analysis_type: str,
):
    """Background task to log analysis to SessionAnalysisLog (runs AFTER response)"""
    from app.core.database import SessionLocal

    db = SessionLocal()
    try:
        keyword_service = KeywordAnalysisService(db)
        keyword_service.save_analysis_log_and_usage(
            session_id=session_id,
            counselor_id=counselor_id,
            tenant_id=tenant_id,
            transcript_segment=transcript_segment,
            result_data=result_data,
            rag_documents=[],
            rag_sources=[],
            token_usage_data=token_usage_data,
        )
        logger.info(f"{analysis_type} logged for session {session_id} (background)")
    except Exception as e:
        logger.error(f"Failed to log {analysis_type} (background): {e}", exc_info=True)
    finally:
        db.close()


def _extract_transcripts_by_time(
    recordings: List[dict], seconds_ago: int
) -> Tuple[str, str]:
    """
    Extract recent transcript and full transcript from recordings.

    Args:
        recordings: List of recording segments with start_time, end_time, transcript_text
        seconds_ago: How many seconds back to consider as "recent" (e.g., 15 for Quick, 60 for Deep)

    Returns:
        Tuple of (recent_transcript, full_transcript)
        - recent_transcript: Segments from the last N seconds
        - full_transcript: All segments combined
    """
    if not recordings:
        return "", ""

    # Sort by segment_number
    sorted_recordings = sorted(recordings, key=lambda r: r.get("segment_number", 0))

    # Build full transcript
    full_parts = []
    for r in sorted_recordings:
        text = r.get("transcript_text", "")
        if text:
            full_parts.append(text)
    full_transcript = "\n".join(full_parts)

    # Calculate cutoff time
    now = datetime.now(timezone.utc)
    cutoff_time = now - timedelta(seconds=seconds_ago)

    # Extract recent segments (end_time >= cutoff_time)
    recent_parts = []
    for r in sorted_recordings:
        end_time_str = r.get("end_time")
        if not end_time_str:
            continue

        try:
            # Parse end_time (ISO 8601 format)
            if isinstance(end_time_str, str):
                end_time = datetime.fromisoformat(end_time_str.replace("Z", "+00:00"))
                if end_time.tzinfo is None:
                    end_time = end_time.replace(tzinfo=timezone.utc)
            elif isinstance(end_time_str, datetime):
                end_time = end_time_str
                if end_time.tzinfo is None:
                    end_time = end_time.replace(tzinfo=timezone.utc)
            else:
                continue

            # Check if this segment is recent
            if end_time >= cutoff_time:
                text = r.get("transcript_text", "")
                if text:
                    recent_parts.append(text)
        except (ValueError, TypeError) as e:
            logger.debug(f"Failed to parse end_time '{end_time_str}': {e}")
            continue

    recent_transcript = "\n".join(recent_parts) if recent_parts else ""

    # Fallback: if no recent segments found, use the last segment
    if not recent_transcript and sorted_recordings:
        last_segment = sorted_recordings[-1]
        recent_transcript = last_segment.get("transcript_text", "")

    return recent_transcript, full_transcript


def _handle_generic_error(e: Exception, operation: str, instance: str):
    raise InternalServerError(
        detail=f"Failed to {operation}: {str(e)}",
        instance=instance,
    )


@router.post("/{session_id}/quick-feedback", response_model=QuickFeedbackResponse)
async def session_quick_feedback(
    session_id: UUID,
    request: Request,
    background_tasks: BackgroundTasks,
    session_mode: str = "practice",
    current_user: Counselor = Depends(get_current_user),
    tenant_id: str = Depends(get_tenant_id),
    db: DBSession = Depends(get_db),
) -> QuickFeedbackResponse:
    """
    å¿«é€Ÿåé¥‹ï¼ˆè¼•é‡ç´šï¼Œ~8ç§’ï¼‰

    - å¾ session è‡ªå‹•è®€å–é€å­—ç¨¿
    - ä½¿ç”¨ QuickFeedbackService ç”Ÿæˆç°¡çŸ­é¼“å‹µè¨Šæ¯
    - è¿”å› 1 å¥è©±ï¼ˆ50å­—å…§ï¼‰

    Args:
        session_mode: "practice" (ç·´ç¿’æ¨¡å¼ï¼Œç„¡å­©å­åœ¨å ´) æˆ– "emergency" (å°è«‡æ¨¡å¼ï¼Œæœ‰å­©å­åœ¨å ´)
    """
    from app.services.core.quick_feedback_service import quick_feedback_service

    instance = str(request.url.path)

    try:
        # Get session and verify authorization
        service = SessionService(db)
        result = service.get_session_with_details(session_id, current_user, tenant_id)
        if not result:
            raise NotFoundError(detail="Session not found", instance=instance)

        session, client, case, has_report = result

        # Extract recent (last 15s) and full transcript from recordings
        # This allows LLM to focus on recent content while having full context
        recordings = session.recordings or []
        recent_transcript, full_transcript = _extract_transcripts_by_time(
            recordings, seconds_ago=15
        )

        # Fallback to transcript_text if no recordings
        if not full_transcript:
            full_transcript = session.transcript_text or ""
        if not recent_transcript:
            recent_transcript = full_transcript  # Use full if no recent

        if not full_transcript:
            raise BadRequestError(
                detail="Session has no transcript",
                instance=instance,
            )

        logger.info(
            f"Quick feedback: recent={len(recent_transcript)} chars, "
            f"full={len(full_transcript)} chars"
        )

        # Build scenario context for analysis
        scenario_context = ""
        if session.scenario or session.scenario_description:
            scenario_context = f"ã€å®¶é•·ç…©æƒ±æƒ…å¢ƒã€‘{session.scenario or ''}"
            if session.scenario_description:
                scenario_context += f"\n{session.scenario_description}"

        # Call quick feedback service with both transcripts + scenario
        feedback_result = await quick_feedback_service.get_quick_feedback(
            recent_transcript=recent_transcript,
            full_transcript=full_transcript,
            tenant_id=tenant_id,
            mode=session_mode,
            scenario_context=scenario_context,
        )

        # Schedule logging as background task (runs AFTER response returned)
        result_data = {
            "analysis_type": "quick_feedback",
            "message": feedback_result["message"],
            "type": feedback_result["type"],
            "_metadata": {
                "session_mode": session_mode,
                "latency_ms": feedback_result["latency_ms"],
                "recent_transcript_length": len(recent_transcript),
                "full_transcript_length": len(full_transcript),
                "scenario": session.scenario,
            },
        }
        # Use REAL token usage from Gemini response
        prompt_tokens = feedback_result.get("prompt_tokens", 0)
        completion_tokens = feedback_result.get("completion_tokens", 0)
        total_tokens = feedback_result.get(
            "total_tokens", prompt_tokens + completion_tokens
        )
        token_usage_data = {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            "estimated_cost_usd": total_tokens * 0.000001,  # Gemini Flash pricing
        }
        background_tasks.add_task(
            _log_analysis_background,
            session_id=session_id,
            counselor_id=current_user.id,
            tenant_id=tenant_id,
            transcript_segment=recent_transcript[:500],
            result_data=result_data,
            token_usage_data=token_usage_data,
            analysis_type="quick_feedback",
        )

        return QuickFeedbackResponse(
            message=feedback_result["message"],
            type=feedback_result["type"],
            timestamp=feedback_result["timestamp"],
            latency_ms=feedback_result["latency_ms"],
        )

    except (NotFoundError, BadRequestError):
        raise
    except Exception as e:
        logger.error(f"Quick feedback failed for session {session_id}: {e}")
        return QuickFeedbackResponse(
            message="ç¹¼çºŒä¿æŒï¼Œä½ åšå¾—å¾ˆå¥½",
            type="fallback_error",
            timestamp=datetime.now().isoformat(),
            latency_ms=0,
        )


@router.post("/{session_id}/deep-analyze", response_model=RealtimeAnalyzeResponse)
async def session_deep_analyze(
    session_id: UUID,
    request: Request,
    background_tasks: BackgroundTasks,
    session_mode: str = "practice",
    use_rag: bool = False,
    current_user: Counselor = Depends(get_current_user),
    tenant_id: str = Depends(get_tenant_id),
    db: DBSession = Depends(get_db),
) -> RealtimeAnalyzeResponse:
    """
    æ·±å±¤åˆ†æé€å­—ç¨¿ï¼ˆå„ªåŒ–ç‰ˆ - å–®æ¬¡ Gemini å‘¼å«ï¼‰

    - å¾ session è‡ªå‹•è®€å–é€å­—ç¨¿
    - ä½¿ç”¨ KeywordAnalysisService.analyze_keywords_simplified() é€²è¡Œåˆ†æ
    - è¿”å› safety_level, summary, suggestions
    - æ¯”åŸç‰ˆå¿« ~50%ï¼ˆ1 æ¬¡å‘¼å« vs 2 æ¬¡å‘¼å«ï¼‰
    """
    from app.services.analysis.keyword_analysis_service import KeywordAnalysisService

    instance = str(request.url.path)
    start_time = time.time()

    try:
        # Get session and verify authorization
        service = SessionService(db)
        result = service.get_session_with_details(session_id, current_user, tenant_id)
        if not result:
            raise NotFoundError(detail="Session not found", instance=instance)

        session, client, case, has_report = result

        # Extract recent (last 60s) and full transcript from recordings
        # This allows LLM to focus on recent content while having full context
        recordings = session.recordings or []
        recent_transcript, full_transcript = _extract_transcripts_by_time(
            recordings, seconds_ago=60
        )

        # Fallback to transcript_text if no recordings
        if not full_transcript:
            full_transcript = session.transcript_text or ""
        if not recent_transcript:
            recent_transcript = full_transcript  # Use full if no recent

        if not full_transcript:
            raise BadRequestError(
                detail="Session has no transcript",
                instance=instance,
            )

        # Initialize keyword service
        keyword_service = KeywordAnalysisService(db)

        # Build scenario context for analysis
        scenario_context = ""
        if session.scenario or session.scenario_description:
            scenario_context = f"ã€å®¶é•·ç…©æƒ±æƒ…å¢ƒã€‘{session.scenario or ''}"
            if session.scenario_description:
                scenario_context += f"\n{session.scenario_description}"

        # Call SIMPLIFIED analysis with both transcripts + scenario
        logger.info(
            f"Deep analyze (simplified) session {session_id}: "
            f"tenant={tenant_id}, session_mode={session_mode}, "
            f"recent={len(recent_transcript)} chars, full={len(full_transcript)} chars, "
            f"scenario={bool(scenario_context)}"
        )

        analysis_result = await keyword_service.analyze_keywords_simplified(
            transcript_segment=recent_transcript,
            full_transcript=full_transcript,
            mode=session_mode,
            tenant_id=tenant_id,
            scenario_context=scenario_context,
        )

        # Extract results
        quick_suggestions = analysis_result.get("quick_suggestions", [])

        # Calculate latency
        latency_ms = int((time.time() - start_time) * 1000)
        provider_metadata = ProviderMetadata(
            provider="gemini", latency_ms=latency_ms, model="gemini-3-flash-preview"
        )

        logger.info(f"Deep analyze completed in {latency_ms}ms")

        # Schedule logging as background task (runs AFTER response returned)
        result_data = {
            "analysis_type": "deep_analyze",
            "safety_level": analysis_result.get("safety_level", "green"),
            "display_text": analysis_result.get("display_text", ""),
            "quick_suggestions": quick_suggestions,
            "_metadata": {
                "session_mode": session_mode,
                "use_rag": use_rag,
                "latency_ms": latency_ms,
                "recent_transcript_length": len(recent_transcript),
                "full_transcript_length": len(full_transcript),
                "scenario": session.scenario,
            },
        }
        # Use REAL token usage from Gemini response
        prompt_tokens = analysis_result.get("prompt_tokens", 0)
        completion_tokens = analysis_result.get("completion_tokens", 0)
        total_tokens = analysis_result.get(
            "total_tokens", prompt_tokens + completion_tokens
        )
        token_usage_data = {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            "estimated_cost_usd": total_tokens * 0.000001,  # Gemini Flash pricing
        }
        background_tasks.add_task(
            _log_analysis_background,
            session_id=session_id,
            counselor_id=current_user.id,
            tenant_id=tenant_id,
            transcript_segment=recent_transcript[:1000],
            result_data=result_data,
            token_usage_data=token_usage_data,
            analysis_type="deep_analyze",
        )

        return RealtimeAnalyzeResponse(
            safety_level=analysis_result.get("safety_level", "green"),
            summary=analysis_result.get("display_text", "åˆ†æå®Œæˆ"),
            alerts=[],
            suggestions=quick_suggestions,
            time_range="0:00-2:00",
            timestamp=datetime.now(timezone.utc).isoformat(),
            rag_sources=[],  # Simplified version doesn't use RAG
            cache_metadata=None,
            provider_metadata=provider_metadata,
        )

    except (NotFoundError, BadRequestError):
        raise
    except Exception as e:
        logger.error(
            f"Deep analyze failed for session {session_id}: {e}", exc_info=True
        )
        _handle_generic_error(e, "deep analyze session", instance)


@router.post("/{session_id}/report", response_model=ParentsReportResponse)
async def session_report(
    session_id: UUID,
    request: Request,
    use_rag: bool = True,
    current_user: Counselor = Depends(get_current_user),
    tenant_id: str = Depends(get_tenant_id),
    db: DBSession = Depends(get_db),
) -> ParentsReportResponse:
    """
    ç”Ÿæˆè¦ªå­å°è©±å ±å‘Š

    - å¾ session è‡ªå‹•è®€å–é€å­—ç¨¿
    - åˆ†æå°è©±ä¸¦æä¾›ï¼šæ‘˜è¦ã€äº®é»ã€æ”¹é€²å»ºè­°
    - use_rag=True æ™‚æœƒæª¢ç´¢ç›¸é—œæ•™é¤Šç†è«–ä½œç‚ºåƒè€ƒ
    """
    from app.services.external.gemini_service import GeminiService
    from app.services.external.openai_service import OpenAIService
    from app.services.rag.rag_retriever import RAGRetriever

    instance = str(request.url.path)
    start_time = time.time()

    try:
        # Get session and verify authorization
        service = SessionService(db)
        result = service.get_session_with_details(session_id, current_user, tenant_id)
        if not result:
            raise NotFoundError(detail="Session not found", instance=instance)

        session, client, case, has_report = result

        # Get transcript from session
        transcript = session.transcript_text or ""
        if not transcript:
            raise BadRequestError(
                detail="Session has no transcript",
                instance=instance,
            )

        # RAG: Retrieve relevant parenting theories
        rag_context = ""
        rag_sources = []  # List of document names for logging
        rag_references = []  # List of ParentsReportReference for response
        if use_rag:
            try:
                openai_service = OpenAIService()
                rag_retriever = RAGRetriever(openai_service)

                # Build a more effective search query
                # Include scenario info + key parts of transcript
                scenario_info = session.scenario or ""
                scenario_desc = session.scenario_description or ""
                search_query = f"{scenario_info} {scenario_desc}\n{transcript[:800]}"
                logger.info(
                    f"RAG search query (first 200 chars): {search_query[:200]}..."
                )

                # Search for parenting-related theories
                rag_results = await rag_retriever.search(
                    query=search_query,
                    top_k=5,
                    threshold=0.25,  # Lower threshold for better recall
                    db=db,
                    category="parenting",
                )

                if rag_results:
                    rag_context = "\n\nã€åƒè€ƒç†è«–ã€‘\n"
                    for i, theory in enumerate(rag_results, 1):
                        theory_text = theory.get("text", "")[:200]
                        theory_doc = theory.get("document", "")
                        theory_title = theory.get("title", theory_doc)
                        theory_category = theory.get("category", "æ•™é¤Šç†è«–")

                        rag_context += f"{i}. {theory_text}... (ä¾†æº: {theory_doc})\n"
                        rag_sources.append(theory_doc)

                        # Build reference for response
                        rag_references.append(
                            ParentsReportReference(
                                title=theory_title,
                                content=theory_text,
                                source=theory_doc,
                                theory=theory_category,
                            )
                        )
                    logger.info(f"RAG found {len(rag_results)} theories for report")
                else:
                    logger.warning(
                        "RAG search returned no results - "
                        "check if parenting documents exist in vector DB"
                    )
            except Exception as e:
                # RAG failure should not block report generation
                logger.warning(f"RAG search failed (continuing without RAG): {e}")
                rag_context = ""

        # Build analysis prompt with optional RAG context
        rag_instruction = ""
        if rag_context:
            rag_instruction = """
ã€é‡è¦ã€‘è«‹åƒè€ƒä¸Šè¿°ç†è«–ä¾†æ”¯æŒä½ çš„åˆ†æå’Œå»ºè­°ã€‚åœ¨ analyze å’Œ suggestion ä¸­å¯ä»¥å¼•ç”¨ç›¸é—œç†è«–ã€‚
"""

        # Calculate transcript duration hint
        transcript_length = len(transcript)
        duration_hint = (
            "çŸ­å°è©±"
            if transcript_length < 500
            else "ä¸­ç­‰å°è©±"
            if transcript_length < 2000
            else "é•·å°è©±"
        )

        # Build scenario context for report
        scenario_section = ""
        if session.scenario or session.scenario_description:
            scenario_section = f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€å®¶é•·ç…©æƒ±æƒ…å¢ƒã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{session.scenario or ''}
{session.scenario_description or ''}

âš ï¸ è«‹åœç¹ä¸Šè¿°å®¶é•·çš„ç…©æƒ±æƒ…å¢ƒé€²è¡Œåˆ†æï¼Œæä¾›é‡å°æ€§çš„å»ºè­°ã€‚
"""

        analysis_prompt = f"""ä½ æ˜¯å°ˆæ¥­çš„è¦ªå­æºé€šåˆ†æå¸«ï¼Œç²¾é€š 8 å¤§æ•™é¤Šæµæ´¾ï¼ˆé˜¿å¾·å‹’æ­£å‘æ•™é¤Šã€è–©æçˆ¾ã€ABAè¡Œç‚ºåˆ†æã€Dan Siegel å…¨è…¦æ•™é¤Šã€Gottman æƒ…ç·’è¼”å°ã€Ross Greene å”ä½œå•é¡Œè§£æ±ºã€Dr. Becky Kennedyã€ç¤¾æœƒæ„è­˜æ•™é¤Šï¼‰ï¼Œè² è²¬åˆ†æå®¶é•·èˆ‡å­©å­çš„å°è©±ï¼Œæä¾›å»ºè¨­æ€§çš„å›é¥‹ã€‚
{scenario_section}{rag_context}{rag_instruction}
ã€å°è©±é€å­—ç¨¿ã€‘ï¼ˆ{duration_hint}ï¼Œå…± {transcript_length} å­—ï¼‰
{transcript}

ã€åˆ†æè¦æ±‚ã€‘
è«‹ä»¥ä¸­æ€§ã€å®¢è§€ã€æº«å’Œçš„ç«‹å ´**æ·±å…¥åˆ†æ**é€™æ¬¡å°è©±ã€‚
âš ï¸ é‡è¦ï¼šè«‹æ ¹æ“šå°è©±é•·åº¦æä¾›**ç›¸æ‡‰æ·±åº¦çš„åˆ†æ**ï¼š
- çŸ­å°è©±ï¼ˆ< 500 å­—ï¼‰ï¼šæä¾›åŸºæœ¬åˆ†æ
- ä¸­ç­‰å°è©±ï¼ˆ500-2000 å­—ï¼‰ï¼šæä¾›è©³ç´°åˆ†æï¼ŒåŒ…å«å¤šå€‹è§€å¯Ÿé»
- é•·å°è©±ï¼ˆ> 2000 å­—ï¼‰ï¼šæä¾›å®Œæ•´ã€æ·±å…¥çš„åˆ†æï¼Œæ¶µè“‹å°è©±ä¸­çš„å„å€‹é—œéµæ™‚åˆ»

è«‹æä¾›ä»¥ä¸‹ 4 å€‹éƒ¨åˆ†ï¼š

1. **é¼“å‹µæ¨™é¡Œ**ï¼ˆencouragementï¼‰
   - ä¸€æ®µæ­£å‘é¼“å‹µçš„è©±ï¼Œè‚¯å®šå®¶é•·é¡˜æ„æºé€šçš„å¿ƒæ„
   - å…·é«”æŒ‡å‡ºå®¶é•·åšå¾—å¥½çš„åœ°æ–¹
   - ä¾‹å¦‚ï¼šã€Œé€™æ¬¡ä½ å·²ç¶“åšäº†ä¸€ä»¶é‡è¦çš„äº‹ï¼šé¡˜æ„å¥½å¥½è·Ÿå­©å­è«‡ã€‚ç•¶ä½ èªªã€æˆ‘æƒ³è½è½ä½ çš„æƒ³æ³•ã€æ™‚ï¼Œå±•ç¾äº†é–‹æ”¾çš„æ…‹åº¦ã€‚ã€

2. **å¾…è§£æ±ºçš„è­°é¡Œ**ï¼ˆissueï¼‰
   - æŒ‡å‡ºé€™æ¬¡å°è©±ä¸­æœ€éœ€è¦æ”¹é€²çš„åœ°æ–¹
   - å®¢è§€æè¿°ï¼Œä¸æ‰¹åˆ¤
   - å¦‚æœå°è©±è¼ƒé•·ï¼Œå¯ä»¥åˆ—å‡ºå¤šå€‹è­°é¡Œ

3. **æºé€šå…§å®¹åˆ†æ**ï¼ˆanalyzeï¼‰
   - **æ·±å…¥åˆ†æ**ç‚ºä½•é€™æ¨£çš„æºé€šæ–¹å¼å¯èƒ½æœ‰å•é¡Œ
   - è§£é‡‹èƒŒå¾Œçš„å¿ƒç†å­¸æˆ–æ•™é¤Šç†è«–åŸç†
   - å¼•ç”¨ç›¸é—œæ•™é¤Šæµæ´¾çš„è§€é»ï¼ˆå¦‚ï¼šè–©æçˆ¾å†°å±±ç†è«–ã€é˜¿å¾·å‹’æ­¸å±¬æ„Ÿã€Gottman æƒ…ç·’è¼”å°ç­‰ï¼‰
   - åˆ†æå°è©±ä¸­çš„æƒ…ç·’å‹•æ…‹ã€æ¬ŠåŠ›é—œä¿‚ã€æºé€šæ¨¡å¼
   - âš ï¸ å°æ–¼é•·å°è©±ï¼Œè«‹æä¾›å®Œæ•´ã€è©³ç›¡çš„åˆ†æï¼ˆ300-500 å­—ï¼‰

4. **å»ºè­°ä¸‹æ¬¡å¯ä»¥é€™æ¨£èªª**ï¼ˆsuggestionï¼‰
   - æä¾›å…·é«”ã€å¯ç›´æ¥ä½¿ç”¨çš„æ›¿ä»£èªªæ³•
   - ç”¨ã€Œã€æ¨™ç¤ºå»ºè­°çš„è©±èª
   - æä¾›å¤šå€‹æƒ…å¢ƒä¸‹çš„å»ºè­°è©±è¡“
   - è§£é‡‹ç‚ºä»€éº¼é€™æ¨£èªªæ›´æœ‰æ•ˆ
   - âš ï¸ å°æ–¼é•·å°è©±ï¼Œæä¾›å¤šç¨®æƒ…å¢ƒçš„å»ºè­°ï¼ˆ200-400 å­—ï¼‰

ã€èªæ°£è¦æ±‚ã€‘
- æº«å’Œã€åŒç†ã€å»ºè¨­æ€§
- é¿å…æ‰¹åˆ¤æˆ–è®“å®¶é•·æ„Ÿåˆ°è¢«æŒ‡è²¬
- å±•ç¾å°ˆæ¥­æ·±åº¦ï¼Œè®“å®¶é•·æ„Ÿå—åˆ°ã€Œæœ‰æ–™ã€çš„åˆ†æ

ã€è¼¸å‡ºæ ¼å¼ã€‘
è«‹ä»¥ JSON æ ¼å¼å›æ‡‰ï¼š

{{
  "encouragement": "æ­£å‘é¼“å‹µæ¨™é¡Œï¼ˆåŒ…å«å…·é«”è§€å¯Ÿï¼‰",
  "issue": "å¾…è§£æ±ºçš„è­°é¡Œï¼ˆå¯ä»¥æ˜¯å¤šé»ï¼Œç”¨æ›è¡Œåˆ†éš”ï¼‰",
  "analyze": "æºé€šå…§å®¹æ·±å…¥åˆ†æï¼ˆæ ¹æ“šå°è©±é•·åº¦ï¼Œæä¾› 150-500 å­—çš„åˆ†æï¼‰",
  "suggestion": "å»ºè­°ä¸‹æ¬¡å¯ä»¥é€™æ¨£èªªï¼ˆæä¾›å¤šå€‹æƒ…å¢ƒçš„å…·é«”è©±è¡“ï¼Œ150-400 å­—ï¼‰"
}}

è«‹é–‹å§‹æ·±å…¥åˆ†æã€‚"""

        # Call Gemini
        gemini_service = GeminiService()
        gemini_response = await gemini_service.chat_completion(
            prompt=analysis_prompt,
            temperature=0.7,
            return_metadata=True,
        )

        llm_raw_response = gemini_response["text"]

        # Parse response
        try:
            if "```json" in llm_raw_response:
                json_start = llm_raw_response.find("```json") + 7
                json_end = llm_raw_response.find("```", json_start)
                json_text = llm_raw_response[json_start:json_end].strip()
            elif "{" in llm_raw_response:
                json_start = llm_raw_response.find("{")
                json_end = llm_raw_response.rfind("}") + 1
                json_text = llm_raw_response[json_start:json_end]
            else:
                raise ValueError("No JSON found in response")

            json_text = re.sub(r",(\s*[}\]])", r"\1", json_text)
            analysis = json.loads(json_text)

        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"Failed to parse report response: {e}")
            raise InternalServerError(
                detail="Failed to parse AI response",
                instance=instance,
            )

        # Build response
        latency_ms = int((time.time() - start_time) * 1000)
        logger.info(f"Report generated for session {session_id} in {latency_ms}ms")

        # Billing: Save analysis log and deduct credits
        try:
            keyword_service = KeywordAnalysisService(db)

            # Get token usage from Gemini response
            token_usage_data = {
                "prompt_tokens": gemini_response.get("prompt_tokens", 0),
                "completion_tokens": gemini_response.get("completion_tokens", 0),
                "total_tokens": gemini_response.get("total_tokens", 0),
                "estimated_cost_usd": gemini_response.get("estimated_cost_usd", 0),
            }

            # Build result data for logging
            result_data = {
                "analysis_type": "report",
                "encouragement": analysis.get("encouragement", ""),
                "issue": analysis.get("issue", ""),
                "analyze": analysis.get("analyze", ""),
                "suggestion": analysis.get("suggestion", ""),
                "rag_sources": rag_sources,
                "_metadata": {
                    "duration_ms": latency_ms,
                    "use_rag": use_rag,
                    "rag_documents_count": len(rag_sources),
                    "transcript_length": len(transcript),
                    "token_usage": token_usage_data,
                    "llm_raw_response": llm_raw_response,
                },
            }

            # Save to session_analysis_logs and update billing
            keyword_service.save_analysis_log_and_usage(
                session_id=session_id,
                counselor_id=current_user.id,
                tenant_id=tenant_id,
                transcript_segment=transcript,
                result_data=result_data,
                rag_documents=[{"source": s} for s in rag_sources],
                rag_sources=rag_sources,
                token_usage_data=token_usage_data,
            )
            logger.info(f"Billing recorded for report session {session_id}")
        except Exception as e:
            # Billing failure should not block report response
            logger.error(f"Failed to record billing for report: {e}", exc_info=True)

        # Create or update Report record for has_report flag
        try:
            existing_report = db.execute(
                select(Report).where(
                    Report.session_id == session_id,
                    Report.deleted_at.is_(None),
                )
            ).scalar_one_or_none()

            # Build content for Report
            report_content_json = {
                "encouragement": analysis.get("encouragement", ""),
                "issue": analysis.get("issue", ""),
                "analyze": analysis.get("analyze", ""),
                "suggestion": analysis.get("suggestion", ""),
                "references": [ref.model_dump() for ref in rag_references],
            }

            # Build markdown content
            report_content_markdown = f"""# è¦ªå­å°è©±å ±å‘Š

## ğŸŒŸ é¼“å‹µ
{analysis.get("encouragement", "")}

## ğŸ’¡ å¾…è§£æ±ºçš„è­°é¡Œ
{analysis.get("issue", "")}

## ğŸ“Š æºé€šå…§å®¹åˆ†æ
{analysis.get("analyze", "")}

## ğŸ’¬ å»ºè­°ä¸‹æ¬¡å¯ä»¥é€™æ¨£èªª
{analysis.get("suggestion", "")}
"""

            if existing_report:
                # Update existing report
                existing_report.content_json = report_content_json
                existing_report.content_markdown = report_content_markdown
                existing_report.status = ReportStatus.DRAFT
                existing_report.prompt_tokens = gemini_response.get("prompt_tokens", 0)
                existing_report.completion_tokens = gemini_response.get(
                    "completion_tokens", 0
                )
                logger.info(f"Updated existing Report for session {session_id}")
            else:
                # Create new report
                new_report = Report(
                    session_id=session_id,
                    client_id=client.id,
                    created_by_id=current_user.id,
                    tenant_id=tenant_id,
                    status=ReportStatus.DRAFT,
                    mode="island_parents",
                    content_json=report_content_json,
                    content_markdown=report_content_markdown,
                    prompt_tokens=gemini_response.get("prompt_tokens", 0),
                    completion_tokens=gemini_response.get("completion_tokens", 0),
                )
                db.add(new_report)
                logger.info(f"Created new Report for session {session_id}")

            db.commit()
        except Exception as e:
            # Report creation failure should not block response
            logger.error(f"Failed to create/update Report record: {e}", exc_info=True)
            db.rollback()

        return ParentsReportResponse(
            encouragement=analysis.get("encouragement", "æ„Ÿè¬ä½ é¡˜æ„èŠ±æ™‚é–“èˆ‡å­©å­æºé€šã€‚"),
            issue=analysis.get("issue", ""),
            analyze=analysis.get("analyze", ""),
            suggestion=analysis.get("suggestion", ""),
            references=rag_references,
            timestamp=datetime.now(timezone.utc).isoformat(),
        )

    except (NotFoundError, BadRequestError, InternalServerError):
        raise
    except Exception as e:
        logger.error(
            f"Report generation failed for session {session_id}: {e}", exc_info=True
        )
        _handle_generic_error(e, "generate report", instance)
