"""
Realtime STT Counseling API
"""
import logging
import os
import uuid
from datetime import datetime, timezone

import httpx
from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException
from sqlalchemy.orm import Session

from app.core.database import get_db
from app.schemas.realtime import (
    ImprovementSuggestion,
    ParentsReportRequest,
    ParentsReportResponse,
    ProviderMetadata,
    QuickFeedbackRequest,
    QuickFeedbackResponse,
    RAGSource,
    RealtimeAnalyzeRequest,
    RealtimeAnalyzeResponse,
)
from app.services.gbq_service import gbq_service
from app.services.gemini_service import GeminiService
from app.services.openai_service import OpenAIService
from app.services.quick_feedback_service import quick_feedback_service

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/v1/realtime", tags=["Realtime Counseling"])

# Initialize services (still needed for generate_parents_report)
gemini_service = GeminiService()
openai_service = OpenAIService()

# REMOVED: SAFETY_WINDOW_SPEAKER_TURNS, SAFETY_WINDOW_CHARACTERS, ANNOTATED_SAFETY_WINDOW_TURNS
# These constants are now defined in KeywordAnalysisService

# REMOVED: CACHE_SYSTEM_INSTRUCTION
# System instruction is now part of keyword_analysis_service prompt templates

# Legacy system instruction (kept for reference, but no longer used by analyze_transcript)
_LEGACY_CACHE_SYSTEM_INSTRUCTION = """ä½ æ˜¯å°ˆæ¥­è«®è©¢ç£å°ï¼Œåˆ†æå³æ™‚è«®è©¢å°è©±ã€‚ä½ çš„è§’è‰²æ˜¯ç«™åœ¨æ¡ˆä¸»èˆ‡è«®è©¢å¸«ä¹‹é–“ï¼Œæä¾›æº«æš–ã€åŒç†ä¸”å…·é«”å¯è¡Œçš„å°ˆæ¥­å»ºè­°ã€‚

ã€è§’è‰²å®šç¾©ã€‘CRITICAL - å¿…é ˆåš´æ ¼éµå®ˆï¼š
- "counselor" = è«®è©¢å¸«/è¼”å°å¸«ï¼ˆå°ˆæ¥­åŠ©äººè€…ï¼Œæä¾›å”åŠ©çš„ä¸€æ–¹ï¼‰
- "client" = æ¡ˆä¸»/å€‹æ¡ˆ/å®¶é•·ï¼ˆæ±‚åŠ©è€…ï¼Œæœ‰å›°æ“¾éœ€è¦å”åŠ©çš„ä¸€æ–¹ï¼‰
- æ‰€æœ‰å•é¡Œã€å›°æ“¾ã€ç—‡ç‹€éƒ½æ˜¯ã€Œæ¡ˆä¸»/å€‹æ¡ˆã€é¢è‡¨çš„ï¼Œä¸æ˜¯è«®è©¢å¸«çš„å•é¡Œ
- åˆ†æç„¦é»ï¼šæ¡ˆä¸»çš„ç‹€æ³ã€éœ€æ±‚ã€é¢¨éšª
- å»ºè­°å°è±¡ï¼šçµ¦è«®è©¢å¸«çš„å°ˆæ¥­å»ºè­°ï¼ˆå¦‚ä½•å”åŠ©æ¡ˆä¸»ï¼‰

ã€åˆ†æç¯„åœã€‘CRITICAL - å¿…é ˆåš´æ ¼éµå®ˆï¼š
ğŸ¯ **ä¸»è¦åˆ†æç„¦é»**ï¼šæœ€æ–°ä¸€åˆ†é˜å…§çš„å°è©±å…§å®¹
   - ä½ æœƒæ”¶åˆ°å®Œæ•´çš„å°è©±è¨˜éŒ„ï¼ˆå¯èƒ½é•·é”æ•¸ååˆ†é˜ï¼‰
   - ä½†ä½ çš„åˆ†æå¿…é ˆèšç„¦åœ¨ã€Œæœ€å¾Œå‡ºç¾çš„å°è©±ã€ï¼ˆæœ€æ–°ä¸€åˆ†é˜ï¼‰
   - å‰é¢çš„å°è©±åƒ…ä½œç‚ºèƒŒæ™¯è„ˆçµ¡åƒè€ƒï¼Œå¹«åŠ©ä½ ç†è§£å‰å› å¾Œæœ

ã€å®‰å…¨ç­‰ç´šè©•ä¼°è¦å‰‡ã€‘CRITICAL - å¿…é ˆåš´æ ¼éµå®ˆï¼š
âš ï¸ **åƒ…æ ¹æ“šã€Œã€æœ€è¿‘å°è©± - ç”¨æ–¼å®‰å…¨è©•ä¼°ã€‘ã€å€å¡Šåˆ¤æ–·å®‰å…¨ç­‰ç´š**
   - æ¨™è¨»å€å¡Šé¡¯ç¤ºæœ€è¿‘ 5-10 å€‹å°è©±è¼ªæ¬¡
   - ä¸è¦å› ç‚ºå®Œæ•´é€å­—ç¨¿ä¸­å‡ºç¾éçš„å±éšªè©å°±è©•ä¼°ç‚ºé«˜é¢¨éšª
   - å¦‚æœæœ€è¿‘å°è©±å·²ç¶“ç·©å’Œã€æ­£å‘ï¼Œå³ä½¿ä¹‹å‰æœ‰å±éšªå…§å®¹ï¼Œä¹Ÿæ‡‰è©•ä¼°ç‚ºè¼ƒä½é¢¨éšª
   - å®‰å…¨ç­‰ç´šåæ˜ ç•¶å‰ç‹€æ…‹ï¼Œä¸æ˜¯æ­·å²ç‹€æ…‹

ğŸ¯ **å»ºè­°å…§å®¹**ï¼š
   - å¯ä»¥åƒè€ƒå®Œæ•´å°è©±æ­·å²ï¼Œæä¾›æ›´æœ‰æ·±åº¦çš„å»ºè­°
   - ä½†è¦èšç„¦åœ¨æœ€è¿‘å°è©±çš„ç•¶å‰ç‹€æ…‹

ã€æ ¸å¿ƒåŸå‰‡ã€‘åŒç†å„ªå…ˆã€æº«å’Œå¼•å°ã€å…·é«”è¡Œå‹•ï¼š

1. **åŒç†èˆ‡ç†è§£ç‚ºå…ˆ**
   - æ°¸é å…ˆç†è§£èˆ‡åŒç†æ¡ˆä¸»ï¼ˆå®¶é•·ï¼‰çš„æ„Ÿå—å’Œè™•å¢ƒ
   - èªå¯æ•™é¤Šå£“åŠ›ã€æƒ…ç·’å¤±æ§æ˜¯æ­£å¸¸çš„äººæ€§åæ‡‰
   - é¿å…æ‰¹åˆ¤ã€æŒ‡è²¬æˆ–è®“æ¡ˆä¸»æ„Ÿåˆ°è¢«å¦å®š

2. **æº«å’Œã€éæ‰¹åˆ¤çš„èªæ°£**
   - âŒ ç¦æ­¢ç”¨èªï¼šã€Œè¡¨é”å‡ºå°å­©å­ä½¿ç”¨èº«é«”æš´åŠ›çš„è¡å‹•ã€ã€Œå¯èƒ½é€ æˆå‚·å®³ã€ã€Œä¸ç•¶ç®¡æ•™ã€
   - âœ… å»ºè­°ç”¨èªï¼šã€Œç†è§£åˆ°åœ¨æ•™é¤Šå£“åŠ›ä¸‹ï¼Œçˆ¶æ¯æœ‰æ™‚æœƒæ„Ÿåˆ°æƒ…ç·’å¤±æ§æ˜¯å¾ˆæ­£å¸¸çš„ã€
   - âœ… ä½¿ç”¨ï¼šã€Œå¯ä»¥è€ƒæ…®ã€ã€Œæˆ–è¨±ã€ã€Œè©¦è©¦çœ‹ã€ç­‰æŸ”å’Œå¼•å°è©
   - âœ… ç„¦é»æ”¾åœ¨ã€Œå¦‚ä½•èª¿æ•´ã€è€Œéã€Œå“ªè£¡åšéŒ¯ã€

3. **å…·é«”ã€ç°¡æ½”çš„å»ºè­°**
   - å»ºè­°è¦å…·é«”å¯è¡Œï¼Œä½†ä¿æŒç°¡çŸ­ï¼ˆä¸è¶…é 50 å­—ï¼‰
   - é¿å…æŠ½è±¡æ¦‚å¿µï¼Œç”¨å…·é«”åšæ³•
   - ä¸è¦å†—é•·çš„æ­¥é©Ÿèªªæ˜æˆ–å°è©±ç¯„ä¾‹

ã€è¼¸å‡ºæ ¼å¼ã€‘è«‹æä¾›ä»¥ä¸‹ JSON æ ¼å¼å›æ‡‰ï¼š

{
  "summary": "æ¡ˆä¸»è™•å¢ƒç°¡è¿°ï¼ˆ1-2 å¥ï¼‰",
  "alerts": [
    "ğŸ’¡ åŒç†æ¡ˆä¸»æ„Ÿå—ï¼ˆ1 å¥ï¼‰",
    "âš ï¸ éœ€é—œæ³¨çš„éƒ¨åˆ†ï¼ˆ1 å¥ï¼‰"
  ],
  "suggestions": [
    "ğŸ’¡ æ ¸å¿ƒå»ºè­°ï¼ˆç°¡çŸ­ï¼Œ< 50 å­—ï¼‰",
    "ğŸ’¡ å…·é«”åšæ³•ï¼ˆç°¡çŸ­ï¼Œ< 50 å­—ï¼‰"
  ]
}

ã€èªæ°£è¦æ±‚ã€‘æº«å’Œã€åŒç†ã€ç°¡æ½”ï¼Œé¿å…æ‰¹åˆ¤æˆ–éåº¦èªªæ•™

ã€RAG ä½¿ç”¨è¦æ±‚ã€‘CRITICAL - å¿…é ˆåš´æ ¼éµå®ˆï¼š
- ç•¶åˆ†ææ¶‰åŠè¦ªå­æ•™é¤Šã€å…’ç«¥ç™¼å±•ã€ç®¡æ•™ç­–ç•¥ç­‰å°ˆæ¥­çŸ¥è­˜æ™‚ï¼Œå¿…é ˆåƒè€ƒä¸Šæ–¹æä¾›çš„ã€ŒğŸ“š ç›¸é—œè¦ªå­æ•™é¤ŠçŸ¥è­˜åº«å…§å®¹ã€
- å„ªå…ˆä½¿ç”¨çŸ¥è­˜åº«ä¸­çš„ç†è«–ã€æ–¹æ³•ã€å»ºè­°ï¼ˆå¦‚æ­£å‘æ•™é¤Šã€æƒ…ç·’æ•™é¤Šç­‰ï¼‰
- ä¸è¦åƒ…æ†‘ä¸€èˆ¬å¸¸è­˜æˆ–æƒ³åƒå›ç­”å°ˆæ¥­å•é¡Œ
- å¦‚æœçŸ¥è­˜åº«å…§å®¹ç›¸é—œï¼Œè«‹åœ¨å»ºè­°ä¸­èå…¥ï¼ˆä¸éœ€æ˜ç¢ºæ¨™æ³¨ä¾†æºï¼‰
"""

# REMOVED: PARENTING_KEYWORDS, _detect_parenting_keywords(), _detect_parenting_theory()
# These functions are now handled by keyword_analysis_service.analyze_keywords()


# Note: _search_rag_knowledge() is kept for generate_parents_report endpoint
def _detect_parenting_theory(title: str) -> str:
    """Detect which parenting theory a document belongs to based on title.

    Args:
        title: Document title

    Returns:
        Theory name in Chinese (e.g., "æ­£å‘æ•™é¤Š", "æƒ…ç·’æ•™é¤Š")
    """
    # Theory keyword mappings (Chinese, English, and file name patterns)
    theory_mappings = {
        "æ­£å‘æ•™é¤Š": ["æ­£å‘æ•™é¤Š", "Positive Discipline", "positive_discipline"],
        "æƒ…ç·’æ•™é¤Š": [
            "æƒ…ç·’æ•™é¤Š",
            "Emotional Coaching",
            "Emotion Coaching",
            "emotional_coaching",
        ],
        "ä¾é™„ç†è«–": ["ä¾é™„ç†è«–", "Attachment Theory", "attachment_theory"],
        "èªçŸ¥ç™¼å±•ç†è«–": ["èªçŸ¥ç™¼å±•", "Cognitive Development", "cognitive_development"],
        "è‡ªæˆ‘æ±ºå®šè«–": ["è‡ªæˆ‘æ±ºå®š", "Self-Determination", "self_determination"],
    }

    # Check each theory's keywords (case-insensitive)
    title_lower = title.lower()
    for theory_name, keywords in theory_mappings.items():
        for keyword in keywords:
            if keyword.lower() in title_lower:
                return theory_name

    # Default if no match found
    return "å…¶ä»–"


async def _search_rag_knowledge(
    transcript: str, db: Session, top_k: int = 3, similarity_threshold: float = 0.5
):
    """Search RAG knowledge base for relevant parenting content.

    NOTE: Only used by generate_parents_report endpoint.
    analyze_transcript now uses keyword_analysis_service which has built-in RAG.

    Args:
        transcript: The transcript text to search
        db: Database session
        top_k: Number of top results to return
        similarity_threshold: Minimum similarity score

    Returns:
        List of RAG sources with title, content, and score
    """
    from app.services.rag_chat_service import RAGChatService

    try:
        # Initialize RAG service
        rag_service = RAGChatService(db=db)

        # Generate embedding for transcript
        query_embedding = await openai_service.create_embedding(transcript)

        # Search similar chunks with parenting category filter
        rows = await rag_service.search_similar_chunks(
            query_embedding=query_embedding,
            top_k=top_k,
            similarity_threshold=similarity_threshold,
            category="parenting",  # Filter for parenting documents only
        )

        # Build RAG sources response
        rag_sources = []
        for row in rows:
            # Detect theory from document title
            theory = _detect_parenting_theory(row.document_title)

            rag_sources.append(
                RAGSource(
                    title=row.document_title,
                    content=row.text[:300],  # Truncate to 300 chars
                    score=round(float(row.similarity_score), 2),
                    theory=theory,
                )
            )

        logger.info(f"RAG search found {len(rag_sources)} relevant sources")
        return rag_sources

    except Exception as e:
        logger.error(f"RAG search failed: {e}")
        # Return empty list on error (fallback)
        return []


# REMOVED: _assess_safety_level() and _build_annotated_transcript()
# These functions are now handled internally by keyword_analysis_service
# via the island_parents prompt templates which include sliding window logic


# REMOVED: _build_emergency_prompt() and _build_practice_prompt()
# These functions are now handled by keyword_analysis_service.analyze_keywords()
# which provides unified prompt management with 200 expert suggestions


def _calculate_gemini_cost(usage_metadata: dict) -> float:
    """Calculate estimated cost for Gemini API usage

    Gemini 3 Flash pricing (as of Dec 2025):
    - Input: $0.50 per 1M tokens
    - Output: $3.00 per 1M tokens
    - Cached input: $0.125 per 1M tokens (75% discount)
    """
    prompt_tokens = usage_metadata.get("prompt_token_count", 0)
    completion_tokens = usage_metadata.get("candidates_token_count", 0)
    cached_tokens = usage_metadata.get("cached_content_token_count", 0)

    # Subtract cached tokens from prompt tokens
    non_cached_prompt = max(0, prompt_tokens - cached_tokens)

    # Calculate costs (convert to per 1K tokens)
    prompt_cost = (non_cached_prompt / 1000) * 0.0005  # $0.50/1M = $0.0005/1K
    cached_cost = (cached_tokens / 1000) * 0.000125  # $0.125/1M = $0.000125/1K
    completion_cost = (completion_tokens / 1000) * 0.003  # $3/1M = $0.003/1K

    return prompt_cost + cached_cost + completion_cost


async def write_to_gbq_async(data: dict) -> None:
    """Write realtime analysis result to BigQuery asynchronously

    This is a wrapper function that catches all exceptions to prevent
    GBQ write failures from affecting API response.

    Args:
        data: Analysis data to write to BigQuery
    """
    try:
        await gbq_service.write_analysis_log(data)
    except Exception as e:
        # Log error but don't raise - GBQ failures should not block API
        logger.error(
            f"Failed to write to BigQuery (non-blocking): {str(e)}", exc_info=True
        )


@router.post("/analyze", response_model=RealtimeAnalyzeResponse)
async def analyze_transcript(
    request: RealtimeAnalyzeRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
):
    """Analyze realtime counseling transcript with AI supervision.

    Now uses unified keyword_analysis_service for consistency with session analysis.

    Returns summary, alerts, and suggestions for the counselor based on
    the conversation in the past 60 seconds.

    This is a demo feature with no authentication required.
    """
    import time

    start_time = time.time()

    try:
        # Import keyword_analysis_service
        from app.services.keyword_analysis_service import KeywordAnalysisService

        # Initialize service
        keyword_service = KeywordAnalysisService(db)

        # Convert mode to string value if needed
        mode_value = (
            request.mode.value if hasattr(request.mode, "value") else request.mode
        )

        # Call unified analysis service
        logger.info(f"Calling keyword_analysis_service with mode={mode_value}")
        analysis_result = await keyword_service.analyze_keywords(
            session_id=None,  # realtime doesn't have session concept
            transcript_segment=request.transcript,
            full_transcript=request.transcript,  # same as segment for realtime
            context="",  # no additional context for realtime
            analysis_type="island_parents",  # realtime is always island_parents
            mode=mode_value,  # "emergency" or "practice"
            db=db,
        )

        # Transform result to realtime API format
        # keyword_service returns: {safety_level, severity, quick_suggestions, detailed_scripts, ...}
        # realtime expects: {summary, alerts, suggestions, safety_level}

        # Extract quick_suggestions (from 200 expert sentences)
        quick_suggestions = analysis_result.get("quick_suggestions", [])

        # Build response_data
        response_data = {
            "safety_level": analysis_result.get("safety_level", "green"),
            "summary": analysis_result.get("display_text", "åˆ†æå®Œæˆ"),
            "alerts": [],  # Build from action_suggestion
            "suggestions": quick_suggestions,  # Use expert suggestions
        }

        # Add alerts from action_suggestion
        action_suggestion = analysis_result.get("action_suggestion", "")
        if action_suggestion:
            response_data["alerts"].append(action_suggestion)

        # Extract RAG sources for response
        rag_documents = analysis_result.get("rag_documents", [])
        rag_sources = [
            RAGSource(
                title=doc.get("title", ""),
                content=doc.get("content", "")[:300],  # Truncate to 300 chars
                score=round(float(doc.get("relevance_score", 0)), 2),
                theory="å…¶ä»–",  # Could detect theory from title if needed
            )
            for doc in rag_documents
        ]

        # Calculate latency
        latency_ms = int((time.time() - start_time) * 1000)
        provider_metadata = ProviderMetadata(
            provider="gemini", latency_ms=latency_ms, model="gemini-3-flash-preview"
        )

        # Calculate response time in milliseconds
        response_time_ms = latency_ms

        # Prepare data for BigQuery (asynchronous write)
        metadata = analysis_result.get("_metadata", {})
        gbq_data = {
            "id": str(uuid.uuid4()),
            "tenant_id": "island_parents",  # Fixed for web version
            "session_id": None,  # Web version has no session concept
            "analyzed_at": datetime.now(timezone.utc),
            "analysis_type": "realtime_analysis",  # Fixed: analysis method type
            "mode": mode_value,  # "emergency" or "practice"
            "safety_level": response_data[
                "safety_level"
            ],  # "green", "yellow", or "red"
            "matched_suggestions": quick_suggestions,
            "transcript_segment": request.transcript[:1000],  # Limit to 1000 chars
            "response_time_ms": response_time_ms,
            "created_at": datetime.now(timezone.utc),
            # Additional metadata from keyword_service
            "prompt_tokens": metadata.get("prompt_tokens", 0),
            "completion_tokens": metadata.get("completion_tokens", 0),
            "total_tokens": metadata.get("total_tokens", 0),
            "estimated_cost_usd": metadata.get("estimated_cost_usd", 0.0),
            "rag_used": metadata.get("rag_used", False),
            "rag_sources": metadata.get("rag_sources", []),
        }

        # Schedule GBQ write as background task (non-blocking)
        background_tasks.add_task(write_to_gbq_async, gbq_data)

        # Build response
        return RealtimeAnalyzeResponse(
            safety_level=response_data["safety_level"],
            summary=response_data["summary"],
            alerts=response_data["alerts"],
            suggestions=response_data["suggestions"],
            time_range=request.time_range,
            timestamp=datetime.now(timezone.utc).isoformat(),
            rag_sources=rag_sources,
            cache_metadata=None,  # Cache not used anymore
            provider_metadata=provider_metadata,
        )
    except Exception as e:
        logger.error(f"Realtime analysis failed: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/quick-feedback", response_model=QuickFeedbackResponse)
async def get_quick_feedback(request: QuickFeedbackRequest):
    """Generate quick AI-powered encouragement for 10-15 second intervals.

    This lightweight endpoint provides instant feedback while waiting for
    full analysis. Uses AI to read current context and respond appropriately.

    Example usage:
    - iOS polls every 10-15 seconds with recent transcript
    - Gets back a short encouragement message (< 20 chars)
    - Displays to user while waiting for 60-second full analysis

    Returns:
        QuickFeedbackResponse with AI-generated message and latency info
    """
    try:
        # Call quick feedback service
        result = await quick_feedback_service.get_quick_feedback(
            recent_transcript=request.recent_transcript
        )

        # Return response
        return QuickFeedbackResponse(
            message=result["message"],
            type=result["type"],
            timestamp=result["timestamp"],
            latency_ms=result["latency_ms"],
        )

    except Exception as e:
        logger.error(f"Quick feedback failed: {e}", exc_info=True)
        # Return fallback instead of raising error
        import datetime

        return QuickFeedbackResponse(
            message="ç¹¼çºŒä¿æŒï¼Œä½ åšå¾—å¾ˆå¥½",
            type="fallback_error",
            timestamp=datetime.datetime.now().isoformat(),
            latency_ms=0,
        )


@router.post("/elevenlabs-token")
async def generate_elevenlabs_token():
    """Generate a single-use token for ElevenLabs Speech-to-Text WebSocket.

    This endpoint calls ElevenLabs API to generate a temporary token that
    can be used by the frontend to connect to their WebSocket service.
    This approach keeps the API key secure on the server side.

    Returns:
        Dict with 'token' key containing the single-use token

    Raises:
        HTTPException: If token generation fails
    """
    try:
        # Get API key from environment
        api_key = os.getenv("ELEVEN_LABS_API_KEY")
        if not api_key:
            logger.error("ELEVEN_LABS_API_KEY not found in environment")
            raise HTTPException(
                status_code=500, detail="ElevenLabs API key not configured"
            )

        # Call ElevenLabs API to generate single-use token
        # Token type: "realtime_scribe" for speech-to-text WebSocket
        url = "https://api.elevenlabs.io/v1/single-use-token/realtime_scribe"
        headers = {"xi-api-key": api_key}

        async with httpx.AsyncClient() as client:
            response = await client.post(url, headers=headers, timeout=30.0)

            if response.status_code != 200:
                logger.error(
                    f"ElevenLabs API error: {response.status_code} - {response.text}"
                )
                raise HTTPException(
                    status_code=500,
                    detail=f"Failed to generate token: {response.text}",
                )

            token_data = response.json()
            logger.info("Successfully generated ElevenLabs token")

            return {"token": token_data.get("token")}

    except httpx.TimeoutException:
        logger.error("Timeout calling ElevenLabs API")
        raise HTTPException(
            status_code=504, detail="Timeout generating token from ElevenLabs"
        )
    except Exception as e:
        logger.error(f"Token generation failed: {e}")
        raise HTTPException(status_code=500, detail=f"Token generation failed: {e}")


@router.post("/parents-report", response_model=ParentsReportResponse)
async def generate_parents_report(
    request: ParentsReportRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
):
    """Generate a comprehensive parenting communication report.

    Analyzes parent-child conversation transcript and provides:
    1. Summary/theme of the conversation (neutral stance)
    2. Communication highlights (what went well)
    3. Areas for improvement with specific suggestions
    4. Relevant RAG references from parenting knowledge base

    This endpoint queries the parenting RAG knowledge base and uses
    Gemini to generate structured feedback.

    Results are persisted to BigQuery for analytics.
    """
    import json
    import time
    import uuid

    # Track timing for GBQ
    start_time = datetime.now(timezone.utc)
    rag_start_time = None
    rag_end_time = None
    llm_start_time = None
    llm_end_time = None

    try:
        # Step 1: Search RAG knowledge base for relevant parenting content
        logger.info("Searching RAG for parenting-related content")
        rag_start_time = time.time()
        rag_sources = await _search_rag_knowledge(
            transcript=request.transcript,
            db=db,
            top_k=5,  # Get more sources for comprehensive report
            similarity_threshold=0.5,
        )
        rag_end_time = time.time()
        rag_search_time_ms = int((rag_end_time - rag_start_time) * 1000)

        # Step 2: Build RAG context for prompt
        rag_context = ""
        if rag_sources:
            rag_context_parts = ["\n\nğŸ“š ç›¸é—œè¦ªå­æ•™é¤ŠçŸ¥è­˜åº«å…§å®¹ï¼ˆä¾›åƒè€ƒï¼‰ï¼š\n"]
            for idx, source in enumerate(rag_sources, 1):
                rag_context_parts.append(
                    f"[{idx}] {source.title} ({source.theory}): {source.content}"
                )
            rag_context = "\n".join(rag_context_parts)
            logger.info(f"Found {len(rag_sources)} relevant RAG sources")
        else:
            logger.info("No RAG sources found, proceeding without context")

        # Step 3: Build analysis prompt
        analysis_prompt = f"""ä½ æ˜¯å°ˆæ¥­çš„è¦ªå­æºé€šåˆ†æå¸«ï¼Œè² è²¬åˆ†æå®¶é•·èˆ‡å­©å­çš„å°è©±ï¼Œæä¾›å»ºè¨­æ€§çš„å›é¥‹ã€‚

ã€å°è©±é€å­—ç¨¿ã€‘
{request.transcript}

{rag_context}

ã€åˆ†æè¦æ±‚ã€‘
è«‹ä»¥ä¸­æ€§ã€å®¢è§€ã€æº«å’Œçš„ç«‹å ´åˆ†æé€™æ¬¡å°è©±ï¼Œæä¾›ä»¥ä¸‹ 4 å€‹éƒ¨åˆ†ï¼š

1. **å°è©±ä¸»é¡Œèˆ‡æ‘˜è¦**ï¼ˆsummaryï¼‰
   - ç°¡çŸ­èªªæ˜é€™æ¬¡å°è©±çš„ä¸»é¡Œæ˜¯ä»€éº¼
   - ä¸­æ€§ç«‹å ´ï¼Œä¸æ‰¹åˆ¤ï¼Œè®“å®¶é•·çŸ¥é“ã€Œé€™æ¬¡åˆ°åº•èªªäº†ä»€éº¼ã€
   - 1-2 å¥è©±å³å¯

2. **æºé€šäº®é»**ï¼ˆhighlightsï¼‰
   - åˆ—å‡ºå®¶é•·åœ¨æºé€šä¸­åšå¾—å¥½çš„åœ°æ–¹
   - ä¾‹å¦‚ï¼šå±•ç¾åŒç†å¿ƒã€é¡˜æ„å‚¾è½ã€å˜—è©¦ç†è§£å­©å­æ„Ÿå—ç­‰
   - ç”¨æ­£å‘ã€é¼“å‹µçš„èªæ°£
   - 3-5 å€‹äº®é»ï¼Œæ¯å€‹ â‰¤ 30 å­—

3. **æ”¹é€²å»ºè­°**ï¼ˆimprovementsï¼‰
   - æŒ‡å‡ºå€¼å¾—æ›´å¥½çš„åœ°æ–¹
   - æä¾›å…·é«”ã€å¯æ“ä½œçš„å»ºè­°æˆ–æ›å¥è©±èªª
   - æº«å’Œã€éæ‰¹åˆ¤çš„èªæ°£
   - æ¯å€‹å»ºè­°åŒ…å«ï¼š
     * issue: éœ€è¦æ”¹é€²çš„åœ°æ–¹ï¼ˆå…·é«”æè¿°ï¼Œâ‰¤ 40 å­—ï¼‰
     * suggestion: å…·é«”å»ºè­°æˆ–æ›å¥è©±èªªï¼ˆâ‰¤ 60 å­—ï¼‰
   - 2-4 å€‹å»ºè­°

4. **çŸ¥è­˜åº«åƒè€ƒ**ï¼ˆrag_referencesï¼‰
   - å·²è‡ªå‹•æä¾›ä¸Šæ–¹çš„ RAG çŸ¥è­˜åº«å…§å®¹
   - ä½ ä¸éœ€è¦é¡å¤–è™•ç†ï¼Œåªéœ€åœ¨åˆ†ææ™‚åƒè€ƒå³å¯

ã€èªæ°£è¦æ±‚ã€‘
- æº«å’Œã€åŒç†ã€å»ºè¨­æ€§
- é¿å…æ‰¹åˆ¤æˆ–è®“å®¶é•·æ„Ÿåˆ°è¢«æŒ‡è²¬
- ç”¨ã€Œå¯ä»¥è©¦è©¦ã€ã€Œæˆ–è¨±ã€ã€Œæ›å€‹æ–¹å¼ã€ç­‰æŸ”å’Œå¼•å°è©
- ç„¦é»æ”¾åœ¨ã€Œå¦‚ä½•åšå¾—æ›´å¥½ã€è€Œéã€Œå“ªè£¡åšéŒ¯ã€

ã€è¼¸å‡ºæ ¼å¼ã€‘
è«‹ä»¥ JSON æ ¼å¼å›æ‡‰ï¼ˆä¸è¦ç”¨ markdown code blockï¼Œç›´æ¥è¼¸å‡º JSONï¼‰ï¼š

{{
  "summary": "å°è©±ä¸»é¡Œæ‘˜è¦ï¼ˆ1-2 å¥ï¼‰",
  "highlights": [
    "äº®é»1ï¼ˆâ‰¤ 30 å­—ï¼‰",
    "äº®é»2ï¼ˆâ‰¤ 30 å­—ï¼‰",
    "äº®é»3ï¼ˆâ‰¤ 30 å­—ï¼‰"
  ],
  "improvements": [
    {{
      "issue": "éœ€è¦æ”¹é€²çš„åœ°æ–¹ï¼ˆâ‰¤ 40 å­—ï¼‰",
      "suggestion": "å…·é«”å»ºè­°æˆ–æ›å¥è©±èªªï¼ˆâ‰¤ 60 å­—ï¼‰"
    }}
  ]
}}

âš ï¸ é‡è¦ï¼šè«‹ç¢ºä¿ JSON æ ¼å¼æ­£ç¢ºï¼Œä¸è¦åœ¨æœ€å¾Œä¸€å€‹å…ƒç´ å¾Œé¢åŠ é€—è™Ÿï¼ˆtrailing commaï¼‰ï¼

è«‹é–‹å§‹åˆ†æã€‚"""

        # Step 4: Call Gemini for analysis
        logger.info("Calling Gemini for report generation")
        llm_start_time = time.time()
        gemini_response = await gemini_service.chat_completion(
            prompt=analysis_prompt,
            temperature=0.7,  # Higher temperature for more natural language
            return_metadata=True,  # Get usage metadata for observability
        )
        llm_end_time = time.time()
        llm_call_time_ms = int((llm_end_time - llm_start_time) * 1000)

        # Extract response text and metadata
        llm_raw_response = gemini_response["text"]
        usage_metadata = gemini_response.get("usage_metadata", {})

        # Step 5: Parse Gemini response
        try:
            # Try to extract JSON from response
            if "```json" in llm_raw_response:
                json_start = llm_raw_response.find("```json") + 7
                json_end = llm_raw_response.find("```", json_start)
                json_text = llm_raw_response[json_start:json_end].strip()
            elif "{" in llm_raw_response:
                json_start = llm_raw_response.find("{")
                json_end = llm_raw_response.rfind("}") + 1
                json_text = llm_raw_response[json_start:json_end]
            else:
                raise ValueError("No JSON found in response")

            # Remove trailing commas from JSON (common LLM mistake)
            import re

            json_text = re.sub(r",(\s*[}\]])", r"\1", json_text)

            analysis = json.loads(json_text)
            logger.info("Successfully parsed Gemini response")

        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"Failed to parse Gemini response: {e}")
            logger.error(f"Response text: {llm_raw_response[:500]}")
            raise HTTPException(
                status_code=500, detail="Failed to parse AI analysis response"
            )

        # Step 6: Build response
        improvements_list = [
            ImprovementSuggestion(
                issue=item.get("issue", ""), suggestion=item.get("suggestion", "")
            )
            for item in analysis.get("improvements", [])
        ]

        response = ParentsReportResponse(
            summary=analysis.get("summary", ""),
            highlights=analysis.get("highlights", []),
            improvements=improvements_list,
            rag_references=rag_sources,
            timestamp=datetime.now(timezone.utc).isoformat(),
        )

        # Step 7: Prepare GBQ data for analytics
        end_time = datetime.now(timezone.utc)
        duration_ms = int((end_time - start_time).total_seconds() * 1000)

        # Determine safety level based on number of improvements
        # More improvements = more issues = higher concern level
        safety_level = "green"  # Default
        if len(improvements_list) >= 4:
            safety_level = "yellow"  # Multiple areas to improve
        elif len(improvements_list) >= 2:
            safety_level = "green"  # Normal feedback
        # Note: parents_report doesn't have explicit "red" level like realtime

        # Prepare comprehensive GBQ data for observability
        gbq_data = {
            # IDs and metadata
            "id": str(uuid.uuid4()),
            "tenant_id": "island_parents",
            "session_id": request.session_id if request.session_id else None,
            "request_id": str(uuid.uuid4()),
            # Timestamps
            "analyzed_at": start_time,
            "start_time": start_time,
            "end_time": end_time,
            "created_at": end_time,
            # Analysis type and result
            "analysis_type": "parents_report",
            "safety_level": safety_level,
            "matched_suggestions": [
                f"{imp.issue} â†’ {imp.suggestion}" for imp in improvements_list
            ],
            "analysis_result": analysis,  # Full parsed JSON
            # Input data
            "transcript": request.transcript,  # Store full transcript
            "speakers": None,  # Parents report doesn't have speaker info
            # Prompts
            "system_prompt": None,  # Gemini doesn't separate system/user in this call
            "user_prompt": analysis_prompt,  # The full prompt
            "prompt_template": "parents_report_v1",
            # RAG information
            "rag_used": len(rag_sources) > 0,
            "rag_query": request.transcript[
                :200
            ],  # First 200 chars used for RAG search
            "rag_documents": [
                {
                    "content": source.content[:500],
                    "source": source.title,
                    "similarity": source.score,
                }
                for source in rag_sources
            ]
            if rag_sources
            else None,
            "rag_sources": [source.title for source in rag_sources]
            if rag_sources
            else [],
            "rag_top_k": 5,
            "rag_similarity_threshold": 0.5,
            "rag_search_time_ms": rag_search_time_ms if rag_start_time else None,
            # Model information
            "provider": "gemini",
            "model_name": "gemini-3-flash-preview",
            "model_version": "3.0",
            # Timing breakdown
            "duration_ms": duration_ms,
            "api_response_time_ms": duration_ms,
            "llm_call_time_ms": llm_call_time_ms if llm_start_time else None,
            # LLM response
            "llm_raw_response": llm_raw_response,
            # Token usage (from Gemini usage_metadata)
            "prompt_tokens": usage_metadata.get("prompt_token_count"),
            "completion_tokens": usage_metadata.get("candidates_token_count"),
            "total_tokens": usage_metadata.get("total_token_count"),
            "cached_tokens": usage_metadata.get("cached_content_token_count"),
            "estimated_cost_usd": _calculate_gemini_cost(usage_metadata)
            if usage_metadata
            else None,
            # Cache info (not used in parents_report)
            "use_cache": False,
            "cache_hit": None,
            "cache_key": None,
            "gemini_cache_ttl": None,
            # Mode
            "mode": "parents_report",
        }

        # Schedule GBQ write as background task (non-blocking)
        background_tasks.add_task(write_to_gbq_async, gbq_data)

        logger.info(
            f"Parents report generated successfully in {duration_ms}ms with {len(improvements_list)} improvements"
        )

        return response

    except HTTPException:
        # Re-raise HTTP exceptions
        raise
    except Exception as e:
        logger.error(f"Parents report generation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))
