"""
Realtime STT Counseling API
"""
import logging
import os
from datetime import datetime, timezone
from typing import List

import httpx
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session

from app.core.database import get_db
from app.schemas.realtime import (
    CacheMetadata,
    ProviderMetadata,
    RAGSource,
    RealtimeAnalyzeRequest,
    RealtimeAnalyzeResponse,
)
from app.services.cache_manager import CacheManager
from app.services.gemini_service import GeminiService
from app.services.openai_service import OpenAIService
from app.services.rag_chat_service import RAGChatService

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/v1/realtime", tags=["Realtime Counseling"])

# Initialize services
gemini_service = GeminiService()
openai_service = OpenAIService()
cache_manager = CacheManager()

# System instruction for cache (å›ºå®šä¸è®Šçš„éƒ¨åˆ†)
CACHE_SYSTEM_INSTRUCTION = """ä½ æ˜¯å°ˆæ¥­è«®è©¢ç£å°ï¼Œåˆ†æå³æ™‚è«®è©¢å°è©±ã€‚ä½ çš„è§’è‰²æ˜¯ç«™åœ¨æ¡ˆä¸»èˆ‡è«®è©¢å¸«ä¹‹é–“ï¼Œæä¾›æº«æš–ã€åŒç†ä¸”å…·é«”å¯è¡Œçš„å°ˆæ¥­å»ºè­°ã€‚

ã€è§’è‰²å®šç¾©ã€‘CRITICAL - å¿…é ˆåš´æ ¼éµå®ˆï¼š
- "counselor" = è«®è©¢å¸«/è¼”å°å¸«ï¼ˆå°ˆæ¥­åŠ©äººè€…ï¼Œæä¾›å”åŠ©çš„ä¸€æ–¹ï¼‰
- "client" = æ¡ˆä¸»/å€‹æ¡ˆ/å®¶é•·ï¼ˆæ±‚åŠ©è€…ï¼Œæœ‰å›°æ“¾éœ€è¦å”åŠ©çš„ä¸€æ–¹ï¼‰
- æ‰€æœ‰å•é¡Œã€å›°æ“¾ã€ç—‡ç‹€éƒ½æ˜¯ã€Œæ¡ˆä¸»/å€‹æ¡ˆã€é¢è‡¨çš„ï¼Œä¸æ˜¯è«®è©¢å¸«çš„å•é¡Œ
- åˆ†æç„¦é»ï¼šæ¡ˆä¸»çš„ç‹€æ³ã€éœ€æ±‚ã€é¢¨éšª
- å»ºè­°å°è±¡ï¼šçµ¦è«®è©¢å¸«çš„å°ˆæ¥­å»ºè­°ï¼ˆå¦‚ä½•å”åŠ©æ¡ˆä¸»ï¼‰

ã€åˆ†æç¯„åœã€‘CRITICAL - å¿…é ˆåš´æ ¼éµå®ˆï¼š
ğŸ¯ **ä¸»è¦åˆ†æç„¦é»**ï¼šæœ€æ–°ä¸€åˆ†é˜å…§çš„å°è©±å…§å®¹
   - ä½ æœƒæ”¶åˆ°å®Œæ•´çš„å°è©±è¨˜éŒ„ï¼ˆå¯èƒ½é•·é”æ•¸ååˆ†é˜ï¼‰
   - ä½†ä½ çš„åˆ†æå¿…é ˆèšç„¦åœ¨ã€Œæœ€å¾Œå‡ºç¾çš„å°è©±ã€ï¼ˆæœ€æ–°ä¸€åˆ†é˜ï¼‰
   - å‰é¢çš„å°è©±åƒ…ä½œç‚ºèƒŒæ™¯è„ˆçµ¡åƒè€ƒï¼Œå¹«åŠ©ä½ ç†è§£å‰å› å¾Œæœ

ã€æ ¸å¿ƒåŸå‰‡ã€‘åŒç†å„ªå…ˆã€æº«å’Œå¼•å°ã€å…·é«”è¡Œå‹•ï¼š

1. **åŒç†èˆ‡ç†è§£ç‚ºå…ˆ**
   - æ°¸é å…ˆç†è§£èˆ‡åŒç†æ¡ˆä¸»ï¼ˆå®¶é•·ï¼‰çš„æ„Ÿå—å’Œè™•å¢ƒ
   - èªå¯æ•™é¤Šå£“åŠ›ã€æƒ…ç·’å¤±æ§æ˜¯æ­£å¸¸çš„äººæ€§åæ‡‰
   - é¿å…æ‰¹åˆ¤ã€æŒ‡è²¬æˆ–è®“æ¡ˆä¸»æ„Ÿåˆ°è¢«å¦å®š

2. **æº«å’Œã€éæ‰¹åˆ¤çš„èªæ°£**
   - âŒ ç¦æ­¢ç”¨èªï¼šã€Œè¡¨é”å‡ºå°å­©å­ä½¿ç”¨èº«é«”æš´åŠ›çš„è¡å‹•ã€ã€Œå¯èƒ½é€ æˆå‚·å®³ã€ã€Œä¸ç•¶ç®¡æ•™ã€
   - âœ… å»ºè­°ç”¨èªï¼šã€Œç†è§£åˆ°åœ¨æ•™é¤Šå£“åŠ›ä¸‹ï¼Œçˆ¶æ¯æœ‰æ™‚æœƒæ„Ÿåˆ°æƒ…ç·’å¤±æ§æ˜¯å¾ˆæ­£å¸¸çš„ã€
   - âœ… ä½¿ç”¨ï¼šã€Œå¯ä»¥è€ƒæ…®ã€ã€Œæˆ–è¨±ã€ã€Œè©¦è©¦çœ‹ã€ç­‰æŸ”å’Œå¼•å°è©
   - âœ… ç„¦é»æ”¾åœ¨ã€Œå¦‚ä½•èª¿æ•´ã€è€Œéã€Œå“ªè£¡åšéŒ¯ã€

3. **å…·é«”ã€ç°¡æ½”çš„å»ºè­°**
   - å»ºè­°è¦å…·é«”å¯è¡Œï¼Œä½†ä¿æŒç°¡çŸ­ï¼ˆä¸è¶…é 50 å­—ï¼‰
   - é¿å…æŠ½è±¡æ¦‚å¿µï¼Œç”¨å…·é«”åšæ³•
   - ä¸è¦å†—é•·çš„æ­¥é©Ÿèªªæ˜æˆ–å°è©±ç¯„ä¾‹

ã€è¼¸å‡ºæ ¼å¼ã€‘è«‹æä¾›ä»¥ä¸‹ JSON æ ¼å¼å›æ‡‰ï¼š

{
  "summary": "æ¡ˆä¸»è™•å¢ƒç°¡è¿°ï¼ˆ1-2 å¥ï¼‰",
  "alerts": [
    "ğŸ’¡ åŒç†æ¡ˆä¸»æ„Ÿå—ï¼ˆ1 å¥ï¼‰",
    "âš ï¸ éœ€é—œæ³¨çš„éƒ¨åˆ†ï¼ˆ1 å¥ï¼‰"
  ],
  "suggestions": [
    "ğŸ’¡ æ ¸å¿ƒå»ºè­°ï¼ˆç°¡çŸ­ï¼Œ< 50 å­—ï¼‰",
    "ğŸ’¡ å…·é«”åšæ³•ï¼ˆç°¡çŸ­ï¼Œ< 50 å­—ï¼‰"
  ]
}

ã€èªæ°£è¦æ±‚ã€‘æº«å’Œã€åŒç†ã€ç°¡æ½”ï¼Œé¿å…æ‰¹åˆ¤æˆ–éåº¦èªªæ•™ã€‚
"""

# Parenting-related keywords that trigger RAG search
PARENTING_KEYWORDS = [
    "è¦ªå­",
    "å­©å­",
    "å°å­©",
    "å…’ç«¥",
    "é’å°‘å¹´",
    "æ•™é¤Š",
    "è‚²å…’",
    "ç®¡æ•™",
    "æºé€š",
    "æƒ…ç·’",
    "è¡Œç‚º",
    "å­¸ç¿’",
    "ç™¼å±•",
    "æˆé•·",
    "å›é€†",
    "é’æ˜¥æœŸ",
    "è¦ªè·",
    "å®¶åº­",
    "çˆ¶æ¯",
    "åª½åª½",
    "çˆ¸çˆ¸",
    "æ•™è‚²",
    "é™ªä¼´",
    "é—œä¿‚",
    "è¡çª",
]


def _detect_parenting_keywords(transcript: str) -> bool:
    """Detect if transcript contains parenting-related keywords.

    Args:
        transcript: The transcript text

    Returns:
        True if parenting keywords detected, False otherwise
    """
    transcript_lower = transcript.lower()
    for keyword in PARENTING_KEYWORDS:
        if keyword in transcript_lower:
            logger.info(f"Parenting keyword detected: {keyword}")
            return True
    return False


def _detect_parenting_theory(title: str) -> str:
    """Detect which parenting theory a document belongs to based on title.

    Args:
        title: Document title

    Returns:
        Theory name in Chinese (e.g., "æ­£å‘æ•™é¤Š", "æƒ…ç·’æ•™é¤Š")
    """
    # Theory keyword mappings (Chinese and English)
    theory_mappings = {
        "æ­£å‘æ•™é¤Š": ["æ­£å‘æ•™é¤Š", "Positive Discipline"],
        "æƒ…ç·’æ•™é¤Š": ["æƒ…ç·’æ•™é¤Š", "Emotional Coaching", "Emotion Coaching"],
        "ä¾é™„ç†è«–": ["ä¾é™„ç†è«–", "Attachment Theory"],
        "èªçŸ¥ç™¼å±•ç†è«–": ["èªçŸ¥ç™¼å±•", "Cognitive Development"],
        "è‡ªæˆ‘æ±ºå®šè«–": ["è‡ªæˆ‘æ±ºå®š", "Self-Determination"],
    }

    # Check each theory's keywords
    for theory_name, keywords in theory_mappings.items():
        for keyword in keywords:
            if keyword in title:
                return theory_name

    # Default if no match found
    return "å…¶ä»–"


async def _search_rag_knowledge(
    transcript: str, db: Session, top_k: int = 3, similarity_threshold: float = 0.7
) -> List[RAGSource]:
    """Search RAG knowledge base for relevant parenting content.

    Args:
        transcript: The transcript text to search
        db: Database session
        top_k: Number of top results to return
        similarity_threshold: Minimum similarity score

    Returns:
        List of RAG sources with title, content, and score
    """
    try:
        # Initialize RAG service
        rag_service = RAGChatService(db=db)

        # Generate embedding for transcript
        query_embedding = await openai_service.create_embedding(transcript)

        # Search similar chunks with parenting category filter
        rows = await rag_service.search_similar_chunks(
            query_embedding=query_embedding,
            top_k=top_k,
            similarity_threshold=similarity_threshold,
            category="parenting",  # Filter for parenting documents only
        )

        # Build RAG sources response
        rag_sources = []
        for row in rows:
            # Detect theory from document title
            theory = _detect_parenting_theory(row.document_title)

            rag_sources.append(
                RAGSource(
                    title=row.document_title,
                    content=row.text[:300],  # Truncate to 300 chars
                    score=round(float(row.similarity_score), 2),
                    theory=theory,
                )
            )

        logger.info(f"RAG search found {len(rag_sources)} relevant sources")
        return rag_sources

    except Exception as e:
        logger.error(f"RAG search failed: {e}")
        # Return empty list on error (fallback)
        return []


async def _analyze_with_codeer(
    transcript: str,
    speakers: List[dict],
    rag_context: str,
    db: Session,
    session_id: str = "",
    model: str = "gpt5-mini",
) -> dict:
    """Analyze transcript using Codeer è¦ªå­å°ˆå®¶ agent.

    Args:
        transcript: Full transcript text
        speakers: List of speaker segments
        rag_context: RAG knowledge context
        db: Database session
        session_id: Session ID for session pooling (optional)
        model: Codeer model selection (claude-sonnet, gemini-flash, or gpt5-mini)

    Returns:
        Dict with summary, alerts, suggestions
    """
    import json

    from app.services.codeer_client import CodeerClient, get_codeer_agent_id
    from app.services.codeer_session_pool import get_codeer_session_pool

    # Get agent ID based on model selection
    try:
        agent_id = get_codeer_agent_id(model)
        logger.info(f"Using Codeer model: {model}, agent_id: {agent_id}")
    except Exception as e:
        logger.error(f"Failed to get Codeer agent ID: {e}")
        raise HTTPException(status_code=400, detail=str(e))

    # Create client with longer timeout for analysis
    client = CodeerClient()
    # Extend timeout to 60 seconds for LLM response
    client.client.timeout = httpx.Timeout(60.0)

    try:
        # Build analysis prompt similar to Gemini's
        # Format: system instruction + RAG context + transcript
        prompt = f"""{CACHE_SYSTEM_INSTRUCTION}

{rag_context if rag_context else ""}

ã€æœ€æ–°å°è©±é€å­—ç¨¿ã€‘
{transcript}

ã€Speaker ç‰‡æ®µã€‘
{json.dumps(speakers, ensure_ascii=False, indent=2)}

è«‹åˆ†æä»¥ä¸Šå°è©±ï¼Œæä¾› JSON æ ¼å¼å›æ‡‰ã€‚
"""

        # Create or reuse chat session with selected agent
        if session_id:
            # Use session pool for reuse
            pool = get_codeer_session_pool()
            chat = await pool.get_or_create_session(
                session_id, client, agent_id=agent_id
            )
            logger.info(f"Using session pool for {session_id} with agent {agent_id}")
        else:
            # Fallback: create new chat with selected agent
            # Use microsecond precision + model name to ensure unique chat names
            import uuid

            unique_suffix = (
                f"{datetime.now().strftime('%Y%m%d-%H%M%S')}-{uuid.uuid4().hex[:8]}"
            )
            chat = await client.create_chat(
                name=f"Realtime-{model}-{unique_suffix}",
                agent_id=agent_id,
            )
            logger.info(
                f"Created new chat (no session_id) with agent {agent_id}: {chat['name']}"
            )

        # Send message to Codeer agent (non-streaming for now)
        # CRITICAL: Must pass agent_id to match the agent used to create the chat
        try:
            response = await client.send_message(
                chat_id=chat["id"], message=prompt, stream=False, agent_id=agent_id
            )
        except Exception as api_error:
            # Handle Codeer API errors (e.g., agent mismatch, timeout, rate limit)
            logger.error(f"Codeer send_message failed: {api_error}")
            return {
                "summary": "Codeer åˆ†æå¤±æ•—",
                "alerts": [f"âš ï¸ API éŒ¯èª¤: {str(api_error)}"],
                "suggestions": ["ğŸ’¡ è«‹æª¢æŸ¥ Codeer API é€£ç·šæˆ–ç¨å¾Œå†è©¦"],
            }

        # Parse Codeer response
        # Codeer returns dict with 'content', 'text', or 'message' field
        try:
            # Extract text from response
            if isinstance(response, dict):
                response_text = (
                    response.get("content")
                    or response.get("text")
                    or response.get("message")
                    or str(response)
                )
            else:
                response_text = str(response)

            # Extract JSON from response
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                json_text = response_text[json_start:json_end].strip()
            elif "{" in response_text:
                json_start = response_text.find("{")
                json_end = response_text.rfind("}") + 1
                json_text = response_text[json_start:json_end]
            else:
                # Fallback: treat as plain text
                json_text = response_text

            analysis = json.loads(json_text)

            # Ensure required fields exist
            if "summary" not in analysis:
                analysis["summary"] = "åˆ†æçµæœ"
            if "alerts" not in analysis:
                analysis["alerts"] = []
            if "suggestions" not in analysis:
                analysis["suggestions"] = []

            return analysis

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse Codeer response as JSON: {e}")
            logger.error(f"Response text: {response_text[:500]}")

            # Fallback response
            return {
                "summary": "Codeer å›æ‡‰è§£æå¤±æ•—",
                "alerts": [f"âš ï¸ ç„¡æ³•è§£æå›æ‡‰: {str(e)}"],
                "suggestions": ["ğŸ’¡ è«‹æª¢æŸ¥ Codeer agent è¨­å®š"],
            }

    finally:
        # Always close the client
        await client.close()


@router.post("/analyze", response_model=RealtimeAnalyzeResponse)
async def analyze_transcript(
    request: RealtimeAnalyzeRequest, db: Session = Depends(get_db)
):
    """Analyze realtime counseling transcript with AI supervision.

    Returns summary, alerts, and suggestions for the counselor based on
    the conversation in the past 60 seconds.

    Supports multiple LLM providers: Gemini (default) and Codeer.
    Gemini supports explicit context caching for improved performance.

    This is a demo feature with no authentication required.
    """
    import time

    start_time = time.time()

    try:
        # Convert speakers to dict format for service
        speakers_dict = [
            {"speaker": s.speaker, "text": s.text} for s in request.speakers
        ]

        # Detect parenting keywords and trigger RAG if needed
        rag_sources = []
        rag_context = ""

        if _detect_parenting_keywords(request.transcript):
            logger.info("Parenting keywords detected, triggering RAG search")
            rag_sources = await _search_rag_knowledge(
                transcript=request.transcript, db=db, top_k=3, similarity_threshold=0.7
            )

            # Build RAG context for Gemini prompt
            if rag_sources:
                rag_context_parts = ["\n\nğŸ“š ç›¸é—œè¦ªå­æ•™é¤ŠçŸ¥è­˜åº«å…§å®¹ï¼ˆä¾›åƒè€ƒï¼‰ï¼š\n"]
                for idx, source in enumerate(rag_sources, 1):
                    rag_context_parts.append(
                        f"[{idx}] {source.title}: {source.content[:200]}..."
                    )
                rag_context = "\n".join(rag_context_parts)

        # Initialize variables
        analysis = {}
        cache_metadata = None
        provider_metadata = None

        # Route to appropriate provider
        if request.provider == "codeer":
            logger.info(
                f"Using Codeer provider for analysis (model: {request.codeer_model})"
            )
            analysis = await _analyze_with_codeer(
                transcript=request.transcript,
                speakers=speakers_dict,
                rag_context=rag_context,
                db=db,
                session_id=request.session_id,
                model=request.codeer_model,
            )

            # Calculate latency
            latency_ms = int((time.time() - start_time) * 1000)
            provider_metadata = ProviderMetadata(
                provider="codeer",
                latency_ms=latency_ms,
                model=f"è¦ªå­å°ˆå®¶ ({request.codeer_model})",
            )

        # Default to Gemini
        else:
            logger.info("Using Gemini provider for analysis")

            # Use cache if enabled and session_id is provided
            if request.use_cache and request.session_id:
                try:
                    logger.info(
                        f"Cache enabled for session {request.session_id}, "
                        f"attempting to get or create cache"
                    )

                    # Get or create cache with accumulated transcript
                    cached_content, is_new = await cache_manager.get_or_create_cache(
                        session_id=request.session_id,
                        system_instruction=CACHE_SYSTEM_INSTRUCTION,
                        accumulated_transcript=request.transcript,
                        ttl_seconds=7200,  # 2 hours
                    )

                    # Check if content is too short for caching
                    if cached_content is None:
                        logger.info(
                            "Content too short for caching, using standard analysis"
                        )
                        analysis = await gemini_service.analyze_realtime_transcript(
                            transcript=request.transcript,
                            speakers=speakers_dict,
                            rag_context=rag_context,
                        )
                        cache_metadata = CacheMetadata(
                            cache_name="",
                            cache_created=False,
                            cached_tokens=0,
                            prompt_tokens=0,
                            message="å°è©±å…§å®¹è¼ƒçŸ­ï¼Œå°šæœªå•Ÿç”¨ cacheï¼ˆéœ€ >= 1024 tokensï¼‰",
                        )
                    else:
                        # Analyze with cache
                        analysis = await gemini_service.analyze_with_cache(
                            cached_content=cached_content,
                            transcript=request.transcript,
                            speakers=speakers_dict,
                            rag_context=rag_context,
                        )

                        # Extract cache metadata from usage_metadata
                        usage_metadata = analysis.get("usage_metadata", {})
                        cache_metadata = CacheMetadata(
                            cache_name=cached_content.name,
                            cache_created=is_new,
                            cached_tokens=usage_metadata.get(
                                "cached_content_token_count", 0
                            ),
                            prompt_tokens=usage_metadata.get("prompt_token_count", 0),
                        )

                        logger.info(
                            f"Cache analysis completed. Cache created: {is_new}, "
                            f"Cached tokens: {cache_metadata.cached_tokens}, "
                            f"Prompt tokens: {cache_metadata.prompt_tokens}"
                        )

                except Exception as cache_error:
                    # Cache failed, fallback to non-cached analysis
                    logger.warning(
                        f"Cache analysis failed, falling back to non-cached: {cache_error}"
                    )
                    analysis = await gemini_service.analyze_realtime_transcript(
                        transcript=request.transcript,
                        speakers=speakers_dict,
                        rag_context=rag_context,
                    )
                    cache_metadata = CacheMetadata(
                        cache_name="",
                        cache_created=False,
                        cached_tokens=0,
                        prompt_tokens=0,
                        error=str(cache_error),
                    )
            else:
                # Cache disabled or no session_id, use standard analysis
                logger.info("Cache disabled or no session_id, using standard analysis")
                analysis = await gemini_service.analyze_realtime_transcript(
                    transcript=request.transcript,
                    speakers=speakers_dict,
                    rag_context=rag_context,
                )

            # Calculate latency
            latency_ms = int((time.time() - start_time) * 1000)
            provider_metadata = ProviderMetadata(
                provider="gemini", latency_ms=latency_ms, model="gemini-2.5-flash"
            )

        # Build response
        return RealtimeAnalyzeResponse(
            summary=analysis.get("summary", ""),
            alerts=analysis.get("alerts", []),
            suggestions=analysis.get("suggestions", []),
            time_range=request.time_range,
            timestamp=datetime.now(timezone.utc).isoformat(),
            rag_sources=rag_sources,
            cache_metadata=cache_metadata,
            provider_metadata=provider_metadata,
        )
    except Exception as e:
        logger.error(f"Realtime analysis failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/elevenlabs-token")
async def generate_elevenlabs_token():
    """Generate a single-use token for ElevenLabs Speech-to-Text WebSocket.

    This endpoint calls ElevenLabs API to generate a temporary token that
    can be used by the frontend to connect to their WebSocket service.
    This approach keeps the API key secure on the server side.

    Returns:
        Dict with 'token' key containing the single-use token

    Raises:
        HTTPException: If token generation fails
    """
    try:
        # Get API key from environment
        api_key = os.getenv("ELEVEN_LABS_API_KEY")
        if not api_key:
            logger.error("ELEVEN_LABS_API_KEY not found in environment")
            raise HTTPException(
                status_code=500, detail="ElevenLabs API key not configured"
            )

        # Call ElevenLabs API to generate single-use token
        # Token type: "realtime_scribe" for speech-to-text WebSocket
        url = "https://api.elevenlabs.io/v1/single-use-token/realtime_scribe"
        headers = {"xi-api-key": api_key}

        async with httpx.AsyncClient() as client:
            response = await client.post(url, headers=headers, timeout=30.0)

            if response.status_code != 200:
                logger.error(
                    f"ElevenLabs API error: {response.status_code} - {response.text}"
                )
                raise HTTPException(
                    status_code=500,
                    detail=f"Failed to generate token: {response.text}",
                )

            token_data = response.json()
            logger.info("Successfully generated ElevenLabs token")

            return {"token": token_data.get("token")}

    except httpx.TimeoutException:
        logger.error("Timeout calling ElevenLabs API")
        raise HTTPException(
            status_code=504, detail="Timeout generating token from ElevenLabs"
        )
    except Exception as e:
        logger.error(f"Token generation failed: {e}")
        raise HTTPException(status_code=500, detail=f"Token generation failed: {e}")
