"""
Quick Feedback Service - è¼•é‡ AI å›žé¥‹æœå‹™

ä½¿ç”¨è¼•é‡ç´š Gemini Flash prompt åˆ¤æ–·å®‰å…¨ç­‰ç´šï¼Œ
ç„¶å¾Œå¾ž 200 å¥å°ˆå®¶å»ºè­°ä¸­é¸æ“‡æœ€é©åˆçš„å›žé¥‹ã€‚

æ”¹é€²ï¼šä¸å†è®“ AI è‡ªç”±ç”Ÿæˆæ–‡å­—ï¼Œè€Œæ˜¯å¾žé è¨­çš„ 200 å¥çŸ­å»ºè­°ä¸­é¸æ“‡ï¼Œ
ç¢ºä¿é¡¯ç¤ºåœ¨åŒå¿ƒåœ“ UI ä¸­çš„æ–‡å­—è¶³å¤ ç°¡çŸ­ï¼ˆå¹³å‡ 9 å­—ï¼‰ã€‚
"""

import datetime
import json
import logging
import time
from typing import Dict, Optional

from app.config.parenting_suggestions import (
    GREEN_SUGGESTIONS,
    RED_SUGGESTIONS,
    YELLOW_SUGGESTIONS,
)
from app.services.external.gemini_service import GeminiService

logger = logging.getLogger(__name__)


class QuickFeedbackService:
    """è¼•é‡ AI å¿«é€Ÿå›žé¥‹æœå‹™ - ä½¿ç”¨ 200 å¥å°ˆå®¶å»ºè­°"""

    def __init__(self):
        self.gemini_service = GeminiService()

    async def get_quick_feedback(
        self,
        recent_transcript: str,
        full_transcript: Optional[str] = None,
        tenant_id: Optional[str] = None,
        mode: Optional[str] = None,
        scenario_context: Optional[str] = None,
    ) -> Dict[str, str]:
        """
        ä½¿ç”¨ AI åˆ¤æ–·å®‰å…¨ç­‰ç´šï¼Œç„¶å¾Œå¾ž 200 å¥å°ˆå®¶å»ºè­°ä¸­é¸æ“‡

        Args:
            recent_transcript: æœ€è¿‘ 15 ç§’çš„é€å­—ç¨¿ï¼ˆé‡é»žåˆ†æžå°è±¡ï¼‰
            full_transcript: å®Œæ•´ç´¯ç©é€å­—ç¨¿ï¼ˆèƒŒæ™¯è„ˆçµ¡ï¼‰
            tenant_id: ç§Ÿæˆ¶ ID
            mode: æ¨¡å¼ ("practice" ç·´ç¿’æ¨¡å¼ / "emergency" å°è«‡æ¨¡å¼)
            scenario_context: å®¶é•·ç…©æƒ±æƒ…å¢ƒæè¿°

        Returns:
            {
                "message": "å¾ž 200 å¥ä¸­é¸å‡ºçš„å°ˆå®¶å»ºè­°",
                "type": "expert_suggestion",
                "timestamp": "ç•¶å‰æ™‚é–“",
                "latency_ms": å»¶é²æ™‚é–“
            }
        """
        start_time = time.time()

        # Use full_transcript as fallback if not provided
        if full_transcript is None:
            full_transcript = recent_transcript

        try:
            # Step 1: åˆ¤æ–·å®‰å…¨ç­‰ç´š + é¸æ“‡å»ºè­°ï¼ˆå–®ä¸€ Gemini å‘¼å«ï¼‰
            (
                safety_level,
                message,
                prompt_tokens,
                completion_tokens,
            ) = await self._analyze_and_select(
                recent_transcript, full_transcript, mode, scenario_context
            )

            latency_ms = int((time.time() - start_time) * 1000)

            return {
                "message": message,
                "type": "expert_suggestion",
                "timestamp": datetime.datetime.now().isoformat(),
                "latency_ms": latency_ms,
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": prompt_tokens + completion_tokens,
            }

        except Exception as e:
            logger.error(f"Quick feedback generation failed: {str(e)}")

            # Fallback to default green suggestion
            return {
                "message": GREEN_SUGGESTIONS[0],  # "è®“å­©å­çŸ¥é“ä½ ç«™åœ¨ä»–é€™é‚Š"
                "type": "fallback",
                "timestamp": datetime.datetime.now().isoformat(),
                "latency_ms": int((time.time() - start_time) * 1000),
                "error": str(e),
            }

    async def _analyze_and_select(
        self,
        recent_transcript: str,
        full_transcript: str,
        mode: Optional[str],
        scenario_context: Optional[str],
    ) -> tuple:
        """
        å–®ä¸€ Gemini å‘¼å«ï¼šåˆ¤æ–·å®‰å…¨ç­‰ç´š + å¾ž 200 å¥ä¸­é¸æ“‡å»ºè­°

        Returns:
            (safety_level, selected_suggestion, prompt_tokens, completion_tokens)
        """
        # Build suggestion lists for prompt
        green_list = "\n".join([f"  - {s}" for s in GREEN_SUGGESTIONS])
        yellow_list = "\n".join([f"  - {s}" for s in YELLOW_SUGGESTIONS])
        red_list = "\n".join([f"  - {s}" for s in RED_SUGGESTIONS])

        # Mode-specific context
        mode_context = ""
        if mode == "practice":
            mode_context = """âš ï¸ æ¨¡å¼ï¼šPracticeï¼ˆå–®äººç·´ç¿’ï¼‰
- åªæœ‰å®¶é•·ä¸€äººåœ¨ç·´ç¿’èªªè©±ï¼Œæ²’æœ‰å­©å­åœ¨å ´
- åˆ†æžé‡é»žï¼šè©•ä¼°ã€Œå®¶é•·çš„èªªè©±æŠ€å·§ã€"""
        else:
            mode_context = """âš ï¸ æ¨¡å¼ï¼šEmergencyï¼ˆå³æ™‚ä»‹å…¥ï¼‰
- çœŸå¯¦è¦ªå­å°è©±ç¾å ´
- åˆ†æžé‡é»žï¼šè©•ä¼°ã€Œè¦ªå­äº’å‹•çš„ç‹€æ…‹ã€"""

        # Scenario context
        scenario_section = ""
        if scenario_context:
            scenario_section = f"\nã€å®¶é•·ç…©æƒ±æƒ…å¢ƒã€‘\n{scenario_context}\n"

        prompt = f"""ä½ æ˜¯è¦ªå­æºé€šå°ˆå®¶ã€‚è«‹åˆ†æžä»¥ä¸‹å°è©±ï¼Œåˆ¤æ–·å®‰å…¨ç­‰ç´šä¸¦é¸æ“‡æœ€é©åˆçš„å°ˆå®¶å»ºè­°ã€‚

{mode_context}
{scenario_section}
ã€å®Œæ•´å°è©±èƒŒæ™¯ã€‘
{full_transcript[:500]}

ã€æœ€è¿‘ 15 ç§’ - é‡é»žåˆ†æžã€‘
{recent_transcript}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€ä»»å‹™ã€‘
1. åˆ¤æ–·å®‰å…¨ç­‰ç´šï¼ˆgreen/yellow/redï¼‰
2. å¾žå°æ‡‰çš„å»ºè­°åº«ä¸­é¸æ“‡ 1 å¥æœ€é©åˆçš„å»ºè­°

ã€å®‰å…¨ç­‰ç´šæ¨™æº–ã€‘
- greenï¼šæºé€šé †æš¢ã€æƒ…ç·’ç©©å®šã€äº’ç›¸å°Šé‡
- yellowï¼šæºé€šä¸è‰¯ã€æƒ…ç·’ç·Šå¼µã€å¿½ç•¥éœ€æ±‚
- redï¼šæƒ…ç·’å´©æ½°ã€è¡çªå‡ç´šã€èªžè¨€æš´åŠ›

ã€å»ºè­°å¥åº« - è«‹é€å­—é¸æ“‡ï¼Œä¸è¦æ”¹å¯«ã€‘

ðŸŸ¢ GREEN å»ºè­°ï¼š
{green_list}

ðŸŸ¡ YELLOW å»ºè­°ï¼š
{yellow_list}

ðŸ”´ RED å»ºè­°ï¼š
{red_list}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
è«‹å›žå‚³ JSON æ ¼å¼ï¼š
{{"safety_level": "green", "suggestion": "ä½ æ­£åœ¨è½ï¼Œè€Œä¸æ˜¯æ€¥è‘—å›ž"}}

CRITICAL:
1. suggestion å¿…é ˆå¾žä¸Šè¿°å»ºè­°åº«ä¸­é€å­—é¸æ“‡ï¼Œä¸è¦æ”¹å¯«æˆ–è‡ªå‰µ
2. æ‰€æœ‰å›žæ‡‰å¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼ˆzh-TWï¼‰"""

        response = await self.gemini_service.generate_text(
            prompt=prompt,
            temperature=0.3,  # Lower temperature for more consistent selection
            max_tokens=200,
            response_format={"type": "json_object"},
        )

        # Extract text and usage metadata
        text = response.text
        prompt_tokens = 0
        completion_tokens = 0
        if hasattr(response, "usage_metadata"):
            usage = response.usage_metadata
            prompt_tokens = getattr(usage, "prompt_token_count", 0) or 0
            completion_tokens = getattr(usage, "candidates_token_count", 0) or 0

        # Parse JSON response
        try:
            json_start = text.find("{")
            json_end = text.rfind("}") + 1
            if json_start >= 0 and json_end > json_start:
                result = json.loads(text[json_start:json_end])
                safety_level = result.get("safety_level", "green").lower()
                suggestion = result.get("suggestion", "")

                # Validate safety level
                if safety_level not in ["green", "yellow", "red"]:
                    safety_level = "green"

                # Validate suggestion is from our pool
                all_suggestions = (
                    GREEN_SUGGESTIONS + YELLOW_SUGGESTIONS + RED_SUGGESTIONS
                )
                if suggestion not in all_suggestions:
                    # AI generated something not in our list, pick a default
                    logger.warning(
                        f"AI generated suggestion not in pool: '{suggestion}', using default"
                    )
                    if safety_level == "green":
                        suggestion = GREEN_SUGGESTIONS[0]
                    elif safety_level == "yellow":
                        suggestion = YELLOW_SUGGESTIONS[0]
                    else:
                        suggestion = RED_SUGGESTIONS[0]

                return safety_level, suggestion, prompt_tokens, completion_tokens

        except json.JSONDecodeError as e:
            logger.warning(f"Failed to parse JSON response: {e}")

        # Fallback
        return "green", GREEN_SUGGESTIONS[0], prompt_tokens, completion_tokens


# å‰µå»ºå…¨å±€å¯¦ä¾‹
quick_feedback_service = QuickFeedbackService()
