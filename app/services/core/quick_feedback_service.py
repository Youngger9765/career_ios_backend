"""
Quick Feedback Service - è¼•é‡ AI å›é¥‹æœå‹™

ä½¿ç”¨è¼•é‡ç´š Gemini Flash prompt ç”Ÿæˆå¿«é€Ÿé¼“å‹µè¨Šæ¯ã€‚
å¼·åˆ¶ 15 å­—ä»¥å…§ï¼Œç¢ºä¿åŒå¿ƒåœ“ UI é¡¯ç¤ºè‰¯å¥½ã€‚
"""

import datetime
import logging
import time
from typing import Dict, Optional

from app.services.external.gemini_service import GeminiService
from app.services.utils.ai_validation import (
    apply_fallback_if_invalid,
    validate_finish_reason,
)

logger = logging.getLogger(__name__)

# 15 å­—ä»¥å…§çš„ fallback è¨Šæ¯
FALLBACK_MESSAGES = [
    "ç¹¼çºŒä¿æŒï¼Œä½ åšå¾—å¾ˆå¥½",
    "ä½ æ­£åœ¨è½å­©å­èªªè©±",
    "èªæ°£å¾ˆæº«å’Œï¼Œå¾ˆæ£’",
    "æœ‰åœ¨åŒç†å­©å­çš„æ„Ÿå—",
]


class QuickFeedbackService:
    """è¼•é‡ AI å¿«é€Ÿå›é¥‹æœå‹™ - å¼·åˆ¶ 15 å­—ä»¥å…§"""

    MAX_CHARS = 15  # æœ€å¤§å­—æ•¸é™åˆ¶

    def __init__(self):
        self.gemini_service = GeminiService()

    async def get_quick_feedback(
        self,
        recent_transcript: str,
        full_transcript: Optional[str] = None,
        tenant_id: Optional[str] = None,
        mode: Optional[str] = None,
        scenario_context: Optional[str] = None,
    ) -> Dict[str, str]:
        """
        ä½¿ç”¨ AI ç”Ÿæˆå¿«é€Ÿå›é¥‹ï¼ˆå¼·åˆ¶ 15 å­—ä»¥å…§ï¼‰

        Args:
            recent_transcript: æœ€è¿‘ 15 ç§’çš„é€å­—ç¨¿
            full_transcript: å®Œæ•´ç´¯ç©é€å­—ç¨¿
            tenant_id: ç§Ÿæˆ¶ ID
            mode: æ¨¡å¼ ("practice" / "emergency")
            scenario_context: å®¶é•·ç…©æƒ±æƒ…å¢ƒæè¿°

        Returns:
            {
                "message": "AI ç”Ÿæˆçš„é¼“å‹µè¨Šæ¯ï¼ˆ15 å­—ä»¥å…§ï¼‰",
                "type": "ai_generated",
                "timestamp": "ç•¶å‰æ™‚é–“",
                "latency_ms": å»¶é²æ™‚é–“
            }
        """
        start_time = time.time()

        if full_transcript is None:
            full_transcript = recent_transcript

        try:
            # Build prompt with strict 15-char limit
            message, prompt_tokens, completion_tokens, finish_ok = await self._generate_feedback(
                recent_transcript, full_transcript, mode, scenario_context
            )

            # Check if response was truncated
            if not finish_ok:
                logger.warning(
                    "Quick feedback response was truncated - using fallback"
                )
                import random
                message = random.choice(FALLBACK_MESSAGES)

            # Validate using centralized helper
            message = apply_fallback_if_invalid(
                text=message,
                min_chars=7,  # è‡³å°‘ 7 å­—æ‰ç®—å®Œæ•´ï¼ˆå¯å« emojiï¼‰
                max_chars=self.MAX_CHARS,  # 15 chars
                fallback=FALLBACK_MESSAGES,  # Will randomly choose
                field_name="quick_feedback_message",
            )

            latency_ms = int((time.time() - start_time) * 1000)

            # Calculate Gemini Flash 1.5 cost using centralized pricing
            from app.core.pricing import calculate_cost_for_model

            estimated_cost_usd = calculate_cost_for_model(
                model_name="gemini-1.5-flash-latest",
                input_tokens=prompt_tokens,
                output_tokens=completion_tokens,
            )

            return {
                "message": message,
                "type": "ai_generated",
                "timestamp": datetime.datetime.now().isoformat(),
                "latency_ms": latency_ms,
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": prompt_tokens + completion_tokens,
                "estimated_cost_usd": estimated_cost_usd,
                "model_name": self.gemini_service.model_name,
                "provider": "gemini",
            }

        except Exception as e:
            logger.error(f"Quick feedback generation failed: {str(e)}")

            # Fallback
            import random

            return {
                "message": random.choice(FALLBACK_MESSAGES),
                "type": "fallback",
                "timestamp": datetime.datetime.now().isoformat(),
                "latency_ms": int((time.time() - start_time) * 1000),
                "error": str(e),
            }

    async def _generate_feedback(
        self,
        recent_transcript: str,
        full_transcript: str,
        mode: Optional[str],
        scenario_context: Optional[str],
    ) -> tuple:
        """
        ä½¿ç”¨ Gemini ç”Ÿæˆ 15 å­—ä»¥å…§çš„å›é¥‹

        Returns:
            (message, prompt_tokens, completion_tokens, finish_ok)
        """
        # Mode context
        mode_context = ""
        if mode == "practice":
            mode_context = "ã€å–®äººç·´ç¿’æ¨¡å¼ã€‘åªæœ‰å®¶é•·åœ¨ç·´ç¿’ï¼Œè©•ä¼°èªªè©±æŠ€å·§"
        else:
            mode_context = "ã€å³æ™‚å°è©±æ¨¡å¼ã€‘çœŸå¯¦è¦ªå­äº’å‹•ï¼Œè©•ä¼°äº’å‹•ç‹€æ…‹"

        # Scenario context
        scenario_section = ""
        if scenario_context:
            scenario_section = f"æƒ…å¢ƒï¼š{scenario_context[:50]}\n"

        prompt = f"""ä½ æ˜¯è¦ªå­æºé€šå°ˆå®¶ã€‚è«‹ç”¨ä¸€å¥è©±ï¼ˆ15 å­—ä»¥å…§ï¼‰çµ¦å®¶é•·å³æ™‚å›é¥‹ã€‚

{mode_context}
{scenario_section}
ã€æœ€è¿‘å°è©±ã€‘
{recent_transcript[:300]}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€å›é¥‹è¦å‰‡ã€‘
1. âš ï¸ å¿…é ˆ 7-15 å­—ï¼ˆç¡¬æ€§é™åˆ¶ï¼ï¼‰
2. æ­£å‘é¼“å‹µç‚ºä¸»
3. å…·é«”æŒ‡å‡ºåšå¾—å¥½çš„åœ°æ–¹
4. ç¹é«”ä¸­æ–‡
5. å¯ä»¥ç”¨ emoji å¢åŠ æº«åº¦ ğŸŒŸ

ã€ç¯„ä¾‹ã€‘ï¼ˆéƒ½æ˜¯ 7-15 å­—ï¼‰
- ä½ æ­£åœ¨è½ï¼Œè€Œä¸æ˜¯æ€¥è‘—å› ğŸ‘‚
- èªæ°£å¾ˆæº«å’Œï¼Œç¹¼çºŒä¿æŒ ğŸ’ª
- æœ‰åœ¨æ¥ä½å­©å­çš„æƒ…ç·’ ğŸ¤—
- ä½ æ²’æœ‰æ€¥è‘—çµ¦ç­”æ¡ˆ
- è®“å­©å­æ„Ÿè¦ºè¢«ç†è§£ â¤ï¸

è«‹ç›´æ¥å›è¦†ä¸€å¥è©±ï¼Œä¸è¦åŠ ä»»ä½•æ¨™é»æˆ–æ ¼å¼ã€‚"""

        response = await self.gemini_service.generate_text(
            prompt=prompt,
            temperature=0.7,
            max_tokens=500,  # å¢åŠ åˆ° 500ï¼Œç¢ºä¿ä¸è¢«æˆªæ–·
        )

        # Validate finish_reason
        finish_ok = validate_finish_reason(response, provider="gemini")

        # Extract text - clean up Gemini's sometimes messy output
        text = response.text.strip()

        # Take first line only (Gemini sometimes adds commentary)
        text = text.split("\n")[0].strip()

        # Remove quotes and extra punctuation
        text = text.strip("\"'ã€‚ï¼Œï¼ã€Œã€")

        # Remove any English text or parentheses garbage (Gemini sometimes adds extra text)
        import re

        text = re.sub(
            r"[\(ï¼ˆ][^ï¼‰\)]*$", "", text
        ).strip()  # Remove trailing parentheses
        text = re.sub(r'[a-zA-Z"]+.*$', "", text).strip()  # Remove English text

        # Get token counts
        prompt_tokens = 0
        completion_tokens = 0
        if hasattr(response, "usage_metadata"):
            usage = response.usage_metadata
            prompt_tokens = getattr(usage, "prompt_token_count", 0) or 0
            completion_tokens = getattr(usage, "candidates_token_count", 0) or 0

        return text, prompt_tokens, completion_tokens, finish_ok


# å‰µå»ºå…¨å±€å¯¦ä¾‹
quick_feedback_service = QuickFeedbackService()
