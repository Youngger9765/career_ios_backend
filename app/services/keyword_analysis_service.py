"""
Keyword Analysis Service - AI-powered transcript keyword extraction
Extracted from app/api/sessions.py analyze_session_keywords endpoint (320 lines)

Multi-tenant support:
- career: è·æ¶¯è«®è©¢åˆ†æ
- island_parents: è¦ªå­æ•™é¤Šåˆ†æ
"""
import json
import logging
import math  # For ceiling rounding
from datetime import datetime, timezone
from decimal import Decimal
from typing import Dict, List
from uuid import UUID

from sqlalchemy.orm import Session as DBSession

from app.models.case import Case
from app.models.client import Client
from app.models.counselor import Counselor  # For counselor credit updates
from app.models.credit_log import CreditLog  # For dual-write pattern
from app.models.session import Session
from app.models.session_analysis_log import SessionAnalysisLog
from app.models.session_usage import SessionUsage
from app.schemas.realtime import CounselingMode
from app.services.gemini_service import GeminiService
from app.services.openai_service import OpenAIService
from app.services.rag_retriever import RAGRetriever

logger = logging.getLogger(__name__)


class KeywordAnalysisService:
    """Service for AI-powered keyword analysis of session transcripts"""

    def __init__(self, db: DBSession):
        self.db = db
        self.gemini_service = GeminiService()
        self.openai_service = OpenAIService()
        self.rag_retriever = RAGRetriever(self.openai_service)

    # Tenant-specific prompt templates
    TENANT_PROMPTS = {
        "career": """ä½ æ˜¯è·æ¶¯è«®è©¢å°ˆå®¶ï¼Œåˆ†æå€‹æ¡ˆçš„è·æ¶¯å›°æƒ‘å’Œè«®è©¢å°è©±ã€‚

èƒŒæ™¯è³‡è¨Šï¼š
{context}

å®Œæ•´å°è©±é€å­—ç¨¿ï¼ˆä¾›åƒè€ƒï¼Œç†è§£èƒŒæ™¯è„ˆçµ¡ï¼‰ï¼š
{full_transcript}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€æœ€è¿‘é€å­—ç¨¿ - ä¸»è¦åˆ†æå°è±¡ã€‘
ï¼ˆè«‹æ ¹æ“šæ­¤å€å¡Šé€²è¡Œé—œéµå­—åˆ†æï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{transcript_segment}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

è«‹åˆ†æä¸¦è¿”å› JSON æ ¼å¼ï¼š
{{
    "keywords": ["é—œéµè©1", "é—œéµè©2", ...],
    "categories": ["é¡åˆ¥1", "é¡åˆ¥2", ...],
    "confidence": 0.85,
    "counselor_insights": "çµ¦è«®è©¢å¸«çš„æ´è¦‹ï¼ˆ50å­—å…§ï¼‰",
    "safety_level": "green|yellow|red",
    "severity": 1-3,
    "display_text": "å€‹æ¡ˆç•¶å‰ç‹€æ³æè¿°",
    "action_suggestion": "å»ºè­°è«®è©¢å¸«æ¡å–çš„è¡Œå‹•"
}}

æ³¨æ„ï¼š
- keywords: è·æ¶¯ç›¸é—œé—œéµè©ï¼ˆç„¦æ…®ã€è¿·æƒ˜ã€è½‰è·ç­‰ï¼‰
- categories: è·æ¶¯é¡åˆ¥ï¼ˆè·æ¶¯æ¢ç´¢ã€å·¥ä½œå£“åŠ›ã€äººéš›é—œä¿‚ç­‰ï¼‰
- safety_level: green=ç©©å®š, yellow=éœ€é—œæ³¨, red=å±æ©Ÿ
- severity: 1=è¼•å¾®, 2=ä¸­ç­‰, 3=åš´é‡
- åˆ†æé‡é»ï¼šæœ€è¿‘é€å­—ç¨¿ï¼Œå®Œæ•´å°è©±åƒ…ä½œç‚ºèƒŒæ™¯åƒè€ƒ
""",
        "island_parents_emergency": """ä½ æ˜¯è¦ªå­æ•™é¤Šå°ˆå®¶ï¼Œæä¾›å³æ™‚å±æ©Ÿæé†’ã€‚é€™æ˜¯äº‹ä¸­æé†’æ¨¡å¼ï¼Œéœ€è¦å¿«é€Ÿåˆ¤æ–·å’Œç°¡æ½”å»ºè­°ã€‚

èƒŒæ™¯è³‡è¨Šï¼š
{context}

å®Œæ•´å°è©±é€å­—ç¨¿ï¼ˆä¾›åƒè€ƒï¼‰ï¼š
{full_transcript}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€æœ€è¿‘å°è©± - ç”¨æ–¼å®‰å…¨è©•ä¼°ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{transcript_segment}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

è«‹åˆ†æä¸¦è¿”å› JSON æ ¼å¼ï¼š
{{
    "safety_level": "green|yellow|red",
    "severity": 1-3,
    "display_text": "ç°¡çŸ­ç‹€æ³æè¿°ï¼ˆ1å¥è©±ï¼‰",
    "action_suggestion": "1-2å¥æœ€é—œéµå»ºè­°",
    "suggested_interval_seconds": 15,
    "keywords": ["é—œéµè©1", "é—œéµè©2"],
    "categories": ["é¡åˆ¥1"]
}}

ç´…é»ƒç¶ ç‡ˆåˆ¤æ–·æ¨™æº–ï¼š
- ğŸ”´ RED (åš´é‡): æƒ…ç·’å´©æ½°ã€å¤±æ§ã€è¡çªå‡ç´šã€èªè¨€æš´åŠ›
- ğŸŸ¡ YELLOW (éœ€èª¿æ•´): æºé€šä¸è‰¯ã€æƒ…ç·’ç·Šå¼µã€å¿½ç•¥æ„Ÿå—
- ğŸŸ¢ GREEN (è‰¯å¥½): æºé€šé †æš¢ã€æƒ…ç·’ç©©å®šã€äº’ç›¸å°Šé‡

âš ï¸ EMERGENCY MODE è¦æ±‚ï¼š
- èšç„¦ç•¶å‰æœ€éœ€è¦è™•ç†çš„å•é¡Œ
- å»ºè­°å¿…é ˆå¿«é€Ÿå¯åŸ·è¡Œ
- é¸æ“‡ 1-2 å¥æœ€é—œéµå»ºè­°å³å¯
- é¿å…å†—é•·èªªæ˜
""",
        "island_parents_practice": """ä½ æ˜¯è¦ªå­æ•™é¤Šå°ˆå®¶ï¼Œæä¾›è©³ç´°æ•™å­¸æŒ‡å°ã€‚é€™æ˜¯äº‹å‰ç·´ç¿’æ¨¡å¼ï¼Œå¯ä»¥æä¾›æ›´å®Œæ•´çš„åˆ†æå’Œå»ºè­°ã€‚

èƒŒæ™¯è³‡è¨Šï¼š
{context}

å®Œæ•´å°è©±é€å­—ç¨¿ï¼ˆä¾›åƒè€ƒï¼Œç†è§£èƒŒæ™¯è„ˆçµ¡ï¼‰ï¼š
{full_transcript}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€æœ€è¿‘å°è©± - ç”¨æ–¼å®‰å…¨è©•ä¼°ã€‘
ï¼ˆè«‹æ ¹æ“šæ­¤å€å¡Šåˆ¤æ–·ç•¶å‰å®‰å…¨ç­‰ç´šï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{transcript_segment}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

è«‹åˆ†æä¸¦è¿”å› JSON æ ¼å¼ï¼š
{{
    "safety_level": "green|yellow|red",
    "severity": 1-3,
    "display_text": "çµ¦å®¶é•·çš„æç¤ºæ–‡å­—",
    "action_suggestion": "è©³ç´°å»ºè­°ï¼ˆ3-4å¥ï¼‰ï¼ŒåŒ…å« Bridge æŠ€å·§èªªæ˜",
    "suggested_interval_seconds": 30,
    "keywords": ["é—œéµè©1", "é—œéµè©2", "é—œéµè©3"],
    "categories": ["é¡åˆ¥1", "é¡åˆ¥2"]
}}

ç´…é»ƒç¶ ç‡ˆåˆ¤æ–·æ¨™æº–ï¼š
- ğŸ”´ RED (åš´é‡): å­©å­æƒ…ç·’å´©æ½°ã€å®¶é•·å¤±æ§ã€è¡çªå‡ç´šã€èªè¨€æš´åŠ›
- ğŸŸ¡ YELLOW (éœ€èª¿æ•´): æºé€šä¸è‰¯ã€æƒ…ç·’ç·Šå¼µã€å–®å‘æŒ‡è²¬ã€å¿½ç•¥æ„Ÿå—
- ğŸŸ¢ GREEN (è‰¯å¥½): æºé€šé †æš¢ã€æƒ…ç·’ç©©å®šã€äº’ç›¸å°Šé‡ã€æœ‰æ•ˆå‚¾è½

âš ï¸ PRACTICE MODE è¦æ±‚ï¼š
- æä¾› 3-4 å¥è©³ç´°å»ºè­°
- èªªæ˜ Bridge æŠ€å·§å’Œæºé€šç­–ç•¥
- å¹«åŠ©å®¶é•·ç†è§£å­©å­è¡Œç‚ºèƒŒå¾Œçš„éœ€æ±‚
- å»ºè­°å…·é«”å°è©±æ–¹å¼å’Œèª¿æ•´æ–¹æ³•

âš ï¸ CRITICAL: å®‰å…¨ç­‰ç´šè©•ä¼°è«‹åªæ ¹æ“šã€Œã€æœ€è¿‘å°è©± - ç”¨æ–¼å®‰å…¨è©•ä¼°ã€‘ã€å€å¡Šåˆ¤æ–·ï¼Œ
å®Œæ•´å°è©±åƒ…ä½œç‚ºç†è§£è„ˆçµ¡åƒè€ƒã€‚å¦‚æœæœ€è¿‘å°è©±å·²ç·©å’Œï¼Œå³ä½¿ä¹‹å‰æœ‰å±éšªå…§å®¹ï¼Œ
ä¹Ÿæ‡‰è©•ä¼°ç‚ºè¼ƒä½é¢¨éšªã€‚

æ³¨æ„ï¼š
- display_text: æè¿°ç•¶å‰è¦ªå­äº’å‹•ç‹€æ³ï¼Œçµ¦å®¶é•·å…·é«”çš„è§€å¯Ÿæç¤º
- action_suggestion: å…·é«”å¯è¡Œçš„æºé€šèª¿æ•´å»ºè­°ï¼ŒåŒ…å«æ•™å­¸æ€§å…§å®¹
- severity: 1=è¼•å¾®, 2=ä¸­ç­‰, 3=åš´é‡
""",
    }

    # RAG category mapping for tenants
    TENANT_RAG_CATEGORIES = {
        "career": "career",
        "island_parents": "parenting",
    }

    async def analyze_partial(
        self,
        session: Session,
        client: Client,
        case: Case,
        transcript_segment: str,
        counselor_id: UUID,
        tenant_id: str,
        mode: CounselingMode = CounselingMode.practice,
    ) -> Dict:
        """
        Multi-tenant partial analysis with RAG support.

        Returns analysis results WITHOUT saving to DB (for immediate response).
        Use save_analysis_log_and_usage() in background task to persist.

        Args:
            session: Session model
            client: Client model
            case: Case model
            transcript_segment: Recent transcript text (e.g., last 60 seconds)
            counselor_id: Counselor UUID
            tenant_id: Tenant identifier (career, island_parents)

        Returns:
            Dict with tenant-specific analysis results + metadata for persistence
        """
        import time
        import uuid

        start_time = time.time()
        analysis_start_time = datetime.now(timezone.utc)

        try:
            # Build context
            context_str = self._build_context(session, client, case)

            # Get full transcript from session
            full_transcript = session.transcript_text or "ï¼ˆå°šç„¡å®Œæ•´é€å­—ç¨¿ï¼‰"

            # Get tenant-specific prompt template (with mode support for island_parents)
            if tenant_id == "island_parents":
                # island_parents tenant: select prompt based on mode
                prompt_key = f"island_parents_{mode.value}"
                prompt_template = self.TENANT_PROMPTS.get(
                    prompt_key, self.TENANT_PROMPTS["island_parents_practice"]
                )
            else:
                # Other tenants: use tenant_id directly (mode not applicable)
                prompt_template = self.TENANT_PROMPTS.get(
                    tenant_id, self.TENANT_PROMPTS["career"]
                )

            prompt = prompt_template.format(
                context=context_str,
                full_transcript=full_transcript,
                transcript_segment=transcript_segment[:500],
            )

            # Call Gemini AI
            ai_response = await self.gemini_service.generate_text(
                prompt, temperature=0.3, response_format={"type": "json_object"}
            )

            # Parse AI response
            result_data = self._parse_ai_response(ai_response)

            # Retrieve RAG documents (optional, based on tenant)
            rag_documents = []
            rag_sources = []
            try:
                rag_category = self.TENANT_RAG_CATEGORIES.get(tenant_id)
                if rag_category:
                    rag_results = await self.rag_retriever.search(
                        query=transcript_segment[:200],
                        top_k=3,
                        threshold=0.7,
                        db=self.db,
                        category=rag_category,
                    )
                    rag_documents = [
                        {
                            "doc_id": None,
                            "title": r["document"],
                            "content": r["text"],
                            "relevance_score": r["score"],
                            "chunk_id": None,
                        }
                        for r in rag_results
                    ]
                    rag_sources = [r["document"] for r in rag_results]
            except Exception as e:
                logger.warning(f"RAG retrieval failed for tenant {tenant_id}: {e}")
                # Continue without RAG documents

            # Get token usage from Gemini
            token_usage = getattr(ai_response, "usage_metadata", None)
            prompt_tokens = 0
            completion_tokens = 0
            total_tokens = 0
            estimated_cost = Decimal("0.0")

            if token_usage:
                prompt_tokens = getattr(token_usage, "prompt_token_count", 0)
                completion_tokens = getattr(token_usage, "candidates_token_count", 0)
                total_tokens = getattr(token_usage, "total_token_count", 0)

                # Estimate cost (Gemini 3 Flash pricing: $0.50/1M input, $3/1M output)
                input_cost = Decimal(prompt_tokens) * Decimal("0.00000050")
                output_cost = Decimal(completion_tokens) * Decimal("0.000003")
                estimated_cost = input_cost + output_cost

            # Calculate timing
            end_time = datetime.now(timezone.utc)
            duration_ms = int((time.time() - start_time) * 1000)

            # Extract LLM raw response
            llm_raw_response = (
                ai_response.text if hasattr(ai_response, "text") else str(ai_response)
            )

            # Add RAG documents and complete metadata to result
            result_data["rag_documents"] = rag_documents
            result_data["_metadata"] = {
                # Request metadata
                "request_id": str(uuid.uuid4()),
                "mode": mode.value,  # counseling mode: emergency or practice
                # Input data
                "time_range": None,  # Not applicable for partial
                "speakers": None,  # Not applicable for partial
                # Prompts
                "system_prompt": None,  # Could extract from template if needed
                "user_prompt": prompt,  # The actual prompt sent to Gemini
                "prompt_template": f"{tenant_id}_{mode.value}_v1"
                if tenant_id == "island_parents"
                else f"{tenant_id}_partial_v1",
                # RAG information
                "rag_used": len(rag_documents) > 0,
                "rag_query": transcript_segment[:200],
                "rag_documents": rag_documents,
                "rag_sources": rag_sources,
                "rag_top_k": 3,
                "rag_similarity_threshold": 0.7,
                "rag_search_time_ms": None,  # Not tracked separately yet
                # Model metadata
                "provider": "gemini",
                "model_name": "gemini-3-flash-preview",
                "model_version": "3.0",
                # Timing breakdown
                "start_time": analysis_start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "duration_ms": duration_ms,
                "api_response_time_ms": duration_ms,  # Same as duration for now
                "llm_call_time_ms": None,  # Not tracked separately yet
                # Token usage
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": total_tokens,
                "cached_tokens": 0,
                "estimated_cost_usd": float(estimated_cost),
                "token_usage": {
                    "prompt_tokens": prompt_tokens,
                    "completion_tokens": completion_tokens,
                    "total_tokens": total_tokens,
                },
                # LLM response
                "llm_raw_response": llm_raw_response,
                "analysis_reasoning": result_data.get(
                    "counselor_insights"
                ),  # For career tenant
                "matched_suggestions": [],  # Not applicable for partial
                # Cache metadata
                "use_cache": False,  # Not using cache yet
                "cache_hit": None,
                "cache_key": None,
                "gemini_cache_ttl": None,
                # Technical metrics
                "transcript_length": len(transcript_segment),
                "duration_seconds": None,  # Not applicable for partial
            }

            return result_data

        except Exception as e:
            logger.error(f"Partial analysis failed for tenant {tenant_id}: {e}")
            # Return fallback result based on tenant
            return self._get_tenant_fallback_result(tenant_id)

    def _parse_iso_datetime(self, iso_string: str) -> datetime:
        """Parse ISO datetime string to datetime object"""
        from datetime import datetime

        if isinstance(iso_string, str):
            return datetime.fromisoformat(iso_string.replace("Z", "+00:00"))
        return iso_string

    def save_analysis_log_and_usage(
        self,
        session_id: UUID,
        counselor_id: UUID,
        tenant_id: str,
        transcript_segment: str,
        result_data: Dict,
        rag_documents: List[dict],
        rag_sources: List[str],
        token_usage_data: Dict,
    ) -> None:
        """
        Save SessionAnalysisLog and update SessionUsage (cumulative ledger).
        Should be called as background task after analyze_partial.

        SessionUsage is a cumulative ledger that tracks:
        - analysis_count: Total number of analyses performed
        - total_tokens: Sum of all tokens used across analyses
        - credits_deducted: Total credits consumed
        - start_time: First analysis timestamp
        - end_time: Latest analysis timestamp

        Args:
            session_id: Session UUID
            counselor_id: Counselor UUID
            tenant_id: Tenant identifier
            transcript_segment: Transcript segment
            result_data: Analysis results (includes _metadata)
            rag_documents: RAG documents used
            rag_sources: RAG source references
            token_usage_data: Token usage info (prompt_tokens, completion_tokens, etc.)
        """
        import uuid

        try:
            # Extract metadata
            metadata = result_data.get("_metadata", {})

            # Parse ISO datetime strings back to datetime objects
            start_time = metadata.get("start_time")
            if start_time and isinstance(start_time, str):
                start_time = self._parse_iso_datetime(start_time)

            end_time = metadata.get("end_time")
            if end_time and isinstance(end_time, str):
                end_time = self._parse_iso_datetime(end_time)

            # Create SessionAnalysisLog with complete GBQ-aligned fields
            analysis_log = SessionAnalysisLog(
                session_id=session_id,
                counselor_id=counselor_id,
                tenant_id=tenant_id,
                # Analysis metadata
                analysis_type="partial_analysis",
                transcript=transcript_segment,
                analysis_result=result_data,
                # Safety assessment
                safety_level=result_data.get("safety_level"),
                severity=str(result_data.get("severity"))
                if result_data.get("severity")
                else None,
                display_text=result_data.get("display_text"),
                action_suggestion=result_data.get("action_suggestion"),
                # Request metadata
                request_id=metadata.get("request_id", str(uuid.uuid4())),
                mode=metadata.get("mode", "analyze_partial"),
                # Input data
                time_range=metadata.get("time_range"),
                speakers=metadata.get("speakers"),
                # Prompts
                system_prompt=metadata.get("system_prompt"),
                user_prompt=metadata.get("user_prompt"),
                prompt_template=metadata.get("prompt_template"),
                # RAG information
                rag_documents=rag_documents,
                rag_sources=rag_sources,
                rag_used=metadata.get("rag_used", False),
                rag_query=metadata.get("rag_query"),
                rag_top_k=metadata.get("rag_top_k"),
                rag_similarity_threshold=metadata.get("rag_similarity_threshold"),
                rag_search_time_ms=metadata.get("rag_search_time_ms"),
                # Model metadata
                provider=metadata.get("provider", "gemini"),
                model_name=metadata.get("model_name", "gemini-3-flash-preview"),
                model_version=metadata.get("model_version", "3.0"),
                # Timing breakdown
                start_time=start_time,
                end_time=end_time,
                duration_ms=metadata.get("duration_ms"),
                api_response_time_ms=metadata.get("api_response_time_ms"),
                llm_call_time_ms=metadata.get("llm_call_time_ms"),
                # Technical metrics
                transcript_length=metadata.get(
                    "transcript_length", len(transcript_segment)
                ),
                duration_seconds=metadata.get("duration_seconds"),
                # Token usage
                token_usage=metadata.get("token_usage", token_usage_data),
                prompt_tokens=token_usage_data.get("prompt_tokens", 0),
                completion_tokens=token_usage_data.get("completion_tokens", 0),
                total_tokens=token_usage_data.get("total_tokens", 0),
                cached_tokens=metadata.get("cached_tokens", 0),
                # Cost
                estimated_cost_usd=Decimal(
                    str(token_usage_data.get("estimated_cost_usd", 0))
                ),
                # LLM response
                llm_raw_response=metadata.get("llm_raw_response"),
                analysis_reasoning=metadata.get("analysis_reasoning"),
                matched_suggestions=metadata.get("matched_suggestions"),
                # Cache metadata
                use_cache=metadata.get("use_cache"),
                cache_hit=metadata.get("cache_hit"),
                cache_key=metadata.get("cache_key"),
                gemini_cache_ttl=metadata.get("gemini_cache_ttl"),
                # Timestamp
                analyzed_at=end_time or datetime.now(timezone.utc),
            )
            self.db.add(analysis_log)

            # Get or create SessionUsage (cumulative ledger pattern)
            session_usage = (
                self.db.query(SessionUsage)
                .filter(
                    SessionUsage.session_id == session_id,
                    SessionUsage.tenant_id == tenant_id,
                )
                .first()
            )

            # Extract token usage
            prompt_tokens = token_usage_data.get("prompt_tokens", 0)
            completion_tokens = token_usage_data.get("completion_tokens", 0)
            total_tokens = token_usage_data.get("total_tokens", 0)
            estimated_cost = Decimal(str(token_usage_data.get("estimated_cost_usd", 0)))

            current_time = datetime.now(timezone.utc)

            # ============================================================
            # INCREMENTAL BILLING WITH CEILING ROUNDING (1 credit = 1 minute)
            # ============================================================

            # Get counselor for credit deduction
            counselor = (
                self.db.query(Counselor).filter(Counselor.id == counselor_id).first()
            )

            if not counselor:
                logger.error(
                    f"Counselor {counselor_id} not found, cannot deduct credits"
                )
                self.db.commit()  # Still save analysis log
                return

            if session_usage:
                # UPDATE existing SessionUsage (subsequent analysis)
                # Calculate current duration and apply ceiling rounding
                if session_usage.start_time:
                    duration_seconds = int(
                        (current_time - session_usage.start_time).total_seconds()
                    )
                    current_minutes = math.ceil(duration_seconds / 60)
                    already_billed = session_usage.last_billed_minutes or 0
                    new_minutes = current_minutes - already_billed

                    logger.info(
                        f"Billing calculation: duration={duration_seconds}s, "
                        f"current_minutes={current_minutes}, already_billed={already_billed}, "
                        f"new_minutes={new_minutes}"
                    )

                    if new_minutes > 0:
                        # Deduct credits (1 credit = 1 minute)
                        credits_to_deduct = float(new_minutes)

                        # 1. Update Counselor (decrement available_credits)
                        counselor.available_credits -= credits_to_deduct

                        # 2. Update SessionUsage (cache)
                        session_usage.credits_deducted = (
                            session_usage.credits_deducted or Decimal("0.0")
                        ) + Decimal(str(credits_to_deduct))
                        session_usage.last_billed_minutes = current_minutes

                        # 3. Write CreditLog (authoritative source)
                        credit_log = CreditLog(
                            counselor_id=counselor_id,
                            resource_type="session",
                            resource_id=str(session_id),
                            credits_delta=-credits_to_deduct,  # Negative for usage
                            transaction_type="usage",
                            raw_data={
                                "feature": "session_analysis",
                                "duration_seconds": duration_seconds,
                                "current_minutes": current_minutes,
                                "incremental_minutes": new_minutes,
                                "analysis_type": "partial_analysis",
                                "tenant_id": tenant_id,
                            },
                            rate_snapshot={
                                "unit": "minute",
                                "rate": 1.0,
                                "rounding": "ceil",
                            },
                            calculation_details={
                                "duration_seconds": duration_seconds,
                                "current_minutes": current_minutes,
                                "already_billed_minutes": already_billed,
                                "new_minutes": new_minutes,
                                "credits_deducted": credits_to_deduct,
                            },
                        )
                        self.db.add(credit_log)

                        logger.info(
                            f"Deducted {credits_to_deduct} credits for session {session_id}: "
                            f"counselor.available_credits={counselor.available_credits}"
                        )
                    else:
                        logger.info(
                            f"No new minutes to bill for session {session_id} "
                            f"(current={current_minutes}, already_billed={already_billed})"
                        )

                # Update cumulative metrics (existing logic)
                session_usage.analysis_count = (session_usage.analysis_count or 0) + 1
                session_usage.total_prompt_tokens = (
                    session_usage.total_prompt_tokens or 0
                ) + prompt_tokens
                session_usage.total_completion_tokens = (
                    session_usage.total_completion_tokens or 0
                ) + completion_tokens
                session_usage.total_tokens = (
                    session_usage.total_tokens or 0
                ) + total_tokens
                session_usage.estimated_cost_usd = (
                    session_usage.estimated_cost_usd or Decimal("0.0")
                ) + estimated_cost
                session_usage.end_time = current_time
                session_usage.duration_seconds = (
                    duration_seconds if session_usage.start_time else 0
                )

                logger.info(
                    f"Updated SessionUsage for session {session_id}: "
                    f"analysis_count={session_usage.analysis_count}, "
                    f"total_tokens={session_usage.total_tokens}, "
                    f"credits_deducted={session_usage.credits_deducted}"
                )
            else:
                # CREATE new SessionUsage (first analysis)
                # First analysis at time T â†’ charge ceil(T/60) minutes
                duration_seconds = 0  # First analysis starts at 0
                current_minutes = 1  # Minimum charge is 1 minute (0:01-1:00 = 1 min)
                credits_to_deduct = 1.0  # 1 credit for first minute

                # 1. Update Counselor
                counselor.available_credits -= credits_to_deduct

                # 2. Create SessionUsage (cache)
                session_usage = SessionUsage(
                    session_id=session_id,
                    counselor_id=counselor_id,
                    tenant_id=tenant_id,
                    usage_type="partial_analysis",
                    status="in_progress",
                    start_time=current_time,
                    end_time=current_time,
                    duration_seconds=duration_seconds,
                    analysis_count=1,
                    total_prompt_tokens=prompt_tokens,
                    total_completion_tokens=completion_tokens,
                    total_tokens=total_tokens,
                    estimated_cost_usd=estimated_cost,
                    pricing_rule={"unit": "minute", "rate": 1.0, "rounding": "ceil"},
                    credits_deducted=Decimal(str(credits_to_deduct)),
                    last_billed_minutes=current_minutes,
                )
                self.db.add(session_usage)

                # 3. Write CreditLog (authoritative)
                credit_log = CreditLog(
                    counselor_id=counselor_id,
                    resource_type="session",
                    resource_id=str(session_id),
                    credits_delta=-credits_to_deduct,
                    transaction_type="usage",
                    raw_data={
                        "feature": "session_analysis",
                        "duration_seconds": duration_seconds,
                        "current_minutes": current_minutes,
                        "incremental_minutes": current_minutes,
                        "analysis_type": "partial_analysis",
                        "tenant_id": tenant_id,
                    },
                    rate_snapshot={
                        "unit": "minute",
                        "rate": 1.0,
                        "rounding": "ceil",
                    },
                    calculation_details={
                        "duration_seconds": duration_seconds,
                        "current_minutes": current_minutes,
                        "already_billed_minutes": 0,
                        "new_minutes": current_minutes,
                        "credits_deducted": credits_to_deduct,
                    },
                )
                self.db.add(credit_log)

                logger.info(
                    f"Created SessionUsage for session {session_id}: "
                    f"total_tokens={total_tokens}, "
                    f"credits_deducted={credits_to_deduct}, "
                    f"last_billed_minutes={current_minutes}"
                )

            # Commit all changes
            self.db.commit()
            logger.info(
                f"Saved analysis log and updated usage for session {session_id}"
            )

        except Exception as e:
            self.db.rollback()
            logger.error(f"Failed to save analysis log: {e}", exc_info=True)

    def _get_tenant_fallback_result(self, tenant_id: str) -> Dict:
        """Get tenant-specific fallback result when AI analysis fails"""
        if tenant_id == "island_parents":
            return {
                "safety_level": "green",
                "severity": 1,
                "display_text": "æ­£åœ¨åˆ†æè¦ªå­å°è©±...",
                "action_suggestion": "æŒçºŒè§€å¯Ÿæºé€šç‹€æ³",
                "suggested_interval_seconds": 20,
                "keywords": ["åˆ†æä¸­"],
                "categories": ["ä¸€èˆ¬"],
                "rag_documents": [],
            }
        else:  # career (default)
            return {
                "keywords": ["æ¢ç´¢ä¸­", "è«®è©¢é€²è¡Œ"],
                "categories": ["ä¸€èˆ¬è«®è©¢"],
                "confidence": 0.5,
                "counselor_insights": "æŒçºŒè§€å¯Ÿæ¡ˆä¸»ç‹€æ…‹ã€‚",
                "safety_level": "green",
                "severity": 1,
                "display_text": "åˆ†æä¸­",
                "action_suggestion": "æŒçºŒé—œæ³¨æ¡ˆä¸»éœ€æ±‚",
                "rag_documents": [],
            }

    async def analyze_transcript_keywords(
        self,
        session: Session,
        client: Client,
        case: Case,
        transcript_segment: str,
        counselor_id: UUID,
    ) -> Dict:
        """
        Analyze transcript segment for keywords using AI with session context.

        Returns: dict with keys: keywords, categories, confidence, counselor_insights
        """
        try:
            # Build AI prompt context
            context_str = self._build_context(session, client, case)

            # Build optimized prompt for fast response
            prompt = self._build_prompt(context_str, transcript_segment)

            # Call Gemini AI
            ai_response = await self.gemini_service.generate_text(
                prompt, temperature=0.3, response_format={"type": "json_object"}
            )

            # Parse AI response
            result_data = self._parse_ai_response(ai_response)

            # Save analysis log to session
            self._save_analysis_log(
                session, transcript_segment, result_data, counselor_id
            )

            return result_data

        except Exception as e:
            logger.error(f"Keyword extraction failed: {e}")
            # Fallback to rule-based analysis
            return self._fallback_rule_based_analysis(
                session, transcript_segment, counselor_id
            )

    def _build_context(self, session: Session, client: Client, case: Case) -> str:
        """Build context string from session, case, and client information"""
        context_parts = []

        # Add client information
        client_info = f"æ¡ˆä¸»è³‡è¨Š: {client.name}"
        if client.current_status:
            client_info += f", ç•¶å‰ç‹€æ³: {client.current_status}"
        if client.notes:
            client_info += f", å‚™è¨»: {client.notes}"
        context_parts.append(client_info)

        # Add case information
        case_info = f"æ¡ˆä¾‹ç›®æ¨™: {case.goals or 'æœªè¨­å®š'}"
        if case.problem_description:
            case_info += f", å•é¡Œæ•˜è¿°: {case.problem_description}"
        context_parts.append(case_info)

        # Add session information
        session_info = f"æœƒè«‡æ¬¡æ•¸: ç¬¬ {session.session_number} æ¬¡"
        if session.notes:
            session_info += f", æœƒè«‡å‚™è¨»: {session.notes}"
        context_parts.append(session_info)

        return "\n".join(context_parts)

    def _build_prompt(self, context: str, transcript_segment: str) -> str:
        """Build AI prompt for keyword extraction"""
        return f"""å¿«é€Ÿåˆ†æä»¥ä¸‹é€å­—ç¨¿ï¼Œæå–é—œéµè©å’Œæ´è¦‹ã€‚

èƒŒæ™¯ï¼š
{context}

é€å­—ç¨¿ï¼š
{transcript_segment[:500]}

JSONå›æ‡‰ï¼ˆç²¾ç°¡ï¼‰ï¼š
{{
    "keywords": ["è©1", "è©2", "è©3", "è©4", "è©5"],
    "categories": ["é¡åˆ¥1", "é¡åˆ¥2", "é¡åˆ¥3"],
    "confidence": 0.85,
    "counselor_insights": "ç°¡çŸ­æ´è¦‹ï¼ˆ50å­—å…§ï¼‰"
}}"""

    def _parse_ai_response(self, ai_response) -> Dict:
        """Parse AI response to extract keywords data"""
        if isinstance(ai_response, str):
            try:
                json_start = ai_response.find("{")
                json_end = ai_response.rfind("}") + 1
                if json_start >= 0 and json_end > json_start:
                    json_str = ai_response[json_start:json_end]
                    return json.loads(json_str)
                else:
                    # Quick fallback
                    return self._get_default_result()
            except json.JSONDecodeError:
                return self._get_default_result()
        else:
            return ai_response

    def _get_default_result(self) -> Dict:
        """Get default keyword analysis result"""
        return {
            "keywords": ["æ¢ç´¢ä¸­", "æƒ…ç·’", "ç™¼å±•"],
            "categories": ["ä¸€èˆ¬è«®è©¢"],
            "confidence": 0.5,
            "counselor_insights": "æŒçºŒè§€å¯Ÿæ¡ˆä¸»ç‹€æ…‹ã€‚",
        }

    def _save_analysis_log(
        self,
        session: Session,
        transcript_segment: str,
        result_data: Dict,
        counselor_id: UUID,
        is_fallback: bool = False,
    ) -> None:
        """Save analysis log to session"""
        from sqlalchemy.orm.attributes import flag_modified

        analysis_log_entry = {
            "analyzed_at": datetime.now(timezone.utc).isoformat(),
            "transcript_segment": transcript_segment[:200],
            "keywords": result_data.get("keywords", [])[:10],
            "categories": result_data.get("categories", [])[:5],
            "confidence": result_data.get("confidence", 0.5),
            "counselor_insights": result_data.get("counselor_insights", "")[:200],
            "counselor_id": str(counselor_id),
        }

        if is_fallback:
            analysis_log_entry["fallback"] = True

        if session.analysis_logs is None:
            session.analysis_logs = []

        session.analysis_logs.append(analysis_log_entry)
        flag_modified(session, "analysis_logs")

        self.db.commit()
        self.db.refresh(session)

    def _fallback_rule_based_analysis(
        self, session: Session, transcript: str, counselor_id: UUID
    ) -> Dict:
        """Fallback rule-based keyword extraction when AI fails"""
        # Common counseling keywords
        emotion_keywords = [
            "ç„¦æ…®",
            "å£“åŠ›",
            "ç·Šå¼µ",
            "é›£é",
            "é–‹å¿ƒ",
            "å®³æ€•",
            "ç”Ÿæ°£",
            "æ²®å–ª",
            "ç„¡åŠ©",
            "è¿·æƒ˜",
            "å›°æ“¾",
            "æ“”å¿ƒ",
            "è‡ªå‘",
        ]
        work_keywords = [
            "å·¥ä½œ",
            "ä¸»ç®¡",
            "åŒäº‹",
            "å…¬å¸",
            "è·æ¶¯",
            "è½‰è·",
            "é›¢è·",
            "ä¸Šç­",
            "åŠ ç­",
            "æ¥­ç¸¾",
            "å‡é·",
        ]
        relationship_keywords = [
            "å®¶äºº",
            "çˆ¶æ¯",
            "ä¼´ä¾¶",
            "æœ‹å‹",
            "é—œä¿‚",
            "æºé€š",
            "è¡çª",
            "ç›¸è™•",
            "å®¶åº­",
        ]
        development_keywords = [
            "ç›®æ¨™",
            "æ–¹å‘",
            "æˆå°±",
            "ç™¼å±•",
            "è¦åŠƒ",
            "æœªä¾†",
            "æ”¹è®Š",
            "å­¸ç¿’",
            "æˆé•·",
        ]

        # Extract keywords found in transcript
        found_keywords = []
        categories = set()

        for word in emotion_keywords:
            if word in transcript:
                found_keywords.append(word)
                categories.add("æƒ…ç·’ç®¡ç†")

        for word in work_keywords:
            if word in transcript:
                found_keywords.append(word)
                categories.add("è·æ¶¯ç™¼å±•")

        for word in relationship_keywords:
            if word in transcript:
                found_keywords.append(word)
                categories.add("äººéš›é—œä¿‚")

        for word in development_keywords:
            if word in transcript:
                found_keywords.append(word)
                categories.add("è‡ªæˆ‘æ¢ç´¢")

        # Default if no keywords found
        if not found_keywords:
            found_keywords = ["æ¢ç´¢ä¸­", "è«®è©¢é€²è¡Œ"]
            categories = {"ä¸€èˆ¬è«®è©¢"}

        # Generate insights based on keywords
        insights = self._generate_simple_insights(found_keywords, relationship_keywords)

        result = {
            "keywords": found_keywords[:10],
            "categories": list(categories)[:5],
            "confidence": 0.6,
            "counselor_insights": insights,
        }

        # Save fallback analysis log
        self._save_analysis_log(
            session, transcript, result, counselor_id, is_fallback=True
        )

        return result

    def _generate_simple_insights(
        self, found_keywords: List[str], relationship_keywords: List[str]
    ) -> str:
        """Generate simple insights from found keywords"""
        if "ç„¦æ…®" in found_keywords or "å£“åŠ›" in found_keywords:
            return "æ¡ˆä¸»è¡¨é”æƒ…ç·’å›°æ“¾ï¼Œå»ºè­°é—œæ³¨å£“åŠ›ä¾†æºåŠå› æ‡‰ç­–ç•¥ã€‚"
        elif "å·¥ä½œ" in found_keywords or "è·æ¶¯" in found_keywords:
            return "æ¡ˆä¸»æåŠè·æ¶¯è­°é¡Œï¼Œå¯æ¢ç´¢å·¥ä½œåƒ¹å€¼è§€èˆ‡ç™¼å±•æ–¹å‘ã€‚"
        elif any(k in found_keywords for k in relationship_keywords):
            return "æ¡ˆä¸»è«‡åŠäººéš›é—œä¿‚ï¼Œå»ºè­°æ¢ç´¢äº’å‹•æ¨¡å¼èˆ‡æºé€šæ–¹å¼ã€‚"
        else:
            keywords_str = ", ".join(found_keywords[:3])
            return f"æ¡ˆä¸»æåŠ {keywords_str}ï¼ŒæŒçºŒé—œæ³¨æ¡ˆä¸»éœ€æ±‚ã€‚"
