"""Gemini service for chat completions using Vertex AI"""

import logging
import os
from typing import Any, Dict, List, Optional

import vertexai
from vertexai.generative_models import GenerationConfig, GenerativeModel

# Import settings when available
try:
    from app.core.config import settings

    PROJECT_ID = getattr(settings, "GEMINI_PROJECT_ID", "groovy-iris-473015-h3")
    LOCATION = getattr(settings, "GEMINI_LOCATION", "us-central1")
    CHAT_MODEL = getattr(settings, "GEMINI_CHAT_MODEL", "gemini-2.5-flash")
except ImportError:
    PROJECT_ID = os.getenv("GEMINI_PROJECT_ID", "groovy-iris-473015-h3")
    LOCATION = os.getenv("GEMINI_LOCATION", "us-central1")
    CHAT_MODEL = os.getenv("GEMINI_CHAT_MODEL", "gemini-2.5-flash")


class GeminiService:
    """Service for Gemini LLM chat completions via Vertex AI"""

    def __init__(self, model_name: Optional[str] = None):
        """Initialize Gemini client (lazy loading)

        Args:
            model_name: Model name to use (default: from config)
        """
        self.project_id = PROJECT_ID
        self.location = LOCATION
        self.model_name = model_name or CHAT_MODEL
        self._chat_model = None
        self._initialized = False

    def _ensure_initialized(self):
        """Lazy initialization of models"""
        if not self._initialized:
            vertexai.init(project=self.project_id, location=self.location)
            self._chat_model = GenerativeModel(self.model_name)
            self._initialized = True

    @property
    def chat_model(self):
        self._ensure_initialized()
        return self._chat_model

    async def generate_text(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 8192,
        response_format: Optional[Dict[str, str]] = None,
    ) -> str:
        """
        Generate text using Gemini

        Args:
            prompt: The prompt to generate from
            temperature: Sampling temperature (0-1)
            max_tokens: Maximum tokens to generate
            response_format: Optional response format (e.g., {"type": "json_object"})

        Returns:
            Generated text
        """
        generation_config: Dict[str, Any] = {
            "temperature": temperature,
            "max_output_tokens": max_tokens,
        }

        # Add JSON mode if requested
        if response_format and response_format.get("type") == "json_object":
            generation_config["response_mime_type"] = "application/json"

        config = GenerationConfig(**generation_config)
        response = self.chat_model.generate_content(prompt, generation_config=config)

        # Log response details
        logger = logging.getLogger(__name__)
        logger.info(
            f"Gemini generate_content completed. Response text length: {len(response.text)}"
        )

        # Check for finish_reason to detect truncation
        if hasattr(response, "candidates") and response.candidates:
            candidate = response.candidates[0]
            if hasattr(candidate, "finish_reason"):
                logger.info(f"Finish reason: {candidate.finish_reason}")
                if candidate.finish_reason != 1:  # 1 = STOP (normal completion)
                    logger.warning(
                        f"Response may be incomplete. Finish reason: {candidate.finish_reason}"
                    )

        # Log usage metadata for cache performance tracking
        if hasattr(response, "usage_metadata"):
            usage = response.usage_metadata
            logger.info(f"ğŸ“Š Usage metadata: {usage}")
            if hasattr(usage, "cached_content_token_count"):
                logger.info(f"ğŸ¯ Cached tokens: {usage.cached_content_token_count}")
            if hasattr(usage, "prompt_token_count"):
                logger.info(f"ğŸ“ Prompt tokens: {usage.prompt_token_count}")
            if hasattr(usage, "candidates_token_count"):
                logger.info(f"ğŸ’¬ Output tokens: {usage.candidates_token_count}")

        return response.text

    async def chat_completion(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 8192,
        response_format: Optional[Dict[str, str]] = None,
    ) -> str:
        """
        Chat completion using Gemini (alias for generate_text for compatibility)

        Args:
            prompt: The prompt to generate from
            temperature: Sampling temperature (0-1)
            max_tokens: Maximum tokens to generate
            response_format: Optional response format (e.g., {"type": "json_object"})

        Returns:
            Generated text
        """
        return await self.generate_text(
            prompt, temperature, max_tokens, response_format
        )

    async def chat_completion_with_messages(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 8192,
        response_format: Optional[Dict[str, str]] = None,
    ) -> str:
        """
        Chat completion using OpenAI-style messages format

        Converts OpenAI messages format to Gemini prompt format.

        Args:
            messages: List of message dicts with 'role' and 'content'
                     Supported roles: system, user, assistant
            temperature: Sampling temperature (0-1)
            max_tokens: Maximum tokens to generate
            response_format: Optional response format (e.g., {"type": "json_object"})

        Returns:
            Generated text
        """
        # Convert OpenAI messages to Gemini prompt
        prompt_parts = []

        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")

            if role == "system":
                prompt_parts.append(f"System: {content}")
            elif role == "user":
                prompt_parts.append(f"User: {content}")
            elif role == "assistant":
                prompt_parts.append(f"Assistant: {content}")

        prompt = "\n\n".join(prompt_parts)

        return await self.generate_text(
            prompt=prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            response_format=response_format,
        )

    async def analyze_realtime_transcript(
        self,
        transcript: str,
        speakers: List[Dict[str, str]],
        rag_context: str = "",
    ) -> Dict[str, Any]:
        """Analyze realtime counseling transcript for AI supervision.

        Args:
            transcript: Full transcript text
            speakers: List of speaker segments with speaker role and text
            rag_context: Optional RAG knowledge base context

        Returns:
            Dict with: summary, alerts, suggestions
        """
        # Build speaker context
        speaker_context = "\n".join([f"{s['speaker']}: {s['text']}" for s in speakers])

        # Detect suicide risk keywords for alerts
        suicide_keywords = ["è‡ªæ®º", "æƒ³æ­»", "æ´»è‘—æ²’æ„ç¾©", "ä¸æƒ³æ´»", "çµæŸç”Ÿå‘½"]
        has_suicide_risk = any(keyword in transcript for keyword in suicide_keywords)

        prompt = f"""ä½ æ˜¯å°ˆæ¥­è«®è©¢ç£å°ï¼Œåˆ†æå³æ™‚è«®è©¢å°è©±ã€‚ä½ çš„è§’è‰²æ˜¯ç«™åœ¨æ¡ˆä¸»èˆ‡è«®è©¢å¸«ä¹‹é–“ï¼Œæä¾›æº«æš–ã€åŒç†ä¸”å…·é«”å¯è¡Œçš„å°ˆæ¥­å»ºè­°ã€‚

ã€è§’è‰²å®šç¾©ã€‘CRITICAL - å¿…é ˆåš´æ ¼éµå®ˆï¼š
- "counselor" = è«®è©¢å¸«/è¼”å°å¸«ï¼ˆå°ˆæ¥­åŠ©äººè€…ï¼Œæä¾›å”åŠ©çš„ä¸€æ–¹ï¼‰
- "client" = æ¡ˆä¸»/å€‹æ¡ˆ/å®¶é•·ï¼ˆæ±‚åŠ©è€…ï¼Œæœ‰å›°æ“¾éœ€è¦å”åŠ©çš„ä¸€æ–¹ï¼‰
- æ‰€æœ‰å•é¡Œã€å›°æ“¾ã€ç—‡ç‹€éƒ½æ˜¯ã€Œæ¡ˆä¸»/å€‹æ¡ˆã€é¢è‡¨çš„ï¼Œä¸æ˜¯è«®è©¢å¸«çš„å•é¡Œ
- åˆ†æç„¦é»ï¼šæ¡ˆä¸»çš„ç‹€æ³ã€éœ€æ±‚ã€é¢¨éšª
- å»ºè­°å°è±¡ï¼šçµ¦è«®è©¢å¸«çš„å°ˆæ¥­å»ºè­°ï¼ˆå¦‚ä½•å”åŠ©æ¡ˆä¸»ï¼‰

ã€åˆ†æç¯„åœã€‘CRITICAL - å¿…é ˆåš´æ ¼éµå®ˆï¼š
ğŸ¯ **ä¸»è¦åˆ†æç„¦é»**ï¼šæœ€æ–°ä¸€åˆ†é˜å…§çš„å°è©±å…§å®¹
   - ä½ æœƒæ”¶åˆ°å®Œæ•´çš„å°è©±è¨˜éŒ„ï¼ˆå¯èƒ½é•·é”æ•¸ååˆ†é˜ï¼‰
   - ä½†ä½ çš„åˆ†æå¿…é ˆèšç„¦åœ¨ã€Œæœ€å¾Œå‡ºç¾çš„å°è©±ã€ï¼ˆæœ€æ–°ä¸€åˆ†é˜ï¼‰
   - å‰é¢çš„å°è©±åƒ…ä½œç‚ºèƒŒæ™¯è„ˆçµ¡åƒè€ƒï¼Œå¹«åŠ©ä½ ç†è§£å‰å› å¾Œæœ

ğŸ“š **èƒŒæ™¯è„ˆçµ¡åƒè€ƒ**ï¼šå‰é¢çš„å°è©±å…§å®¹
   - äº†è§£æ¡ˆä¸»å’Œè«®è©¢å¸«çš„äº’å‹•æ­·ç¨‹å³å¯
   - ä¸éœ€è¦åœ¨åˆ†æä¸­è©³ç´°æåŠéæ—©çš„æ­·å²å…§å®¹

âœ… **è¼¸å‡ºè¦æ±‚**ï¼š
   - summaryï¼šèšç„¦æœ€æ–°ä¸€åˆ†é˜çš„æ ¸å¿ƒè­°é¡Œå’Œäº’å‹•é‡é»
   - alertsï¼šé‡å°æœ€æ–°ä¸€åˆ†é˜éœ€è¦ç«‹å³é—œæ³¨çš„ç‹€æ³
   - suggestionsï¼šåŸºæ–¼æœ€æ–°ä¸€åˆ†é˜çš„å°è©±ï¼Œçµ¦å‡ºå…·é«”è¡Œå‹•å»ºè­°

âŒ **é¿å…**ï¼š
   - ä¸è¦å°æ•´æ®µå°è©±åšç¸½é«”æ€§çš„å›é¡§æˆ–ç¸½çµ
   - ä¸è¦æåŠéæ—©çš„æ­·å²å…§å®¹ï¼ˆé™¤éèˆ‡ç•¶ä¸‹ç›´æ¥ç›¸é—œï¼‰
   - ä¸è¦åƒå¯«å ±å‘Šä¸€æ¨£ç¸½çµå…¨éƒ¨å…§å®¹

âœ… **æ­£ç¢ºå¿ƒæ…‹**ï¼šåƒä¸€å€‹å¯¦æ™‚åœ¨æ—è§€å¯Ÿçš„ç£å°ï¼Œé‡å°ã€Œç•¶ä¸‹é€™ä¸€åˆ»ã€çµ¦å‡ºå³æ™‚å»ºè­°

ã€æ ¸å¿ƒåŸå‰‡ã€‘åŒç†å„ªå…ˆã€æº«å’Œå¼•å°ã€å…·é«”è¡Œå‹•ï¼š

1. **åŒç†èˆ‡ç†è§£ç‚ºå…ˆ**
   - æ°¸é å…ˆç†è§£èˆ‡åŒç†æ¡ˆä¸»ï¼ˆå®¶é•·ï¼‰çš„æ„Ÿå—å’Œè™•å¢ƒ
   - èªå¯æ•™é¤Šå£“åŠ›ã€æƒ…ç·’å¤±æ§æ˜¯æ­£å¸¸çš„äººæ€§åæ‡‰
   - é¿å…æ‰¹åˆ¤ã€æŒ‡è²¬æˆ–è®“æ¡ˆä¸»æ„Ÿåˆ°è¢«å¦å®š

2. **æº«å’Œã€éæ‰¹åˆ¤çš„èªæ°£**
   - âŒ ç¦æ­¢ç”¨èªï¼šã€Œè¡¨é”å‡ºå°å­©å­ä½¿ç”¨èº«é«”æš´åŠ›çš„è¡å‹•ã€ã€Œå¯èƒ½é€ æˆå‚·å®³ã€ã€Œä¸ç•¶ç®¡æ•™ã€
   - âœ… å»ºè­°ç”¨èªï¼šã€Œç†è§£åˆ°åœ¨æ•™é¤Šå£“åŠ›ä¸‹ï¼Œçˆ¶æ¯æœ‰æ™‚æœƒæ„Ÿåˆ°æƒ…ç·’å¤±æ§æ˜¯å¾ˆæ­£å¸¸çš„ã€
   - âœ… ä½¿ç”¨ï¼šã€Œå¯ä»¥è€ƒæ…®ã€ã€Œæˆ–è¨±ã€ã€Œè©¦è©¦çœ‹ã€ç­‰æŸ”å’Œå¼•å°è©
   - âœ… ç„¦é»æ”¾åœ¨ã€Œå¦‚ä½•èª¿æ•´ã€è€Œéã€Œå“ªè£¡åšéŒ¯ã€

3. **å…·é«”ã€ç°¡æ½”çš„å»ºè­°**
   - å»ºè­°è¦å…·é«”å¯è¡Œï¼Œä½†ä¿æŒç°¡çŸ­ï¼ˆä¸è¶…é 50 å­—ï¼‰
   - é¿å…æŠ½è±¡æ¦‚å¿µï¼Œç”¨å…·é«”åšæ³•
   - ä¸è¦å†—é•·çš„æ­¥é©Ÿèªªæ˜æˆ–å°è©±ç¯„ä¾‹

ã€è¼¸å‡ºæ ¼å¼èˆ‡ç¯„ä¾‹ã€‘

å°è©±å…§å®¹ï¼š
{speaker_context}
{rag_context}

ã€ç°¡æ½”æ€§è¦æ±‚ã€‘CRITICAL - å¿…é ˆéµå®ˆï¼š
- âœ… summaryï¼š1-2 å¥è©±å³å¯ï¼ŒæŠ“æ ¸å¿ƒé‡é»
- âœ… alertsï¼šæœ€å¤š 2-3 é …ï¼Œæ¯é … 1 å¥è©±
- âœ… suggestionsï¼šæœ€å¤š 2-3 é …ï¼Œæ¯é …ç°¡æ˜æ‰¼è¦ï¼ˆä¸è¶…é 50 å­—ï¼‰
- âŒ ä¸è¦éåº¦è©³ç´°çš„æ­¥é©Ÿèªªæ˜ï¼ˆå¦‚ã€Œç¬¬ä¸€æ­¥ã€ç¬¬äºŒæ­¥ã€ç¬¬ä¸‰æ­¥ã€ï¼‰
- âŒ ä¸è¦å†—é•·çš„å°è©±ç¯„ä¾‹ï¼ˆç°¡çŸ­æç¤ºå³å¯ï¼‰
- âŒ ä¸è¦é‡è¤‡æˆ–å†—é¤˜çš„å…§å®¹

è«‹æä¾›ä»¥ä¸‹ JSON æ ¼å¼å›æ‡‰ï¼ˆä¸è¦ markdown code blockï¼‰ï¼š

{{
  "summary": "æ¡ˆä¸»è™•å¢ƒç°¡è¿°ï¼ˆ1-2 å¥ï¼‰",

  "alerts": [
    "ğŸ’¡ åŒç†æ¡ˆä¸»æ„Ÿå—ï¼ˆ1 å¥ï¼‰",
    "âš ï¸ éœ€é—œæ³¨çš„éƒ¨åˆ†ï¼ˆ1 å¥ï¼‰"
  ],

  "suggestions": [
    "ğŸ’¡ æ ¸å¿ƒå»ºè­°ï¼ˆç°¡çŸ­ï¼Œ< 50 å­—ï¼‰",
    "ğŸ’¡ å…·é«”åšæ³•ï¼ˆç°¡çŸ­ï¼Œ< 50 å­—ï¼‰"
  ]
}}

ã€èªæ°£è¦æ±‚ã€‘æº«å’Œã€åŒç†ã€ç°¡æ½”ï¼Œé¿å…æ‰¹åˆ¤æˆ–éåº¦èªªæ•™ã€‚

{
    "âš ï¸ è‡ªæ®ºé¢¨éšªè­¦ç¤ºï¼šå¦‚æœç™¼ç¾è‡ªæ®ºç›¸é—œé—œéµå­—ï¼ˆè‡ªæ®ºã€æƒ³æ­»ã€æ´»è‘—æ²’æ„ç¾©ç­‰ï¼‰ï¼Œè«‹åœ¨ alerts ç¬¬ä¸€é …æ˜ç¢ºæ¨™ç¤ºã€ğŸš¨ è‡ªæ®ºé¢¨éšªè­¦ç¤ºã€ä¸¦å»ºè­°ç«‹å³è©•ä¼°èˆ‡è½‰ä»‹ã€‚" if has_suicide_risk else ""
}

è«‹åš´æ ¼éµå®ˆä¸Šè¿°åŸå‰‡ï¼Œä»¥æº«æš–ã€å°ˆæ¥­ã€å…·é«”çš„æ–¹å¼æä¾›ç£å°å»ºè­°ã€‚
"""

        logger = logging.getLogger(__name__)
        logger.info(
            f"Starting realtime transcript analysis. Transcript length: {len(transcript)}, "
            f"Speakers: {len(speakers)}, RAG context length: {len(rag_context)}"
        )

        response = await self.generate_text(
            prompt=prompt,
            temperature=0.7,  # Increased from 0.3 for more empathetic, human-like responses
            max_tokens=4000,  # Increased from 2500 - previous value caused JSON truncation (finish_reason=2)
            response_format={"type": "json_object"},
        )

        # Parse JSON from response
        import json

        try:
            result = json.loads(response)

            # Ensure lists are present
            if "alerts" not in result:
                result["alerts"] = []
            if "suggestions" not in result:
                result["suggestions"] = []
            if "summary" not in result:
                result["summary"] = "åˆ†æä¸­..."

            logger.info(
                f"Successfully parsed Gemini response. Summary length: {len(result.get('summary', ''))}, "
                f"Alerts: {len(result.get('alerts', []))}, Suggestions: {len(result.get('suggestions', []))}"
            )

            return result
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse Gemini JSON response: {e}")
            logger.error(f"Response text length: {len(response)}")
            logger.error(f"Response text (first 500 chars): {response[:500]}")
            logger.error(f"Response text (last 500 chars): {response[-500:]}")

            # Check if response was truncated by examining finish_reason
            # Note: The response here is already text, but we can check if it's incomplete
            if len(response) >= 7900:  # Near max_tokens limit
                logger.warning(
                    f"Response length ({len(response)}) is near max_tokens (8000). "
                    "Response may have been truncated. Consider increasing max_tokens."
                )

            # Fallback: try to extract JSON from text
            import re

            json_match = re.search(r"\{.*\}", response, re.DOTALL)
            if json_match:
                logger.info("Attempting to extract JSON from response using regex...")
                try:
                    extracted_result = json.loads(json_match.group())
                    logger.info("Successfully extracted JSON from response")
                    return extracted_result
                except json.JSONDecodeError as regex_error:
                    logger.error(f"Failed to parse extracted JSON: {regex_error}")
                    logger.error(
                        f"Extracted JSON (first 500 chars): {json_match.group()[:500]}"
                    )

            # Log fallback usage
            logger.error(
                "All JSON parsing attempts failed. Returning fallback error response."
            )

            # Final fallback
            return {
                "summary": "åˆ†æå¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦",
                "alerts": ["ç„¡æ³•è§£æ AI å›æ‡‰"],
                "suggestions": ["è«‹æª¢æŸ¥è¼¸å…¥å…§å®¹"],
            }


# Create singleton instance
gemini_service = GeminiService()
