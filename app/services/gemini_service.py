"""Gemini service for chat completions using Vertex AI"""

import logging
import os
from typing import Any, Dict, List, Optional

import vertexai
from vertexai.generative_models import GenerationConfig, GenerativeModel

# Import settings when available
try:
    from app.core.config import settings

    PROJECT_ID = getattr(settings, "GEMINI_PROJECT_ID", "groovy-iris-473015-h3")
    LOCATION = getattr(settings, "GEMINI_LOCATION", "us-central1")
    CHAT_MODEL = getattr(settings, "GEMINI_CHAT_MODEL", "gemini-2.5-flash")
except ImportError:
    PROJECT_ID = os.getenv("GEMINI_PROJECT_ID", "groovy-iris-473015-h3")
    LOCATION = os.getenv("GEMINI_LOCATION", "us-central1")
    CHAT_MODEL = os.getenv("GEMINI_CHAT_MODEL", "gemini-2.5-flash")


class GeminiService:
    """Service for Gemini LLM chat completions via Vertex AI"""

    def __init__(self, model_name: Optional[str] = None):
        """Initialize Gemini client (lazy loading)

        Args:
            model_name: Model name to use (default: from config)
        """
        self.project_id = PROJECT_ID
        self.location = LOCATION
        self.model_name = model_name or CHAT_MODEL
        self._chat_model = None
        self._initialized = False

    def _ensure_initialized(self):
        """Lazy initialization of models"""
        if not self._initialized:
            vertexai.init(project=self.project_id, location=self.location)
            self._chat_model = GenerativeModel(self.model_name)
            self._initialized = True

    @property
    def chat_model(self):
        self._ensure_initialized()
        return self._chat_model

    async def generate_text(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 8192,
        response_format: Optional[Dict[str, str]] = None,
    ) -> str:
        """
        Generate text using Gemini

        Args:
            prompt: The prompt to generate from
            temperature: Sampling temperature (0-1)
            max_tokens: Maximum tokens to generate
            response_format: Optional response format (e.g., {"type": "json_object"})

        Returns:
            Generated text
        """
        generation_config: Dict[str, Any] = {
            "temperature": temperature,
            "max_output_tokens": max_tokens,
        }

        # Add JSON mode if requested
        if response_format and response_format.get("type") == "json_object":
            generation_config["response_mime_type"] = "application/json"

        config = GenerationConfig(**generation_config)
        response = self.chat_model.generate_content(prompt, generation_config=config)

        # Log response details
        logger = logging.getLogger(__name__)
        logger.info(
            f"Gemini generate_content completed. Response text length: {len(response.text)}"
        )

        # Check for finish_reason to detect truncation
        if hasattr(response, "candidates") and response.candidates:
            candidate = response.candidates[0]
            if hasattr(candidate, "finish_reason"):
                logger.info(f"Finish reason: {candidate.finish_reason}")
                if candidate.finish_reason != 1:  # 1 = STOP (normal completion)
                    logger.warning(
                        f"Response may be incomplete. Finish reason: {candidate.finish_reason}"
                    )

        return response.text

    async def chat_completion(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 8192,
        response_format: Optional[Dict[str, str]] = None,
    ) -> str:
        """
        Chat completion using Gemini (alias for generate_text for compatibility)

        Args:
            prompt: The prompt to generate from
            temperature: Sampling temperature (0-1)
            max_tokens: Maximum tokens to generate
            response_format: Optional response format (e.g., {"type": "json_object"})

        Returns:
            Generated text
        """
        return await self.generate_text(
            prompt, temperature, max_tokens, response_format
        )

    async def chat_completion_with_messages(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 8192,
        response_format: Optional[Dict[str, str]] = None,
    ) -> str:
        """
        Chat completion using OpenAI-style messages format

        Converts OpenAI messages format to Gemini prompt format.

        Args:
            messages: List of message dicts with 'role' and 'content'
                     Supported roles: system, user, assistant
            temperature: Sampling temperature (0-1)
            max_tokens: Maximum tokens to generate
            response_format: Optional response format (e.g., {"type": "json_object"})

        Returns:
            Generated text
        """
        # Convert OpenAI messages to Gemini prompt
        prompt_parts = []

        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")

            if role == "system":
                prompt_parts.append(f"System: {content}")
            elif role == "user":
                prompt_parts.append(f"User: {content}")
            elif role == "assistant":
                prompt_parts.append(f"Assistant: {content}")

        prompt = "\n\n".join(prompt_parts)

        return await self.generate_text(
            prompt=prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            response_format=response_format,
        )

    async def analyze_realtime_transcript(
        self,
        transcript: str,
        speakers: List[Dict[str, str]],
        rag_context: str = "",
    ) -> Dict[str, Any]:
        """Analyze realtime counseling transcript for AI supervision.

        Args:
            transcript: Full transcript text
            speakers: List of speaker segments with speaker role and text
            rag_context: Optional RAG knowledge base context

        Returns:
            Dict with: summary, alerts, suggestions
        """
        # Build speaker context
        speaker_context = "\n".join([f"{s['speaker']}: {s['text']}" for s in speakers])

        # Detect suicide risk keywords for alerts
        suicide_keywords = ["è‡ªæ®º", "æƒ³æ­»", "æ´»è‘—æ²’æ„ç¾©", "ä¸æƒ³æ´»", "çµæŸç”Ÿå‘½"]
        has_suicide_risk = any(keyword in transcript for keyword in suicide_keywords)

        prompt = f"""ä½ æ˜¯å°ˆæ¥­è«®è©¢ç£å°ï¼Œåˆ†æå³æ™‚è«®è©¢å°è©±ã€‚ä½ çš„è§’è‰²æ˜¯ç«™åœ¨æ¡ˆä¸»èˆ‡è«®è©¢å¸«ä¹‹é–“ï¼Œæä¾›æº«æš–ã€åŒç†ä¸”å…·é«”å¯è¡Œçš„å°ˆæ¥­å»ºè­°ã€‚

ã€è§’è‰²å®šç¾©ã€‘CRITICAL - å¿…é ˆåš´æ ¼éµå®ˆï¼š
- "counselor" = è«®è©¢å¸«/è¼”å°å¸«ï¼ˆå°ˆæ¥­åŠ©äººè€…ï¼Œæä¾›å”åŠ©çš„ä¸€æ–¹ï¼‰
- "client" = æ¡ˆä¸»/å€‹æ¡ˆ/å®¶é•·ï¼ˆæ±‚åŠ©è€…ï¼Œæœ‰å›°æ“¾éœ€è¦å”åŠ©çš„ä¸€æ–¹ï¼‰
- æ‰€æœ‰å•é¡Œã€å›°æ“¾ã€ç—‡ç‹€éƒ½æ˜¯ã€Œæ¡ˆä¸»/å€‹æ¡ˆã€é¢è‡¨çš„ï¼Œä¸æ˜¯è«®è©¢å¸«çš„å•é¡Œ
- åˆ†æç„¦é»ï¼šæ¡ˆä¸»çš„ç‹€æ³ã€éœ€æ±‚ã€é¢¨éšª
- å»ºè­°å°è±¡ï¼šçµ¦è«®è©¢å¸«çš„å°ˆæ¥­å»ºè­°ï¼ˆå¦‚ä½•å”åŠ©æ¡ˆä¸»ï¼‰

ã€æ ¸å¿ƒåŸå‰‡ã€‘åŒç†å„ªå…ˆã€æº«å’Œå¼•å°ã€å…·é«”è¡Œå‹•ï¼š

1. **åŒç†èˆ‡ç†è§£ç‚ºå…ˆ**
   - æ°¸é å…ˆç†è§£èˆ‡åŒç†æ¡ˆä¸»ï¼ˆå®¶é•·ï¼‰çš„æ„Ÿå—å’Œè™•å¢ƒ
   - èªå¯æ•™é¤Šå£“åŠ›ã€æƒ…ç·’å¤±æ§æ˜¯æ­£å¸¸çš„äººæ€§åæ‡‰
   - é¿å…æ‰¹åˆ¤ã€æŒ‡è²¬æˆ–è®“æ¡ˆä¸»æ„Ÿåˆ°è¢«å¦å®š

2. **æº«å’Œã€éæ‰¹åˆ¤çš„èªæ°£**
   - âŒ ç¦æ­¢ç”¨èªï¼šã€Œè¡¨é”å‡ºå°å­©å­ä½¿ç”¨èº«é«”æš´åŠ›çš„è¡å‹•ã€ã€Œå¯èƒ½é€ æˆå‚·å®³ã€ã€Œä¸ç•¶ç®¡æ•™ã€
   - âœ… å»ºè­°ç”¨èªï¼šã€Œç†è§£åˆ°åœ¨æ•™é¤Šå£“åŠ›ä¸‹ï¼Œçˆ¶æ¯æœ‰æ™‚æœƒæ„Ÿåˆ°æƒ…ç·’å¤±æ§æ˜¯å¾ˆæ­£å¸¸çš„ã€
   - âœ… ä½¿ç”¨ï¼šã€Œå¯ä»¥è€ƒæ…®ã€ã€Œæˆ–è¨±ã€ã€Œè©¦è©¦çœ‹ã€ç­‰æŸ”å’Œå¼•å°è©
   - âœ… ç„¦é»æ”¾åœ¨ã€Œå¦‚ä½•èª¿æ•´ã€è€Œéã€Œå“ªè£¡åšéŒ¯ã€

3. **å…·é«”ã€å¯åŸ·è¡Œçš„å»ºè­°**
   - æ¯å€‹å»ºè­°éƒ½å¿…é ˆåŒ…å«æ˜ç¢ºçš„æ­¥é©Ÿæˆ–å…·é«”è¡Œå‹•
   - æä¾›å¯¦éš›å¯ç”¨çš„å°è©±ç¯„ä¾‹
   - é¿å…æŠ½è±¡æ¦‚å¿µï¼ˆå¦‚ã€Œå»ºç«‹è‰¯å¥½æºé€šã€ï¼‰ï¼Œæ”¹ç”¨å…·é«”åšæ³•ï¼ˆå¦‚ã€Œæ™šé¤å¾ŒèŠ± 10 åˆ†é˜...ã€ï¼‰

ã€è¼¸å‡ºæ ¼å¼èˆ‡ç¯„ä¾‹ã€‘

å°è©±å…§å®¹ï¼š
{speaker_context}
{rag_context}

è«‹æä¾›ä»¥ä¸‹ JSON æ ¼å¼å›æ‡‰ï¼ˆä¸è¦ markdown code blockï¼‰ï¼š

{{
  "summary": "å®¢è§€æè¿°æ¡ˆä¸»è™•å¢ƒï¼Œä¸å¸¶æ‰¹åˆ¤ã€‚ä¾‹å¦‚ï¼šã€æ¡ˆä¸»ï¼ˆå®¶é•·ï¼‰æ­£é¢è‡¨å­©å­é’æ˜¥æœŸçš„æºé€šæŒ‘æˆ°ï¼Œåœ¨ç®¡æ•™éç¨‹ä¸­æ„Ÿåˆ°æŒ«æŠ˜èˆ‡ç„¡åŠ›ã€‚ã€",

  "alerts": [
    "ğŸ’¡ ç†è§£èˆ‡åŒç†ï¼šå…ˆåŒç†æ¡ˆä¸»çš„æ„Ÿå—ã€‚ä¾‹å¦‚ï¼šã€ç†è§£åˆ°æ¡ˆä¸»åœ¨å¤šæ¬¡å˜—è©¦æºé€šç„¡æ•ˆå¾Œï¼Œæ„Ÿåˆ°éå¸¸æŒ«æŠ˜ï¼Œé€™æ˜¯å¾ˆæ­£å¸¸çš„åæ‡‰ã€‚ã€",
    "âš ï¸ éœ€è¦é—œæ³¨çš„éƒ¨åˆ†ï¼šæº«å’ŒæŒ‡å‡ºéœ€è¦èª¿æ•´çš„åœ°æ–¹ã€‚ä¾‹å¦‚ï¼šã€å¯ä»¥ç•™æ„ç•¶æƒ…ç·’å‡é«˜æ™‚ï¼Œæš«æ™‚é›¢é–‹ç¾å ´å†·éœå¯èƒ½æœƒæœ‰å¹«åŠ©ã€‚ã€",
    "âš ï¸ é¢¨éšªè©•ä¼°ï¼šå¦‚æœ‰è‡ªå‚·ã€è‡ªæ®ºé¢¨éšªæˆ–æš´åŠ›å‚¾å‘ï¼Œæ˜ç¢ºæ¨™ç¤ºä¸¦å»ºè­°è½‰ä»‹"
  ],

  "suggestions": [
    "ğŸ’¡ å…·é«”è¡Œå‹•å»ºè­°ï¼ˆå«å°è©±ç¯„ä¾‹ï¼‰ï¼šã€å»ºè­°è«®è©¢å¸«å¼•å°æ¡ˆä¸»å»ºç«‹æƒ…ç·’å†·éœæ©Ÿåˆ¶ã€‚å…·é«”æ­¥é©Ÿï¼š(1) ç•¶æ„Ÿåˆ°å¿«è¦ç™¼è„¾æ°£æ™‚ï¼Œå…ˆæ·±å‘¼å¸ä¸‰æ¬¡ (2) å‘Šè¨´å­©å­ï¼šã€Œåª½åª½ç¾åœ¨éœ€è¦å†·éœä¸€ä¸‹ï¼Œæˆ‘å€‘ç­‰ä¸€ä¸‹å†è«‡å¥½å—ï¼Ÿã€(3) åˆ°å¦ä¸€å€‹æˆ¿é–“æˆ–æˆ¶å¤–èµ°èµ° 5-10 åˆ†é˜ (4) å†·éœå¾Œå†å›ä¾†æºé€šã€‚ã€",
    "ğŸ’¡ å¼•ç”¨å°ˆæ¥­çŸ¥è­˜ï¼ˆè‹¥æœ‰ RAGï¼‰ï¼šã€å¯åƒè€ƒæ­£å‘æ•™é¤Šçš„ã€Œæƒ…ç·’æ€¥æ•‘ç®±ã€æ¦‚å¿µï¼Œå”åŠ©æ¡ˆä¸»å»ºç«‹è‡ªå·±çš„æƒ…ç·’èª¿ç¯€å·¥å…·åŒ…...ã€",
    "ğŸ’¡ å¾ŒçºŒè¿½è¹¤ï¼šã€ä¸‹æ¬¡æœƒè«‡æ™‚ï¼Œå¯è©¢å•æ¡ˆä¸»é€™é€±æœ‰æ²’æœ‰é‡åˆ°é¡ä¼¼æƒ…å¢ƒï¼Œä»¥åŠä½¿ç”¨å†·éœç­–ç•¥çš„æ•ˆæœå¦‚ä½•ã€‚ã€"
  ]
}}

ã€å…·é«”èªæ°£ç¯„ä¾‹å°ç…§ã€‘

âŒ å¤ªç›´æ¥/æ‰¹åˆ¤ï¼š
ã€Œè«®è©¢å¸«è¡¨é”å‡ºå°å­©å­ä½¿ç”¨èº«é«”æš´åŠ›çš„è¡å‹•ï¼Œé€™å°å­©å­å¯èƒ½é€ æˆèº«å¿ƒå‚·å®³ã€

âœ… æº«å’ŒåŒç†ï¼š
ã€Œç†è§£åˆ°åœ¨æ•™é¤Šå£“åŠ›ä¸‹ï¼Œçˆ¶æ¯æœ‰æ™‚æœƒæ„Ÿåˆ°æƒ…ç·’å¤±æ§æ˜¯å¾ˆæ­£å¸¸çš„ã€‚é€™å€‹æ™‚åˆ»ï¼Œæˆ‘å€‘å¯ä»¥å…ˆç…§é¡§è‡ªå·±çš„æƒ…ç·’ï¼Œå†ä¾†è™•ç†å­©å­çš„è¡Œç‚ºã€‚å»ºè­°å¯ä»¥è©¦è©¦çœ‹ï¼šç•¶æ„Ÿåˆ°å¿«è¦å‹•æ‰‹æ™‚ï¼Œå…ˆæš«åœä¸¦æ·±å‘¼å¸ï¼Œå‘Šè¨´å­©å­ã€åª½åª½ç¾åœ¨å¾ˆç”Ÿæ°£ï¼Œæˆ‘éœ€è¦å†·éœä¸€ä¸‹ã€ï¼Œç„¶å¾Œé›¢é–‹ç¾å ´ 5-10 åˆ†é˜ã€‚ã€

âŒ å¤ªæŠ½è±¡ï¼š
ã€Œå»ºè­°æ”¹å–„è¦ªå­æºé€šã€

âœ… å…·é«”å¯è¡Œï¼š
ã€Œå»ºè­°æ¯å¤©æ™šé¤å¾Œå›ºå®š 15 åˆ†é˜ã€èŠå¤©æ™‚å…‰ã€ï¼š(1) é—œæ‰é›»è¦–å’Œæ‰‹æ©Ÿ (2) å•å­©å­ï¼šã€ä»Šå¤©å­¸æ ¡æœ‰ä»€éº¼æœ‰è¶£çš„äº‹ï¼Ÿã€æˆ–ã€ä»Šå¤©å¿ƒæƒ…æ€éº¼æ¨£ï¼Ÿã€(3) å°ˆå¿ƒè†è½ï¼Œä¸æ€¥è‘—çµ¦å»ºè­° (4) ç”¨ã€æˆ‘è½åˆ°ä½ èªª...æ˜¯é€™æ¨£å—ï¼Ÿã€ä¾†ç¢ºèªç†è§£ã€‚ã€

{
    "âš ï¸ è‡ªæ®ºé¢¨éšªè­¦ç¤ºï¼šå¦‚æœç™¼ç¾è‡ªæ®ºç›¸é—œé—œéµå­—ï¼ˆè‡ªæ®ºã€æƒ³æ­»ã€æ´»è‘—æ²’æ„ç¾©ç­‰ï¼‰ï¼Œè«‹åœ¨ alerts ç¬¬ä¸€é …æ˜ç¢ºæ¨™ç¤ºã€ğŸš¨ è‡ªæ®ºé¢¨éšªè­¦ç¤ºã€ä¸¦å»ºè­°ç«‹å³è©•ä¼°èˆ‡è½‰ä»‹ã€‚" if has_suicide_risk else ""
}

è«‹åš´æ ¼éµå®ˆä¸Šè¿°åŸå‰‡ï¼Œä»¥æº«æš–ã€å°ˆæ¥­ã€å…·é«”çš„æ–¹å¼æä¾›ç£å°å»ºè­°ã€‚
"""

        logger = logging.getLogger(__name__)
        logger.info(
            f"Starting realtime transcript analysis. Transcript length: {len(transcript)}, "
            f"Speakers: {len(speakers)}, RAG context length: {len(rag_context)}"
        )

        response = await self.generate_text(
            prompt=prompt,
            temperature=0.7,  # Increased from 0.3 for more empathetic, human-like responses
            max_tokens=4000,  # Increased from 2500 - previous value caused JSON truncation (finish_reason=2)
            response_format={"type": "json_object"},
        )

        # Parse JSON from response
        import json

        try:
            result = json.loads(response)

            # Ensure lists are present
            if "alerts" not in result:
                result["alerts"] = []
            if "suggestions" not in result:
                result["suggestions"] = []
            if "summary" not in result:
                result["summary"] = "åˆ†æä¸­..."

            logger.info(
                f"Successfully parsed Gemini response. Summary length: {len(result.get('summary', ''))}, "
                f"Alerts: {len(result.get('alerts', []))}, Suggestions: {len(result.get('suggestions', []))}"
            )

            return result
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse Gemini JSON response: {e}")
            logger.error(f"Response text length: {len(response)}")
            logger.error(f"Response text (first 500 chars): {response[:500]}")
            logger.error(f"Response text (last 500 chars): {response[-500:]}")

            # Check if response was truncated by examining finish_reason
            # Note: The response here is already text, but we can check if it's incomplete
            if len(response) >= 7900:  # Near max_tokens limit
                logger.warning(
                    f"Response length ({len(response)}) is near max_tokens (8000). "
                    "Response may have been truncated. Consider increasing max_tokens."
                )

            # Fallback: try to extract JSON from text
            import re

            json_match = re.search(r"\{.*\}", response, re.DOTALL)
            if json_match:
                logger.info("Attempting to extract JSON from response using regex...")
                try:
                    extracted_result = json.loads(json_match.group())
                    logger.info("Successfully extracted JSON from response")
                    return extracted_result
                except json.JSONDecodeError as regex_error:
                    logger.error(f"Failed to parse extracted JSON: {regex_error}")
                    logger.error(
                        f"Extracted JSON (first 500 chars): {json_match.group()[:500]}"
                    )

            # Log fallback usage
            logger.error(
                "All JSON parsing attempts failed. Returning fallback error response."
            )

            # Final fallback
            return {
                "summary": "åˆ†æå¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦",
                "alerts": ["ç„¡æ³•è§£æ AI å›æ‡‰"],
                "suggestions": ["è«‹æª¢æŸ¥è¼¸å…¥å…§å®¹"],
            }


# Create singleton instance
gemini_service = GeminiService()
