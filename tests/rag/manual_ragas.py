"""
Test RAGAS basic functionality with existing RAG system
"""

import asyncio

from dotenv import load_dotenv

# Load environment variables
load_dotenv()


async def test_ragas_basic():
    """Test basic RAGAS evaluation with sample data"""
    from datasets import Dataset
    from ragas import evaluate
    from ragas.metrics import (
        answer_relevancy,
        context_precision,
        context_recall,
        faithfulness,
    )

    # Sample test data simulating RAG output
    test_data = {
        "question": [
            "å¦‚ä½•æº–å‚™è»Ÿé«”å·¥ç¨‹å¸«é¢è©¦ï¼Ÿ",
            "è½‰è·åˆ°ç§‘æŠ€æ¥­éœ€è¦ä»€éº¼æº–å‚™ï¼Ÿ",
        ],
        "answer": [
            "æº–å‚™è»Ÿé«”å·¥ç¨‹å¸«é¢è©¦éœ€è¦å¹¾å€‹æ­¥é©Ÿï¼š1. ç·´ç¿’æ¼”ç®—æ³•é¡Œç›®ï¼Œç†Ÿæ‚‰å¸¸è¦‹çš„è³‡æ–™çµæ§‹å’Œæ¼”ç®—æ³•ã€‚2. æº–å‚™ç³»çµ±è¨­è¨ˆå•é¡Œï¼Œäº†è§£å¤§è¦æ¨¡ç³»çµ±çš„æ¶æ§‹ã€‚3. è¤‡ç¿’ç¨‹å¼èªè¨€åŸºç¤çŸ¥è­˜ã€‚4. æº–å‚™è¡Œç‚ºé¢è©¦å•é¡Œã€‚",
            "è½‰è·åˆ°ç§‘æŠ€æ¥­éœ€è¦ï¼š1. å­¸ç¿’ç›¸é—œæŠ€è¡“æŠ€èƒ½ï¼Œå¦‚ç¨‹å¼èªè¨€ã€æ¡†æ¶ç­‰ã€‚2. å»ºç«‹å€‹äººå°ˆæ¡ˆä½œå“é›†ã€‚3. åƒèˆ‡é–‹æºå°ˆæ¡ˆæˆ–æŠ€è¡“ç¤¾ç¾¤ã€‚4. æº–å‚™å±¥æ­·å’Œé¢è©¦ã€‚5. å»ºç«‹äººè„ˆç¶²çµ¡ã€‚",
        ],
        "contexts": [
            [
                "è»Ÿé«”å·¥ç¨‹å¸«é¢è©¦é€šå¸¸åŒ…å«ä¸‰å€‹éƒ¨åˆ†ï¼šæ¼”ç®—æ³•é¡Œç›®ã€ç³»çµ±è¨­è¨ˆã€è¡Œç‚ºå•é¡Œã€‚",
                "æº–å‚™æ¼”ç®—æ³•é¢è©¦æ™‚ï¼Œå»ºè­°åœ¨ LeetCode ä¸Šç·´ç¿’è‡³å°‘ 150 é¡Œä¸­ç­‰é›£åº¦çš„é¡Œç›®ã€‚",
                "ç³»çµ±è¨­è¨ˆé¢è©¦æœƒè€ƒå¯Ÿå€™é¸äººè¨­è¨ˆå¤§è¦æ¨¡åˆ†æ•£å¼ç³»çµ±çš„èƒ½åŠ›ã€‚",
            ],
            [
                "è½‰è·åˆ°ç§‘æŠ€æ¥­éœ€è¦å…·å‚™ç´®å¯¦çš„æŠ€è¡“åŸºç¤ï¼Œå»ºè­°å…ˆé¸æ“‡ä¸€å€‹ç¨‹å¼èªè¨€æ·±å…¥å­¸ç¿’ã€‚",
                "å»ºç«‹å€‹äººå°ˆæ¡ˆä½œå“é›†èƒ½å¤ å±•ç¤ºä½ çš„å¯¦éš›é–‹ç™¼èƒ½åŠ›ï¼Œå°æ±‚è·å¾ˆæœ‰å¹«åŠ©ã€‚",
                "åƒèˆ‡é–‹æºå°ˆæ¡ˆå¯ä»¥ç´¯ç©å¯¦æˆ°ç¶“é©—ï¼ŒåŒæ™‚å»ºç«‹æ¥­ç•Œäººè„ˆã€‚",
            ],
        ],
        "ground_truth": [
            "æº–å‚™è»Ÿé«”å·¥ç¨‹å¸«é¢è©¦éœ€è¦ç·´ç¿’æ¼”ç®—æ³•ã€ç³»çµ±è¨­è¨ˆå’Œè¡Œç‚ºé¢è©¦ä¸‰å€‹æ–¹é¢ã€‚",
            "è½‰è·ç§‘æŠ€æ¥­éœ€è¦å­¸ç¿’æŠ€è¡“ã€å»ºç«‹ä½œå“é›†ã€åƒèˆ‡ç¤¾ç¾¤æ´»å‹•ã€‚",
        ],
    }

    # Create dataset
    dataset = Dataset.from_dict(test_data)

    print("ğŸš€ Starting RAGAS evaluation...")
    print(f"Dataset size: {len(dataset)} examples")
    print("Metrics: faithfulness, answer_relevancy, context_recall, context_precision")
    print()

    # Run evaluation
    try:
        result = evaluate(
            dataset,
            metrics=[
                faithfulness,
                answer_relevancy,
                context_recall,
                context_precision,
            ],
        )

        print("âœ… RAGAS evaluation completed!")
        print()
        print("=" * 60)
        print("EVALUATION RESULTS")
        print("=" * 60)

        # Convert to pandas first to access data
        df = result.to_pandas()

        # Calculate average for each metric
        metrics = [
            "faithfulness",
            "answer_relevancy",
            "context_recall",
            "context_precision",
        ]
        for metric_name in metrics:
            if metric_name in df.columns:
                avg_value = df[metric_name].mean()
                print(f"{metric_name:20s}: {avg_value:.4f}")

        print("=" * 60)
        print()
        print(f"âœ… Evaluation successful! Tested {len(df)} samples.")
        print()

        return result

    except Exception as e:
        print(f"âŒ Error during evaluation: {str(e)}")
        import traceback

        traceback.print_exc()
        raise


async def test_ragas_with_real_data():
    """Test RAGAS with data from actual database"""
    from sqlalchemy import create_engine, select
    from sqlalchemy.orm import Session

    from app.core.config import settings
    from app.models.document import Chunk, Document
    from app.services.openai_service import OpenAIService

    print("ğŸ” Testing RAGAS with real database data...")

    # Create database connection
    engine = create_engine(settings.DATABASE_URL)

    with Session(engine) as db:
        # Get a sample document
        stmt = select(Document).limit(1)
        result = db.execute(stmt)
        document = result.scalar_one_or_none()

        if not document:
            print("âš ï¸  No documents found in database. Please upload a PDF first.")
            return

        print(f"Using document: {document.title}")

        # Get chunks for this document
        stmt = select(Chunk).where(Chunk.doc_id == document.id).limit(5)
        result = db.execute(stmt)
        chunks = result.scalars().all()

        if not chunks:
            print("âš ï¸  No chunks found for this document.")
            return

        print(f"Found {len(chunks)} chunks")

        # Create a simple test question
        openai_service = OpenAIService()
        sample_context = [chunk.text for chunk in chunks[:3]]

        # Generate answer using the chunks as context
        question = "è«‹æ ¹æ“šæ–‡ä»¶å…§å®¹ï¼Œèªªæ˜ä¸»è¦é‡é»æ˜¯ä»€éº¼ï¼Ÿ"
        context_text = "\n\n".join(sample_context)
        prompt = f"æ ¹æ“šä»¥ä¸‹å…§å®¹å›ç­”å•é¡Œï¼š\n\n{context_text}\n\nå•é¡Œï¼š{question}"

        answer = await openai_service.chat_completion(
            [{"role": "user", "content": prompt}]
        )

        # Prepare RAGAS dataset
        from datasets import Dataset

        test_data = {
            "question": [question],
            "answer": [answer],
            "contexts": [sample_context],
        }

        dataset = Dataset.from_dict(test_data)

        # Evaluate
        from ragas import evaluate
        from ragas.metrics import answer_relevancy, faithfulness

        result = evaluate(
            dataset,
            metrics=[faithfulness, answer_relevancy],
        )

        print()
        print("=" * 60)
        print("REAL DATA EVALUATION RESULTS")
        print("=" * 60)

        # Convert to pandas and extract metrics
        df = result.to_pandas()
        for metric_name in ["faithfulness", "answer_relevancy"]:
            if metric_name in df.columns:
                avg_value = df[metric_name].mean()
                print(f"{metric_name:20s}: {avg_value:.4f}")

        print("=" * 60)
        print()
        print(f"Question: {question}")
        print(f"Answer: {answer[:200]}...")

        return result


async def main():
    """Run all tests"""
    print("=" * 60)
    print("RAGAS FUNCTIONALITY TEST")
    print("=" * 60)
    print()

    # Test 1: Basic functionality with sample data
    print("TEST 1: Basic RAGAS evaluation with sample data")
    print("-" * 60)
    await test_ragas_basic()

    print()
    print()

    # Test 2: Real data from database (optional)
    print("TEST 2: RAGAS evaluation with real database data")
    print("-" * 60)
    try:
        await test_ragas_with_real_data()
    except Exception as e:
        print(f"âš ï¸  Skipping real data test: {str(e)}")

    print()
    print("âœ… All tests completed!")


if __name__ == "__main__":
    asyncio.run(main())
